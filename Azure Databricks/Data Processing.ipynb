{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "185d2a77-2088-4ecf-8db3-dd3bc611469a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello varshini\n"
     ]
    }
   ],
   "source": [
    "print(\"hello varshini\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "79f0bacb-3d9e-4742-b13e-94e25caf5070",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Name</th><th>Age</th></tr></thead><tbody><tr><td>John</td><td>25</td></tr><tr><td>Jane</td><td>30</td></tr><tr><td>Sam</td><td>22</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "John",
         25
        ],
        [
         "Jane",
         30
        ],
        [
         "Sam",
         22
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Name",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Create a Spark Dataframe \n",
    "data = [(\"John\", 25), (\"Jane\", 30), (\"Sam\", 22)] \n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "#Display the Dataframe\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5d30ac2c-f7a0-434b-b540-44a647fe9509",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Age</th><th>count</th></tr></thead><tbody><tr><td>25</td><td>1</td></tr><tr><td>30</td><td>1</td></tr><tr><td>22</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         25,
         1
        ],
        [
         30,
         1
        ],
        [
         22,
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "Age",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#display the data as a bar chart\n",
    "df.groupBy(\"Age\").count().display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f2f78f18-82d9-401a-8a37-fced01131c7f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1943595768095691>, line 7\u001B[0m\n",
       "\u001B[1;32m      4\u001B[0m lines \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msocket\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocalhost\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mport\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m9999\u001B[39m)\u001B[38;5;241m.\u001B[39mload()\n",
       "\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#Split the lines into words\u001B[39;00m\n",
       "\u001B[0;32m----> 7\u001B[0m words \u001B[38;5;241m=\u001B[39m lines\u001B[38;5;241m.\u001B[39mselectExpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexplode(split(value,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)) as word\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#Count the number of words\u001B[39;00m\n",
       "\u001B[1;32m     10\u001B[0m wordCounts \u001B[38;5;241m=\u001B[39m words\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     50\u001B[0m     )\n",
       "\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3872\u001B[0m, in \u001B[0;36mDataFrame.selectExpr\u001B[0;34m(self, *expr)\u001B[0m\n",
       "\u001B[1;32m   3870\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(expr) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(expr[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m):\n",
       "\u001B[1;32m   3871\u001B[0m     expr \u001B[38;5;241m=\u001B[39m expr[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n",
       "\u001B[0;32m-> 3872\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselectExpr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jseq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   3873\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mParseException\u001B[0m: \n",
       "[PARSE_SYNTAX_ERROR] Syntax error at or near '('. SQLSTATE: 42601 (line 1, pos 13)\n",
       "\n",
       "== SQL ==\n",
       "explode(split(value,)) as word\n",
       "-------------^^^\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "ParseException",
        "evalue": "\n[PARSE_SYNTAX_ERROR] Syntax error at or near '('. SQLSTATE: 42601 (line 1, pos 13)\n\n== SQL ==\nexplode(split(value,)) as word\n-------------^^^\n"
       },
       "metadata": {
        "errorSummary": "[PARSE_SYNTAX_ERROR] Syntax error at or near '('. SQLSTATE: 42601"
       },
       "removedWidgets": [],
       "sqlProps": {
        "errorClass": "PARSE_SYNTAX_ERROR",
        "pysparkCallSite": null,
        "pysparkFragment": null,
        "sqlState": "42601",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mParseException\u001B[0m                            Traceback (most recent call last)",
        "File \u001B[0;32m<command-1943595768095691>, line 7\u001B[0m\n\u001B[1;32m      4\u001B[0m lines \u001B[38;5;241m=\u001B[39m spark\u001B[38;5;241m.\u001B[39mreadStream\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msocket\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhost\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlocalhost\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mport\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;241m9999\u001B[39m)\u001B[38;5;241m.\u001B[39mload()\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m#Split the lines into words\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m words \u001B[38;5;241m=\u001B[39m lines\u001B[38;5;241m.\u001B[39mselectExpr(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mexplode(split(value,\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m)) as word\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      9\u001B[0m \u001B[38;5;66;03m#Count the number of words\u001B[39;00m\n\u001B[1;32m     10\u001B[0m wordCounts \u001B[38;5;241m=\u001B[39m words\u001B[38;5;241m.\u001B[39mgroupBy(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword\u001B[39m\u001B[38;5;124m\"\u001B[39m)\u001B[38;5;241m.\u001B[39mcount()\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:47\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     45\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 47\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     48\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     49\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     50\u001B[0m     )\n\u001B[1;32m     51\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/sql/dataframe.py:3872\u001B[0m, in \u001B[0;36mDataFrame.selectExpr\u001B[0;34m(self, *expr)\u001B[0m\n\u001B[1;32m   3870\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(expr) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(expr[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mlist\u001B[39m):\n\u001B[1;32m   3871\u001B[0m     expr \u001B[38;5;241m=\u001B[39m expr[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# type: ignore[assignment]\u001B[39;00m\n\u001B[0;32m-> 3872\u001B[0m jdf \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jdf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mselectExpr\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jseq\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpr\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3873\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(jdf, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msparkSession)\n",
        "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1355\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1349\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1350\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1351\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1352\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1354\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1355\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1356\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1358\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1359\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(temp_arg, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m_detach\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
        "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions/captured.py:230\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    226\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    227\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    228\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    229\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 230\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
        "\u001B[0;31mParseException\u001B[0m: \n[PARSE_SYNTAX_ERROR] Syntax error at or near '('. SQLSTATE: 42601 (line 1, pos 13)\n\n== SQL ==\nexplode(split(value,)) as word\n-------------^^^\n"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#example code for kafka\n",
    "\n",
    "#Read streaming data from a socket (simulated source)\n",
    "lines = spark.readStream.format(\"socket\").option(\"host\", \"localhost\").option(\"port\", 9999).load()\n",
    "\n",
    "#Split the lines into words\n",
    "words = lines.selectExpr(\"explode(split(value,' ')) as word\")\n",
    "\n",
    "#Count the number of words\n",
    "wordCounts = words.groupBy(\"word\").count()\n",
    "\n",
    "# Start the streaming query to console\n",
    "query = wordCounts.writeStream.outputlode(\"complete\").format(\"console\").start()\n",
    "\n",
    "#Avait termination (keep it running)\n",
    "\n",
    "query.waitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1ef583c1-b667-458f-8b4b-7d9231e26cdf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c0</th><th>_c1</th><th>_c2</th><th>_c3</th><th>_c4</th><th>_c5</th></tr></thead><tbody><tr><td>FL_DATE</td><td>CARRIER</td><td>ORIGIN</td><td>DEST</td><td>DEP_DELAY</td><td>ARR_DELAY</td></tr><tr><td>2023-09-01</td><td>AA</td><td>ATL</td><td>DFW</td><td>5</td><td>10</td></tr><tr><td>2023-09-01</td><td>UA</td><td>LAX</td><td>JFK</td><td>-3</td><td>0</td></tr><tr><td>2023-09-01</td><td>DL</td><td>SFO</td><td>ORD</td><td>7</td><td>15</td></tr><tr><td>2023-09-02</td><td>AA</td><td>DFW</td><td>LAX</td><td>0</td><td>-5</td></tr><tr><td>2023-09-02</td><td>UA</td><td>JFK</td><td>ATL</td><td>-2</td><td>0</td></tr><tr><td>2023-09-02</td><td>DL</td><td>ORD</td><td>LAX</td><td>20</td><td>30</td></tr><tr><td>2023-09-03</td><td>AA</td><td>LAX</td><td>SFO</td><td>10</td><td>12</td></tr><tr><td>2023-09-03</td><td>UA</td><td>ATL</td><td>ORD</td><td>0</td><td>-10</td></tr><tr><td>2023-09-03</td><td>DL</td><td>SFO</td><td>JFK</td><td>5</td><td>25</td></tr><tr><td>2023-09-04</td><td>AA</td><td>JFK</td><td>LAX</td><td>0</td><td>0</td></tr><tr><td>2023-09-04</td><td>UA</td><td>ORD</td><td>ATL</td><td>15</td><td>20</td></tr><tr><td>2023-09-04</td><td>DL</td><td>LAX</td><td>SFO</td><td>-5</td><td>-10</td></tr><tr><td>2023-09-05</td><td>AA</td><td>LAX</td><td>JFK</td><td>20</td><td>25</td></tr><tr><td>2023-09-05</td><td>UA</td><td>DFW</td><td>ATL</td><td>0</td><td>0</td></tr><tr><td>2023-09-05</td><td>DL</td><td>JFK</td><td>LAX</td><td>10</td><td>15</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "FL_DATE",
         "CARRIER",
         "ORIGIN",
         "DEST",
         "DEP_DELAY",
         "ARR_DELAY"
        ],
        [
         "2023-09-01",
         "AA",
         "ATL",
         "DFW",
         "5",
         "10"
        ],
        [
         "2023-09-01",
         "UA",
         "LAX",
         "JFK",
         "-3",
         "0"
        ],
        [
         "2023-09-01",
         "DL",
         "SFO",
         "ORD",
         "7",
         "15"
        ],
        [
         "2023-09-02",
         "AA",
         "DFW",
         "LAX",
         "0",
         "-5"
        ],
        [
         "2023-09-02",
         "UA",
         "JFK",
         "ATL",
         "-2",
         "0"
        ],
        [
         "2023-09-02",
         "DL",
         "ORD",
         "LAX",
         "20",
         "30"
        ],
        [
         "2023-09-03",
         "AA",
         "LAX",
         "SFO",
         "10",
         "12"
        ],
        [
         "2023-09-03",
         "UA",
         "ATL",
         "ORD",
         "0",
         "-10"
        ],
        [
         "2023-09-03",
         "DL",
         "SFO",
         "JFK",
         "5",
         "25"
        ],
        [
         "2023-09-04",
         "AA",
         "JFK",
         "LAX",
         "0",
         "0"
        ],
        [
         "2023-09-04",
         "UA",
         "ORD",
         "ATL",
         "15",
         "20"
        ],
        [
         "2023-09-04",
         "DL",
         "LAX",
         "SFO",
         "-5",
         "-10"
        ],
        [
         "2023-09-05",
         "AA",
         "LAX",
         "JFK",
         "20",
         "25"
        ],
        [
         "2023-09-05",
         "UA",
         "DFW",
         "ATL",
         "0",
         "0"
        ],
        [
         "2023-09-05",
         "DL",
         "JFK",
         "LAX",
         "10",
         "15"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c0",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c2",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c3",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c4",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "_c5",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 rows\n+----------+-------+------+----+---------+---------+\n|       _c0|    _c1|   _c2| _c3|      _c4|      _c5|\n+----------+-------+------+----+---------+---------+\n|   FL_DATE|CARRIER|ORIGIN|DEST|DEP_DELAY|ARR_DELAY|\n|2023-09-01|     AA|   ATL| DFW|        5|       10|\n|2023-09-01|     UA|   LAX| JFK|       -3|        0|\n|2023-09-01|     DL|   SFO| ORD|        7|       15|\n|2023-09-02|     AA|   DFW| LAX|        0|       -5|\n|2023-09-02|     UA|   JFK| ATL|       -2|        0|\n|2023-09-02|     DL|   ORD| LAX|       20|       30|\n|2023-09-03|     AA|   LAX| SFO|       10|       12|\n|2023-09-03|     UA|   ATL| ORD|        0|      -10|\n|2023-09-03|     DL|   SFO| JFK|        5|       25|\n+----------+-------+------+----+---------+---------+\nonly showing top 10 rows\n\nroot\n |-- _c0: string (nullable = true)\n |-- _c1: string (nullable = true)\n |-- _c2: string (nullable = true)\n |-- _c3: string (nullable = true)\n |-- _c4: string (nullable = true)\n |-- _c5: string (nullable = true)\n\nCleaned data\n+----------+-------+------+----+---------+---------+\n|       _c0|    _c1|   _c2| _c3|      _c4|      _c5|\n+----------+-------+------+----+---------+---------+\n|   FL_DATE|CARRIER|ORIGIN|DEST|DEP_DELAY|ARR_DELAY|\n|2023-09-01|     AA|   ATL| DFW|        5|       10|\n|2023-09-01|     UA|   LAX| JFK|       -3|        0|\n|2023-09-01|     DL|   SFO| ORD|        7|       15|\n|2023-09-02|     AA|   DFW| LAX|        0|       -5|\n|2023-09-02|     UA|   JFK| ATL|       -2|        0|\n|2023-09-02|     DL|   ORD| LAX|       20|       30|\n|2023-09-03|     AA|   LAX| SFO|       10|       12|\n|2023-09-03|     UA|   ATL| ORD|        0|      -10|\n|2023-09-03|     DL|   SFO| JFK|        5|       25|\n|2023-09-04|     AA|   JFK| LAX|        0|        0|\n|2023-09-04|     UA|   ORD| ATL|       15|       20|\n|2023-09-04|     DL|   LAX| SFO|       -5|      -10|\n|2023-09-05|     AA|   LAX| JFK|       20|       25|\n|2023-09-05|     UA|   DFW| ATL|        0|        0|\n|2023-09-05|     DL|   JFK| LAX|       10|       15|\n+----------+-------+------+----+---------+---------+\n\nFiltered data\n+----------+---+---+---+---+---+\n|       _c0|_c1|_c2|_c3|_c4|_c5|\n+----------+---+---+---+---+---+\n|2023-09-01| UA|LAX|JFK| -3|  0|\n|2023-09-02| AA|DFW|LAX|  0| -5|\n|2023-09-02| UA|JFK|ATL| -2|  0|\n|2023-09-03| UA|ATL|ORD|  0|-10|\n|2023-09-04| AA|JFK|LAX|  0|  0|\n|2023-09-04| DL|LAX|SFO| -5|-10|\n|2023-09-05| UA|DFW|ATL|  0|  0|\n+----------+---+---+---+---+---+\n\nGrouped data\n+---+--------+\n|_c1|avg(_c5)|\n+---+--------+\n| UA|    -2.5|\n| AA|    -2.5|\n| DL|   -10.0|\n+---+--------+\n\nMin,Max,Mean\n+---------+---------+-------------------+\n|Min_Delay|Max_Delay|         Mean_Delay|\n+---------+---------+-------------------+\n|      -10|        0|-3.5714285714285716|\n+---------+---------+-------------------+\n\nFlight count\n+---+-----+\n|_c1|count|\n+---+-----+\n| UA|    4|\n| AA|    2|\n| DL|    1|\n+---+-----+\n\nHistogram\n+---+\n|_c5|\n+---+\n|  0|\n| -5|\n|  0|\n|-10|\n|  0|\n|-10|\n|  0|\n+---+\n\n"
     ]
    }
   ],
   "source": [
    "#Task 1: Load the Dataset\n",
    "#Display the first 10 rows and inspect the schema of the dataset.\n",
    "\n",
    "# File location and type\n",
    "file_location = \"/FileStore/tables/data.csv\"\n",
    "file_type = \"csv\"\n",
    "\n",
    "# CSV options\n",
    "infer_schema = \"false\"\n",
    "first_row_is_header = \"false\"\n",
    "delimiter = \",\"\n",
    "\n",
    "# The applied options are for CSV files. For other file types, these will be ignored.\n",
    "df = spark.read.format(file_type) \\\n",
    "  .option(\"inferSchema\", infer_schema) \\\n",
    "  .option(\"header\", first_row_is_header) \\\n",
    "  .option(\"sep\", delimiter) \\\n",
    "  .load(file_location)\n",
    "\n",
    "display(df)\n",
    "\n",
    "# Show the first 10 rows\n",
    "print(\"Top 10 rows\")\n",
    "df.show(10)\n",
    "\n",
    "# Inspect the schema\n",
    "df.printSchema()\n",
    "\n",
    "#Task 2: Data Cleaning\n",
    "# Drop rows with null values\n",
    "df_cleaned = df.dropna()\n",
    "print(\"Cleaned data\")\n",
    "df_cleaned.show()\n",
    "\n",
    "# Filter rows where 'ARR DELAY' is less than or equal to zero\n",
    "df_filtered = df_cleaned.filter(df_cleaned['_c5'] <= 0)\n",
    "print(\"Filtered data\")\n",
    "df_filtered.show()\n",
    "\n",
    "#Task 3: Aggregation and Summary Statistics\n",
    "# Group by 'CARRIER' and calculate the average of 'ARR DELAY'\n",
    "df_grouped = df_filtered.groupBy('_c1').agg({'_c5': 'avg'})\n",
    "print(\"Grouped data\")\n",
    "df_grouped.show()\n",
    "\n",
    "# Calculate min, max, and mean for 'ARR DELAY'\n",
    "print(\"Min,Max,Mean\")\n",
    "df_filtered.selectExpr(\n",
    "    'min(`_c5`) as Min_Delay',\n",
    "    'max(`_c5`) as Max_Delay',\n",
    "    'mean(`_c5`) as Mean_Delay'\n",
    ").show()\n",
    "\n",
    "# Count the total number of flights per 'CARRIER'\n",
    "df_grouped_count = df_filtered.groupBy('_c1').count()\n",
    "print(\"Flight count\")\n",
    "df_grouped_count.show()\n",
    "\n",
    "# Histogram for the distribution of 'ARR DELAY'\n",
    "print(\"Histogram\")\n",
    "df_filtered.select('_c5').show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c8bc37d-db2f-4562-98a4-d26f15707683",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+------+----+---------+---------+\n|       _c0|    _c1|   _c2| _c3|      _c4|      _c5|\n+----------+-------+------+----+---------+---------+\n|   FL_DATE|CARRIER|ORIGIN|DEST|DEP_DELAY|ARR_DELAY|\n|2023-09-01|     AA|   ATL| DFW|        5|       10|\n|2023-09-01|     UA|   LAX| JFK|       -3|        0|\n|2023-09-01|     DL|   SFO| ORD|        7|       15|\n|2023-09-02|     AA|   DFW| LAX|        0|       -5|\n|2023-09-02|     UA|   JFK| ATL|       -2|        0|\n|2023-09-02|     DL|   ORD| LAX|       20|       30|\n|2023-09-03|     AA|   LAX| SFO|       10|       12|\n|2023-09-03|     UA|   ATL| ORD|        0|      -10|\n|2023-09-03|     DL|   SFO| JFK|        5|       25|\n|2023-09-04|     AA|   JFK| LAX|        0|        0|\n|2023-09-04|     UA|   ORD| ATL|       15|       20|\n|2023-09-04|     DL|   LAX| SFO|       -5|      -10|\n|2023-09-05|     AA|   LAX| JFK|       20|       25|\n|2023-09-05|     UA|   DFW| ATL|        0|        0|\n|2023-09-05|     DL|   JFK| LAX|       10|       15|\n+----------+-------+------+----+---------+---------+\n\n+----------+---+---+---+---+---+\n|       _c0|_c1|_c2|_c3|_c4|_c5|\n+----------+---+---+---+---+---+\n|2023-09-01| AA|ATL|DFW|  5| 10|\n|2023-09-01| DL|SFO|ORD|  7| 15|\n|2023-09-02| DL|ORD|LAX| 20| 30|\n|2023-09-03| AA|LAX|SFO| 10| 12|\n|2023-09-03| DL|SFO|JFK|  5| 25|\n|2023-09-04| UA|ORD|ATL| 15| 20|\n|2023-09-05| AA|LAX|JFK| 20| 25|\n|2023-09-05| DL|JFK|LAX| 10| 15|\n+----------+---+---+---+---+---+\n\n+-------+--------+\n|    _c1|avg(_c5)|\n+-------+--------+\n|     UA|     2.0|\n|     AA|     8.4|\n|     DL|    15.0|\n|CARRIER|    NULL|\n+-------+--------+\n\n+---------+-----+\n|      _c5|count|\n+---------+-----+\n|       15|    2|\n|       30|    1|\n|        0|    4|\n|      -10|    2|\n|       25|    2|\n|ARR_DELAY|    1|\n|       20|    1|\n|       10|    1|\n|       12|    1|\n|       -5|    1|\n+---------+-----+\n\n+---------+\n| max(_c5)|\n+---------+\n|ARR_DELAY|\n+---------+\n\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c1</th><th>avg(_c5)</th></tr></thead><tbody><tr><td>UA</td><td>2.0</td></tr><tr><td>AA</td><td>8.4</td></tr><tr><td>DL</td><td>15.0</td></tr><tr><td>CARRIER</td><td>null</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "UA",
         2.0
        ],
        [
         "AA",
         8.4
        ],
        [
         "DL",
         15.0
        ],
        [
         "CARRIER",
         null
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "avg(_c5)",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c1</th><th>count</th></tr></thead><tbody><tr><td>UA</td><td>5</td></tr><tr><td>AA</td><td>5</td></tr><tr><td>DL</td><td>5</td></tr><tr><td>CARRIER</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "UA",
         5
        ],
        [
         "AA",
         5
        ],
        [
         "DL",
         5
        ],
        [
         "CARRIER",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c1",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>_c5</th><th>count</th></tr></thead><tbody><tr><td>15</td><td>2</td></tr><tr><td>30</td><td>1</td></tr><tr><td>0</td><td>4</td></tr><tr><td>-10</td><td>2</td></tr><tr><td>25</td><td>2</td></tr><tr><td>ARR_DELAY</td><td>1</td></tr><tr><td>20</td><td>1</td></tr><tr><td>10</td><td>1</td></tr><tr><td>12</td><td>1</td></tr><tr><td>-5</td><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "15",
         2
        ],
        [
         "30",
         1
        ],
        [
         "0",
         4
        ],
        [
         "-10",
         2
        ],
        [
         "25",
         2
        ],
        [
         "ARR_DELAY",
         1
        ],
        [
         "20",
         1
        ],
        [
         "10",
         1
        ],
        [
         "12",
         1
        ],
        [
         "-5",
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "_c5",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "### **Task 3: Data Cleaning**\n",
    "\n",
    "#1. **Handle missing values**: Drop rows with missing values from the dataset.\n",
    "  # - Use `.na.drop()` to remove rows containing `null` values.\n",
    "  # - Verify if there are any null values left using `.filter()`.\n",
    "\n",
    "   #Example:\n",
    "   #```python\n",
    "df_cleaned = df.na.drop()\n",
    "df_cleaned.show()\n",
    "   #```\n",
    "\n",
    "#2. **Filter rows**: Create a filtered DataFrame where arrival delays are greater than `0`.\n",
    "   \n",
    " #  Example:\n",
    "  # ```python\n",
    "df_filtered = df.filter(df['_c5'] > 0)\n",
    "df_filtered.show()\n",
    "   #```\n",
    "\n",
    "#---\n",
    "\n",
    "### **Task 4: Aggregating and Summarizing the Data**\n",
    "\n",
    "#1. **Find the average arrival delay by airline**:\n",
    " #  - Group by `CARRIER` and calculate the average of `ARR_DELAY`.\n",
    "\n",
    "  # Example:\n",
    "   #```python\n",
    "df.groupBy(\"_c1\").agg({\"_c5\": \"avg\"}).show()\n",
    "#   ```\n",
    "\n",
    "#2. **Count the number of flights per airline**:\n",
    " #  - Group by `CARRIER` and count the total number of flights.\n",
    "\n",
    "  # Example:\n",
    "   #```python\n",
    "df.groupBy(\"_c5\").count().show()\n",
    "   #```\n",
    "\n",
    "#3. **Find the minimum and maximum delay** for all flights:\n",
    " #  - Use `.agg()` to calculate both the minimum and maximum delay.\n",
    "\n",
    "  # Example:\n",
    "  # ```python\n",
    "df.agg({\"_c5\": \"min\", \"_c5\": \"max\"}).show()\n",
    "   #```\n",
    "\n",
    "#---\n",
    "\n",
    "### **Task 5: Visualizing the Data**\n",
    "\n",
    "#1. **Plot the average delay per airline** using Databricks’ built-in visualization tools:\n",
    "   #- Use `display()` to visualize the result from the average delay aggregation.\n",
    "\n",
    "   ##Example:\n",
    "   #```python\n",
    "display(df.groupBy(\"_c1\").agg({\"_c5\": \"avg\"}))\n",
    "   #```\n",
    "\n",
    "#2. **Visualize flight count by airline** using a bar chart:\n",
    " #  - Use the `display()` function and convert the table into a bar chart in the UI.\n",
    "\n",
    "   #Example:\n",
    "  # ```python\n",
    "display(df.groupBy(\"_c1\").count())\n",
    "   #```\n",
    "\n",
    "#3. **Plot the distribution of arrival delays** using a histogram:\n",
    "   #- Group by `ARR_DELAY` and count, then use the Databricks visualization tool to create a histogram.\n",
    "\n",
    "   #Example:\n",
    "   #```python\n",
    "display(df.groupBy(\"_c5\").count())\n",
    "   #```\n",
    "\n",
    "#---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16a2506a-1263-4f3e-85da-bc77f09ff2a0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hii\n"
     ]
    }
   ],
   "source": [
    "print(\"hii\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Data Processing",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
