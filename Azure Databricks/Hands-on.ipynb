{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5552cfef-22c4-4d74-9829-a1a1c2d6e409",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n",
      "|EmployeeID|         Name|Department|JoiningDate|Salary|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|      1001|     John Doe|        HR| 2021-01-15| 55000|\n",
      "|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n",
      "|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n",
      "|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n",
      "|      1005| David Wilson|        IT| 2021-06-25| 58000|\n",
      "|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n",
      "|      1007| James Miller|        IT| 2019-08-14| 65000|\n",
      "|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "\n",
      "Top 10 rows\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|EmployeeID|         Name|Department|JoiningDate|Salary|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|      1001|     John Doe|        HR| 2021-01-15| 55000|\n",
      "|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n",
      "|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n",
      "|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n",
      "|      1005| David Wilson|        IT| 2021-06-25| 58000|\n",
      "|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n",
      "|      1007| James Miller|        IT| 2019-08-14| 65000|\n",
      "|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "\n",
      "root\n",
      " |-- EmployeeID: string (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Department: string (nullable = true)\n",
      " |-- JoiningDate: string (nullable = true)\n",
      " |-- Salary: string (nullable = true)\n",
      "\n",
      "Cleaned Data:\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|EmployeeID|         Name|Department|JoiningDate|Salary|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n",
      "|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n",
      "|      1005| David Wilson|        IT| 2021-06-25| 58000|\n",
      "|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n",
      "|      1007| James Miller|        IT| 2019-08-14| 65000|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "\n",
      "+----------+------------+----------+-----------+------+\n",
      "|EmployeeID|        Name|Department|JoiningDate|Salary|\n",
      "+----------+------------+----------+-----------+------+\n",
      "|      1005|David Wilson|        IT| 2021-06-25| 58000|\n",
      "+----------+------------+----------+-----------+------+\n",
      "\n",
      "Avg salary\n",
      "+----------+------------------+\n",
      "|Department|       avg(Salary)|\n",
      "+----------+------------------+\n",
      "|   Finance|           68500.0|\n",
      "|        IT|61666.666666666664|\n",
      "+----------+------------------+\n",
      "\n",
      "Department count\n",
      "+----------+-----+\n",
      "|Department|count|\n",
      "+----------+-----+\n",
      "|   Finance|    2|\n",
      "|        IT|    3|\n",
      "+----------+-----+\n",
      "\n",
      "Data written to CSV\n"
     ]
    }
   ],
   "source": [
    "#Assignment 1\n",
    "\n",
    "#Tasks:\n",
    "#1. Load the CSV data:\n",
    "\n",
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/employee_data.csv\",\"dbfs:/FileStore/employee_data.csv\")\n",
    "\n",
    "# Load the file from DBFS\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/employee_data.csv\")\n",
    "df.show()\n",
    "\n",
    "# Show the first 10 rows\n",
    "print(\"Top 10 rows\")\n",
    "df.show(10)\n",
    "\n",
    "# Inspect the schema\n",
    "df.printSchema()\n",
    "\n",
    "#2. Data Cleaning:\n",
    "\n",
    "# Remove rows where Salary is less than 55000\n",
    "df_cleaned = df.filter(df['Salary'] > 55000)\n",
    "\n",
    "# Filter employees who joined after the year 2020\n",
    "df_cleaned2 = df_cleaned.filter(df_cleaned['JoiningDate'] > '2020-12-31')\n",
    "\n",
    "# Show the cleaned data\n",
    "print(\"Cleaned Data:\")\n",
    "df_cleaned.show()\n",
    "df_cleaned2.show()\n",
    "\n",
    "#3. Data Aggregation:\n",
    "\n",
    "# Find the average salary by Department\n",
    "avg_salary = df_cleaned.groupBy('Department').agg({'Salary': 'avg'})\n",
    "print(\"Avg salary\")\n",
    "avg_salary.show()\n",
    "\n",
    "# Count the number of employees in each Department\n",
    "employee_count = df_cleaned.groupBy('Department').count()\n",
    "print(\"Department count\")\n",
    "employee_count.show()\n",
    "\n",
    "#4. Write the Data to CSV:\n",
    "df.write.format(\"csv\").option(\"header\",\"true\").save(\"/workspace/Shared/employeecsv_output\")\n",
    "print(\"Data written to CSV\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bfa6903b-7cc0-459d-8214-b963f583c4c0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+---------+-----------+-----+\n",
      "|   Category|Price|ProductID|ProductName|Stock|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|Electronics| 1200|      101|     Laptop|   35|\n",
      "|Electronics|  800|      102| Smartphone|   80|\n",
      "|  Furniture|  150|      103| Desk Chair|   60|\n",
      "|Electronics|  300|      104|    Monitor|   45|\n",
      "|  Furniture|  350|      105|       Desk|   25|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "\n",
      "Top 10 rows\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|   Category|Price|ProductID|ProductName|Stock|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|Electronics| 1200|      101|     Laptop|   35|\n",
      "|Electronics|  800|      102| Smartphone|   80|\n",
      "|  Furniture|  150|      103| Desk Chair|   60|\n",
      "|Electronics|  300|      104|    Monitor|   45|\n",
      "|  Furniture|  350|      105|       Desk|   25|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "\n",
      "root\n",
      " |-- Category: string (nullable = true)\n",
      " |-- Price: long (nullable = true)\n",
      " |-- ProductID: long (nullable = true)\n",
      " |-- ProductName: string (nullable = true)\n",
      " |-- Stock: long (nullable = true)\n",
      "\n",
      "Cleaned Data:\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|   Category|Price|ProductID|ProductName|Stock|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|Electronics| 1200|      101|     Laptop|   35|\n",
      "|Electronics|  800|      102| Smartphone|   80|\n",
      "|Electronics|  300|      104|    Monitor|   45|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "\n",
      "Total Stock\n",
      "+---------+----------+\n",
      "| Category|TotalStock|\n",
      "+---------+----------+\n",
      "|Furniture|        85|\n",
      "+---------+----------+\n",
      "\n",
      "Average Price\n",
      "+-----------+-----------------+\n",
      "|   Category|         AvgPrice|\n",
      "+-----------+-----------------+\n",
      "|Electronics|766.6666666666666|\n",
      "|  Furniture|            250.0|\n",
      "+-----------+-----------------+\n",
      "\n",
      "Data written to JSON\n"
     ]
    }
   ],
   "source": [
    "#Assignment 2\n",
    "\n",
    "#Tasks:\n",
    "#1. Load the JSON data:\n",
    "\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/product_data.json\", \"dbfs:/FileStore/product_data.json\")\n",
    "\n",
    "# Load \n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/product_data.json\")\n",
    "df.show()\n",
    "\n",
    "# Show the first 10 rows\n",
    "print(\"Top 10 rows\")\n",
    "df.show(10)\n",
    "\n",
    "# Inspect the schema\n",
    "df.printSchema()\n",
    "\n",
    "#2. Data Cleaning:\n",
    "\n",
    "#Remove rows where Stock is less than 30.\n",
    "df_cleaned = df.filter(df['Stock'] >= 30)  \n",
    "\n",
    "#Filter the products that belong to the \"Electronics\" category.\n",
    "df_cleaned = df.filter(df['Category'] == 'Electronics')\n",
    "\n",
    "#show\n",
    "print(\"Cleaned Data:\")\n",
    "df_cleaned.show()\n",
    "\n",
    "#3. Data Aggregation:\n",
    "\n",
    "#Calculate the total stock for products in the \"Furniture\" category.\n",
    "total_stock = df.filter(df['Category'] == 'Furniture').groupBy('Category').agg({'Stock': 'sum'}).withColumnRenamed('sum(Stock)', 'TotalStock')\n",
    "print(\"Total Stock\")\n",
    "total_stock.show()\n",
    "\n",
    "#Find the average price of all products in the dataset.\n",
    "avg_price = df.groupBy('Category').agg({'Price': 'avg'}).withColumnRenamed('avg(Price)', 'AvgPrice')\n",
    "print(\"Average Price\")\n",
    "avg_price.show()\n",
    "\n",
    "#4. Write the Data to JSON:\n",
    "df_cleaned.coalesce(1).write.json('/FileStore/product_data_cleaned.json')\n",
    "print(\"Data written to JSON\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b891f79-2edc-411d-ad9c-b80a4aa6492c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+----------+-----------+------+\n",
      "|EmployeeID|         Name|Department|JoiningDate|Salary|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|      1001|     John Doe|        HR| 2021-01-15| 55000|\n",
      "|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n",
      "|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n",
      "|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n",
      "|      1005| David Wilson|        IT| 2021-06-25| 58000|\n",
      "|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n",
      "|      1007| James Miller|        IT| 2019-08-14| 65000|\n",
      "|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|   Category|Price|ProductID|ProductName|Stock|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|Electronics| 1200|      101|     Laptop|   35|\n",
      "|Electronics|  800|      102| Smartphone|   80|\n",
      "|  Furniture|  150|      103| Desk Chair|   60|\n",
      "|Electronics|  300|      104|    Monitor|   45|\n",
      "|  Furniture|  350|      105|       Desk|   25|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "\n",
      "Data saved\n",
      "Registered\n",
      "Data modified\n",
      "Data retrieved\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|   Category|Price|ProductID|ProductName|Stock|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|Electronics| 1200|      101|     Laptop|   35|\n",
      "|Electronics|  800|      102| Smartphone|   80|\n",
      "|  Furniture|  150|      103| Desk Chair|   60|\n",
      "|Electronics|  300|      104|    Monitor|   45|\n",
      "|  Furniture|  350|      105|       Desk|   25|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "\n",
      "Data retrieved\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|EmployeeID|         Name|Department|JoiningDate|Salary|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|      1001|     John Doe|        HR| 2021-01-15| 55000|\n",
      "|      1002|   Jane Smith|        IT| 2020-03-10| 62000|\n",
      "|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n",
      "|      1004|Michael Brown|        HR| 2018-12-22| 54000|\n",
      "|      1005| David Wilson|        IT| 2021-06-25| 58000|\n",
      "|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n",
      "|      1007| James Miller|        IT| 2019-08-14| 65000|\n",
      "|      1008|Barbara Moore|        HR| 2021-03-29| 53000|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "\n",
      "employees in Finance department\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|EmployeeID|         Name|Department|JoiningDate|Salary|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "|      1003|Emily Johnson|   Finance| 2019-07-01| 70000|\n",
      "|      1006|  Linda Davis|   Finance| 2020-11-15| 67000|\n",
      "+----------+-------------+----------+-----------+------+\n",
      "\n",
      "expensive electronics\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|   Category|Price|ProductID|ProductName|Stock|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "|Electronics|  800|      102| Smartphone|   80|\n",
      "+-----------+-----+---------+-----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Assignment 3:\n",
    "\n",
    "# Tasks:\n",
    "\n",
    "# 1. Convert CSV and JSON Data to Delta Format:\n",
    "\n",
    "# Load employee.csv file data\n",
    "df_employee = spark.read.csv('/FileStore/employee_data.csv', header=True, inferSchema=True).cache()\n",
    "df_employee.show()\n",
    "\n",
    "# Load product_data.json file\n",
    "df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/product_data.json\")\n",
    "df.show()\n",
    "\n",
    "#save data\n",
    "df_employee.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/delta/employee_data\")\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/dbfs/FileStore/delta/product_data\")\n",
    "print(\"Data saved\")\n",
    "\n",
    "#2. Register Delta Tables:\n",
    "\n",
    "print(\"Registered\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS employee_delta USING DELTA LOCATION '/dbfs/FileStore/delta/employee_data'\")\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS product_delta USING DELTA LOCATION '/dbfs/FileStore/delta/product_data'\")\n",
    "\n",
    "#3. Data Modifications with Delta Tables:\n",
    "\n",
    "# Increase salary by 5% for IT department employees\n",
    "spark.sql(\"UPDATE employee_delta SET Salary = Salary * 1.05 WHERE Department = 'IT'\")\n",
    "\n",
    "# Delete products where stock is less than 40\n",
    "spark.sql(\"DELETE FROM product_delta WHERE Stock < 40\")\n",
    "print(\"Data modified\")\n",
    "\n",
    "#4. Time Travel with Delta Tables:\n",
    "\n",
    "# Query the product Delta table to show its state before the delete\n",
    "# operation (use time travel).\n",
    "print(\"Data retrieved\")\n",
    "df_product_version_before_delete = spark.sql(\"SELECT * FROM product_delta VERSION AS OF 0\")\n",
    "df_product_version_before_delete.show()\n",
    "\n",
    "# Retrieve the version of the employee Delta table before the salary update.\n",
    "print(\"Data retrieved\")\n",
    "df_employee_version_before_update = spark.sql(\"SELECT * FROM employee_delta VERSION AS OF 0\")\n",
    "df_employee_version_before_update.show()\n",
    "\n",
    "#5. Query Delta Tables:\n",
    "\n",
    "# Query the employee Delta table to find the employees in the Finance department.\n",
    "print(\"employees in Finance department\")\n",
    "df_finance_employees = spark.sql(\"SELECT * FROM employee_delta WHERE Department = 'Finance'\")\n",
    "df_finance_employees.show()\n",
    "\n",
    "# Query the product Delta table to find all products in the Electronics category with a price greater than 500.\n",
    "print(\"expensive electronics\")\n",
    "df_expensive_electronics = spark.sql(\"SELECT * FROM product_delta WHERE Category = 'Electronics' AND Price > 500\")\n",
    "df_expensive_electronics.show()\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f97fe95-d8d2-406f-9e28-c8f0fcf936f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Databricks Delta Lake Exercise\n",
    "#Tasks\n",
    "#1. Create Delta Tables Using 3 Methods\n",
    "\n",
    "#1. Load the sales_data.csv file into a DataFrame.\n",
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/Sales_data.csv\",\"dbfs:/FileStore/Sales_data.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6a8962e3-b187-40ed-a63b-84ceb4247e9b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1001|2024-01-15|      C001|Widget A|      10|25.50|\n",
      "|   1002|2024-01-16|      C002|Widget B|       5|15.75|\n",
      "|   1003|2024-01-16|      C001|Widget C|       8|22.50|\n",
      "|   1004|2024-01-17|      C003|Widget A|      15|25.50|\n",
      "|   1005|2024-01-18|      C004|Widget D|       7|30.00|\n",
      "|   1006|2024-01-19|      C002|Widget B|       9|15.75|\n",
      "|   1007|2024-01-20|      C005|Widget C|      12|22.50|\n",
      "|   1008|2024-01-21|      C003|Widget A|      10|25.50|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file from DBFS\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/Sales_data.csv\")\n",
    "sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4a2d623e-afe3-4f36-bc8f-7f59b97b6868",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Write the DataFrame as a Delta Table.\n",
    "sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/sales_delta\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e9b9421-753c-4299-816a-33cd4c6c3df9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Load the customer_data.json file into a DataFrame.\n",
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/customer_data.json\", \"dbfs:/FileStore/customer_data.json\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3312fc5f-80bf-471e-b65c-f07ad6e982fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------+------+----------+\n",
      "|CustomerID| CustomerName|Region|SignupDate|\n",
      "+----------+-------------+------+----------+\n",
      "|      C001|     John Doe| North|2022-07-01|\n",
      "|      C002|   Jane Smith| South|2023-02-15|\n",
      "|      C003|Emily Johnson|  East|2021-11-20|\n",
      "|      C004|Michael Brown|  West|2022-12-05|\n",
      "|      C005|  Linda Davis| North|2023-03-10|\n",
      "+----------+-------------+------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the JSON file from DBFS\n",
    "customer_df = spark.read.option(\"multiline\", \"true\").json(\"/FileStore/customer_data.json\")\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77985f1d-f964-4237-a70d-3a80330aa15a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Write the DataFrame as a Delta Table.\n",
    "customer_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/customer_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "982c4122-5a41-4305-bb4f-4b6a700d1d10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Convert an existing Parquet file into a Delta Table\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"/FileStore/customer_data.json\")\n",
    "\n",
    "parquet_df = spark.read.format(\"parquet\").load(\"/FileStore/customer_data.json\")\n",
    "parquet_df.write.format(\"delta\").save(\"/delta/customer_data_parquet_to_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a69710d-b110-496a-83b1-30cc7ebf26a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Data Management\n",
    "#1. Load the new_sales_data.csv file into a DataFrame\n",
    "# Move the file from Workspace to DBFS\n",
    "dbutils.fs.cp(\"file:/Workspace/Shared/new_sales_data.csv\",\"dbfs:/FileStore/new_sales_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01549a25-b2f5-4fc8-98a1-35c559f8d10c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+--------+--------+-----+\n",
      "|OrderID| OrderDate|CustomerID| Product|Quantity|Price|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "|   1009|2024-01-22|      C006|Widget E|      14|20.00|\n",
      "|   1010|2024-01-23|      C007|Widget F|       6|35.00|\n",
      "|   1002|2024-01-16|      C002|Widget B|      10|15.75|\n",
      "+-------+----------+----------+--------+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load the new_sales_data.csv file from DBFS\n",
    "new_sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/new_sales_data.csv\")\n",
    "new_sales_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ae2a154-abc5-4939-9686-066ce38e7b10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Write the new DataFrame as a Delta Table.\n",
    "new_sales_df.write.format(\"delta\").mode(\"overwrite\").save(\"/FileStore/delta/new_sales_delta\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18236375-ba83-40a3-b58e-6f117b0ff552",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the sales_data.csv into a DataFrame\n",
    "sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/Sales_data.csv\")\n",
    "\n",
    "# Convert the DataFrame to a Delta Table\n",
    "sales_df.write.format(\"delta\").save(\"/delta/sales_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff22a5e9-ef19-417b-9be3-e48b9490b479",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Perform a MERGE INTO operation to update and insert records into the existing Delta table.\n",
    "from delta.tables import *\n",
    "\n",
    "# Load the Delta Table\n",
    "sales_delta = DeltaTable.forPath(spark, \"/delta/sales_data\")\n",
    "\n",
    "# Load new sales data\n",
    "new_sales_df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"/FileStore/new_sales_data.csv\")\n",
    "\n",
    "# Perform the MERGE INTO operation\n",
    "sales_delta.alias(\"old\").merge(\n",
    "    new_sales_df.alias(\"new\"),\n",
    "    \"old.OrderID = new.OrderID\"\n",
    ").whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30d44814-df65-4e3c-92d0-b60c8ef93f16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a new Delta Table\n",
    "spark.sql(\"CREATE TABLE IF NOT EXISTS sales_data_delta USING DELTA LOCATION '/delta/sales_data'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "179957c9-5236-4803-a76a-47a132c741f9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[OrderID: string, OrderDate: string, CustomerID: string, Product: string, Quantity: string, Price: string]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#diaplay table\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM sales_data_delta\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ef2a964c-3135-4101-b58f-24fa01b6f396",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,clusteringStats:struct<inputZCubeFiles:struct<numFiles:bigint,size:bigint>,inputOtherFiles:struct<numFiles:bigint,size:bigint>,inputNumZCubes:bigint,mergedFiles:struct<numFiles:bigint,size:bigint>,numOutputZCubes:bigint>,numBins:bigint,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint,skippedArchivedFiles:bigint,clusteringMetrics:struct<sizeOfTableInBytesBeforeLazyClustering:bigint,isNewMetadataCreated:boolean,isPOTriggered:boolean,numFilesSkippedWithoutStats:bigint,numFilesClassifiedToIntermediateNodes:bigint,sizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,logicalSizeOfFilesClassifiedToIntermediateNodesInBytes:bigint,numFilesClassifiedToLeafNodes:bigint,sizeOfFilesClassifiedToLeafNodesInBytes:bigint,logicalSizeOfFilesClassifiedToLeafNodesInBytes:bigint,numThreadsForClassifier:int,clusterThresholdStrategy:string,minFileSize:bigint,maxFileSize:bigint,nodeMinNumFilesToCompact:bigint,numIdealFiles:bigint,numClusteringTasksPlanned:int,numCompactionTasksPlanned:int,numOptimizeBatchesPlanned:int,numLeafNodesExpanded:bigint,numLeafNodesClustered:bigint,numGetFilesForNodeCalls:bigint,numSamplingJobs:bigint,numLeafNodesCompacted:bigint,numIntermediateNodesCompacted:bigint,totalSizeOfDataToCompactInBytes:bigint,totalLogicalSizeOfDataToCompactInBytes:bigint,numIntermediateNodesClustered:bigint,numFilesSkippedAfterExpansion:bigint,totalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalLogicalSizeOfFilesSkippedAfterExpansionInBytes:bigint,totalSizeOfDataToRewriteInBytes:bigint,totalLogicalSizeOfDataToRewriteInBytes:bigint,timeMetrics:struct<classifierTimeMs:bigint,optimizerTimeMs:bigint,metadataLoadTimeMs:bigint,totalGetFilesForNodeCallsTimeMs:bigint,totalSamplingTimeMs:bigint,metadataCreationTimeMs:bigint>,maxOptimizeBatchesInParallel:bigint,currentIteration:int,maxIterations:int,clusteringStrategy:string>>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#3. Optimize Delta Table\n",
    "#1. Apply the OPTIMIZE command on the Delta Table\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE delta.`/delta/sales_data` ZORDER BY (CustomerID)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b91295a-61a5-4de5-820b-8b83976a6f39",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#4. Advanced Features\n",
    "#1. Use DESCRIBE HISTORY to inspect the history of changes for a Delta Table.\n",
    "spark.sql(\"\"\"\n",
    "    DESCRIBE HISTORY delta.`/delta/sales_data`\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ebf3a48-1132-4d1f-abaa-6dc557722ff4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[path: string]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. Use VACUUM to remove old files from the Delta Table.\n",
    "spark.sql(\"\"\"\n",
    "    VACUUM delta.`/delta/sales_data` RETAIN 168 HOURS\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5a35c4-aee6-4849-bfc9-cf746c1544a7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[OrderID: string, OrderDate: string, CustomerID: string, Product: string, Quantity: string, Price: string]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#5. Hands-on Exercises\n",
    "#1. Using Delta Lake for Data Versioning:\n",
    "#Query historical versions of the Delta Table using Time Travel.\n",
    "spark.sql(\"\"\"\n",
    "    SELECT * FROM delta.`/delta/sales_data` VERSION AS OF 1\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d65610e8-4aef-4fb7-80ea-279196e2ddd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Building a Reliable Data Lake with Delta Lake:\n",
    "#Implement schema enforcement and handle data updates with Delta Lake.\n",
    "df.write.format(\"delta\").mode(\"append\").option(\"mergeSchema\", \"true\").save(\"/delta/sales_data\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Hands-on",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
