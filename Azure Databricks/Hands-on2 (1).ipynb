{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ea10373-18c2-46b0-82dd-e97c3d8836b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Databricks Exercise\n",
    "\n",
    "#1. Introduction to Databricks\n",
    "#Task: Creating a Databricks Notebook\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df1 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/teacher_data-1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee0bfd1d-febe-4b9f-93ce-a320b8df4205",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-------+-----------------+------+\n|TeacherID|         Name|Subject|YearsOfExperience|Salary|\n+---------+-------------+-------+-----------------+------+\n|        1|     John Doe|   Math|               10| 55000|\n|        2|   Jane Smith|English|                8| 52000|\n|        3| Mark Johnson|Science|               12| 60000|\n|        4|  Emily Davis|History|                5| 48000|\n|        5|Michael Brown|   Math|               15| 62000|\n+---------+-------------+-------+-----------------+------+\nonly showing top 5 rows\n\nroot\n |-- TeacherID: string (nullable = true)\n |-- Name: string (nullable = true)\n |-- Subject: string (nullable = true)\n |-- YearsOfExperience: string (nullable = true)\n |-- Salary: string (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#Performing DataFrame Operations\n",
    "\n",
    "# Show the first few rows of the DataFrame\n",
    "df1.show(5)\n",
    "\n",
    "# Display the schema of the DataFrame\n",
    "df1.printSchema()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9edb6b2-5799-49e8-b1a3-fc0a4f6bffd3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the DataFrame: 10\nexperienced teachers\n+---------+-------------+-------+-----------------+------+\n|TeacherID|         Name|Subject|YearsOfExperience|Salary|\n+---------+-------------+-------+-----------------+------+\n|        3| Mark Johnson|Science|               12| 60000|\n|        5|Michael Brown|   Math|               15| 62000|\n|        9|   James Hall|   Math|               11| 58000|\n+---------+-------------+-------+-----------------+------+\n\nAvg salary\n+-------+------------------+\n|Subject|       avg(Salary)|\n+-------+------------------+\n|Science|           55500.0|\n|   Math|58333.333333333336|\n|English|           52000.0|\n|History|           47500.0|\n+-------+------------------+\n\n+---------+-------------+-------+-----------------+------+\n|TeacherID|         Name|Subject|YearsOfExperience|Salary|\n+---------+-------------+-------+-----------------+------+\n|        5|Michael Brown|   Math|               15| 62000|\n|        3| Mark Johnson|Science|               12| 60000|\n|        9|   James Hall|   Math|               11| 58000|\n|        1|     John Doe|   Math|               10| 55000|\n|        6| Laura Wilson|English|                9| 54000|\n|        2|   Jane Smith|English|                8| 52000|\n|        7| Robert White|Science|                7| 51000|\n|       10|   Sarah King|English|                6| 50000|\n|        4|  Emily Davis|History|                5| 48000|\n|        8|  Linda Green|History|                4| 47000|\n+---------+-------------+-------+-----------------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Count the number of rows in the DataFrame\n",
    "row_count = df1.count()\n",
    "print(f\"Number of rows in the DataFrame: {row_count}\")\n",
    "\n",
    "# Filter the DataFrame for teachers with more than 10 years of experience\n",
    "experienced_teachers = df1.filter(df1[\"YearsOfExperience\"] > 10)\n",
    "print(\"experienced teachers\")\n",
    "experienced_teachers.show()\n",
    "\n",
    "# Group by subject and calculate the average salary for each subject\n",
    "avg_salary = df1.groupBy('Subject').agg({'Salary': 'avg'})\n",
    "print(\"Avg salary\")\n",
    "avg_salary.show()\n",
    "\n",
    "# Sort the DataFrame by salary in descending order\n",
    "df1.orderBy(df1[\"Salary\"].desc()).show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81d1dcea-129c-4006-ad84-2f82e6a64b43",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"/?o=4215158048388427#setting/sparkui/0915-120127-7rswl9xc/driver-3838952452065686731\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[8]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Databricks Shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "\n            <div>\n                <p><b>SparkSession - hive</b></p>\n                \n        <div>\n            <p><b>SparkContext</b></p>\n\n            <p><a href=\"/?o=4215158048388427#setting/sparkui/0915-120127-7rswl9xc/driver-3838952452065686731\">Spark UI</a></p>\n\n            <dl>\n              <dt>Version</dt>\n                <dd><code>v3.3.2</code></dd>\n              <dt>Master</dt>\n                <dd><code>local[8]</code></dd>\n              <dt>AppName</dt>\n                <dd><code>Databricks Shell</code></dd>\n            </dl>\n        </div>\n        \n            </div>\n        ",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "textData": null,
       "type": "htmlSandbox"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2. Setting Up Azure Databricks Workspace and Configuring Clusters\n",
    "#Task: Configuring Clusters\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1b81d133-32f2-4f66-bcb3-109474e5a8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------+------------------+\n|         event_time|event_type|user_id|            amount|\n+-------------------+----------+-------+------------------+\n|2024-09-01 18:46:00|      view|    125|               0.0|\n|2024-09-01 14:20:00|      view|    590|               0.0|\n|2024-09-01 21:34:00|     click|    626|214.32524639559162|\n|2024-09-01 18:50:00|  purchase|    847| 82.90346160818179|\n|2024-09-01 18:15:00|      view|    861|               0.0|\n|2024-09-01 17:24:00|     click|    338| 760.3444131051176|\n|2024-09-01 02:01:00|  purchase|    466| 708.8959146777304|\n|2024-09-01 07:46:00|  purchase|    503| 842.3751935682646|\n|2024-09-01 20:38:00|     click|    435|  887.214593409447|\n|2024-09-01 05:30:00|  purchase|     91|  847.908033341641|\n|2024-09-01 01:27:00|  purchase|    375|454.64873191440034|\n|2024-09-01 23:16:00|     click|    889|380.58625997139364|\n|2024-09-01 18:43:00|  purchase|     60|111.30156572734003|\n|2024-09-01 14:31:00|     click|    499| 993.1213927385505|\n|2024-09-01 02:10:00|      view|    836|               0.0|\n|2024-09-01 22:12:00|  purchase|    349| 506.2652143289465|\n|2024-09-01 12:49:00|      view|    564|               0.0|\n|2024-09-01 05:43:00|     click|    699|27.815051301618634|\n|2024-09-01 23:57:00|  purchase|    904| 519.7424221880517|\n|2024-09-01 13:25:00|  purchase|    612| 746.3459090481603|\n+-------------------+----------+-------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "#3. Real-Time Data Processing with Databricks\n",
    "#Task: Implementing Databricks for Real-Time Data Processing\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "df2 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_1.csv\")\n",
    "\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c37755f4-1dc5-4ee9-89ae-323696af2910",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- event_time: timestamp (nullable = true)\n |-- event_type: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- amount: float (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Define the schema for the streaming data\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, FloatType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"event_time\", TimestampType(), True),\n",
    "    StructField(\"event_type\", StringType(), True),\n",
    "    StructField(\"user_id\", StringType(), True),\n",
    "    StructField(\"amount\", FloatType(), True)\n",
    "])\n",
    "\n",
    "# Read the CSV file as a streaming DataFrame\n",
    "input_path = \"/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_1.csv\"\n",
    "\n",
    "# Read the file as if it's a stream\n",
    "df_stream = spark.readStream \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(schema) \\\n",
    "    .csv(input_path)\n",
    "\n",
    "# Display the streaming DataFrame (for debugging purposes)\n",
    "df_stream.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "84584f5e-8ef8-46b3-9055-dc3973c4bdfb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1012568290716912>:19\u001B[0m\n",
       "\u001B[1;32m     12\u001B[0m query \u001B[38;5;241m=\u001B[39m aggregated_stream\u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtruncate\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Wait for the streaming query to terminate\u001B[39;00m\n",
       "\u001B[0;32m---> 19\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:198\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n",
       "\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n",
       "\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m--> 198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 65ec29ff-84e2-497e-9cfc-b8db69e7681d, runId = c5aaa45c-f847-48bd-9cf8-06bbcf35aa98] terminated with exception: Option 'basePath' must be a directory"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mStreamingQueryException\u001B[0m                   Traceback (most recent call last)\nFile \u001B[0;32m<command-1012568290716912>:19\u001B[0m\n\u001B[1;32m     12\u001B[0m query \u001B[38;5;241m=\u001B[39m aggregated_stream\u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcomplete\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     14\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mconsole\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     15\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtruncate\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfalse\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     16\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     18\u001B[0m \u001B[38;5;66;03m# Wait for the streaming query to terminate\u001B[39;00m\n\u001B[0;32m---> 19\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/query.py:198\u001B[0m, in \u001B[0;36mStreamingQuery.awaitTermination\u001B[0;34m(self, timeout)\u001B[0m\n\u001B[1;32m    196\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jsq\u001B[38;5;241m.\u001B[39mawaitTermination(\u001B[38;5;28mint\u001B[39m(timeout \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m1000\u001B[39m))\n\u001B[1;32m    197\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m--> 198\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsq\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mawaitTermination\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mStreamingQueryException\u001B[0m: [STREAM_FAILED] Query [id = 65ec29ff-84e2-497e-9cfc-b8db69e7681d, runId = c5aaa45c-f847-48bd-9cf8-06bbcf35aa98] terminated with exception: Option 'basePath' must be a directory",
       "errorSummary": "<span class='ansi-red-fg'>StreamingQueryException</span>: [STREAM_FAILED] Query [id = 65ec29ff-84e2-497e-9cfc-b8db69e7681d, runId = c5aaa45c-f847-48bd-9cf8-06bbcf35aa98] terminated with exception: Option 'basePath' must be a directory",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import window, sum\n",
    "\n",
    "# Real-time aggregation: Sum amount by event_type and per minute\n",
    "aggregated_stream = df_stream \\\n",
    "    .groupBy(\n",
    "        window(\"event_time\", \"1 minute\"),  # Group by 1-minute window\n",
    "        \"event_type\"\n",
    "    ) \\\n",
    "    .agg(sum(\"amount\").alias(\"total_amount\"))\n",
    "\n",
    "# Output the real-time aggregation to the console\n",
    "query = aggregated_stream.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the streaming query to terminate\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e63df7b7-66c8-48ee-9c63-f0163d09b706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Real-time aggregation: Sum amount by event_type and per minute\n",
    "schema=\"event_time TIMESTAMP, event_type STRING, user_id STRING, amount DOUBLE\"\n",
    "streamind_data=spark.readStream.format(\"csv\").schema(schema).option(\"header\",\"true\").load(\"dbfs:/FileStore/\")\n",
    "# Real-time aggregation\n",
    "aggregated_data=(streamind_data.groupBy(\"event_type\").agg({\"amount\":\"sum\"}).withColumnRenamed(\"sum(amount)\",\"total_amount\"))\n",
    "query = (aggregated_data.writeStream\n",
    "         .outputMode(\"complete\")\n",
    "         .format(\"console\")\n",
    "         .start())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac707dc1-f469-4ad2-81ef-1339619123b0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2812599f-e9ab-4b08-bfa1-a61fa5e2f847",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----------+------+--------+-----------+----------------+-----------+\n|transaction_id|product_id| price|quantity|   category|transaction_date|customer_id|\n+--------------+----------+------+--------+-----------+----------------+-----------+\n|             1|       150|299.99|       2|Electronics|      2024-01-15|       1005|\n|             2|       120| 19.99|       5|    Grocery|      2024-02-10|       1012|\n|             3|       135| 49.50|       3|   Clothing|      2024-03-18|       1003|\n|             4|       180|899.99|       1|  Furniture|      2024-04-21|       1023|\n|             5|       160| 15.00|       7|       Toys|      2024-05-05|       1007|\n|             6|       145| 79.99|       4|Electronics|      2024-06-02|       1015|\n|             7|       170|129.99|       2|   Clothing|      2024-07-15|       1027|\n|             8|       190|350.00|       1|  Furniture|      2024-08-09|       1030|\n|             9|       110|  5.99|      10|    Grocery|      2024-09-01|       1010|\n|            10|       175| 45.50|       5|       Toys|      2024-09-12|       1008|\n+--------------+----------+------+--------+-----------+----------------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#4. Data Exploration and Visualization in Databricks\n",
    "#Task: Visualizing Data in Databricks\n",
    "df3 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/Sales_data_1.csv\")\n",
    "\n",
    "df3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d95768-7fca-4ad7-b1dc-6f1239e1bf30",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- transaction_id: string (nullable = true)\n |-- product_id: string (nullable = true)\n |-- price: string (nullable = true)\n |-- quantity: string (nullable = true)\n |-- category: string (nullable = true)\n |-- transaction_date: string (nullable = true)\n |-- customer_id: string (nullable = true)\n\n+-------+------------------+------------------+------------------+------------------+--------+----------------+---------------+\n|summary|    transaction_id|        product_id|             price|          quantity|category|transaction_date|    customer_id|\n+-------+------------------+------------------+------------------+------------------+--------+----------------+---------------+\n|  count|                10|                10|                10|                10|      10|              10|             10|\n|   mean|               5.5|             153.5|           189.594|               4.0|    null|            null|         1014.0|\n| stddev|3.0276503540974917|26.357583770562545|277.18704720418987|2.8674417556808756|    null|            null|9.5102284117914|\n|    min|                 1|               110|            129.99|                 1|Clothing|      2024-01-15|           1003|\n|    max|                 9|               190|            899.99|                 7|    Toys|      2024-09-12|           1030|\n+-------+------------------+------------------+------------------+------------------+--------+----------------+---------------+\n\nTotal rows in the dataset: 10\n"
     ]
    }
   ],
   "source": [
    "# Display schema of the DataFrame\n",
    "df3.printSchema()\n",
    "\n",
    "# Describe to see summary statistics of numerical columns\n",
    "df3.describe().show()\n",
    "\n",
    "# Count the total number of rows\n",
    "row_count = df3.count()\n",
    "print(f\"Total rows in the dataset: {row_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77c36274-6720-4e4e-8af2-77ad785a4dff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>category</th><th>total_sales</th></tr></thead><tbody><tr><td>Furniture</td><td>1249.99</td></tr><tr><td>Electronics</td><td>919.94</td></tr><tr><td>Clothing</td><td>408.48</td></tr><tr><td>Toys</td><td>332.5</td></tr><tr><td>Grocery</td><td>159.85</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Furniture",
         1249.99
        ],
        [
         "Electronics",
         919.94
        ],
        [
         "Clothing",
         408.48
        ],
        [
         "Toys",
         332.5
        ],
        [
         "Grocery",
         159.85
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "category",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "total_sales",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>price</th><th>quantity</th></tr></thead><tbody><tr><td>299.99</td><td>2</td></tr><tr><td>19.99</td><td>5</td></tr><tr><td>49.50</td><td>3</td></tr><tr><td>899.99</td><td>1</td></tr><tr><td>15.00</td><td>7</td></tr><tr><td>79.99</td><td>4</td></tr><tr><td>129.99</td><td>2</td></tr><tr><td>350.00</td><td>1</td></tr><tr><td>5.99</td><td>10</td></tr><tr><td>45.50</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "299.99",
         "2"
        ],
        [
         "19.99",
         "5"
        ],
        [
         "49.50",
         "3"
        ],
        [
         "899.99",
         "1"
        ],
        [
         "15.00",
         "7"
        ],
        [
         "79.99",
         "4"
        ],
        [
         "129.99",
         "2"
        ],
        [
         "350.00",
         "1"
        ],
        [
         "5.99",
         "10"
        ],
        [
         "45.50",
         "5"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "price",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "quantity",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Calculate total sales by category\n",
    "sales_by_category = df3.withColumn(\"total_sales\", col(\"price\") * col(\"quantity\")) \\\n",
    "    .groupBy(\"category\") \\\n",
    "    .agg(sum(\"total_sales\").alias(\"total_sales\")) \\\n",
    "    .orderBy(\"total_sales\", ascending=False)\n",
    "\n",
    "# Display the result as a table\n",
    "display(sales_by_category)\n",
    "\n",
    "df3.select(\"price\", \"quantity\").display()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80a5e986-2fe0-4953-acd0-8025ec4950af",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "image/png": "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\n"
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "data:image/png;base64,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\n",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "image"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert Spark DataFrame to Pandas for visualization\n",
    "sales_by_category_pd = sales_by_category.toPandas()\n",
    "\n",
    "# Create a bar plot\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.barplot(x='category', y='total_sales', data=sales_by_category_pd)\n",
    "plt.title(\"Total Sales by Category\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5d56910-c9ca-41ea-9059-67c2be61aebf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n|2024-09-03| South|Widget B|      20|15.75|\n|2024-09-03|  East|Widget C|      10|22.50|\n|2024-09-04| North|Widget D|       7|30.00|\n|2024-09-04|  West|Widget B|       9|15.75|\n+----------+------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#6. Reading and Writing Data in Databricks\n",
    "#Task: Reading and Writing Data in Various Formats\n",
    "\n",
    "# Reading CSV file\n",
    "df4 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/sales_data.csv\")\n",
    "\n",
    "df4.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92da3040-1098-44be-bc9c-14b2693d8da8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to json file format\n"
     ]
    }
   ],
   "source": [
    "# Writing to JSON\n",
    "df4.write.json(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/sales_data_json\")\n",
    "print(\"Written to json file format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1cee6d4d-3ef2-411b-bdf8-758a4025a5ae",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to parquet file format\n"
     ]
    }
   ],
   "source": [
    "# Writing to Parquet\n",
    "df4.write.parquet(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/sales_data_parquet\")\n",
    "print(\"Written to parquet file format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c760f377-d992-4996-ba55-26904d16cf5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Written to delta table format\n"
     ]
    }
   ],
   "source": [
    "# Writing to Delta format\n",
    "df4.write.format(\"delta\").save(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/sales_data_delta\")\n",
    "print(\"Written to delta table format\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b11680f3-eb7c-45e7-8d46-a86b0b2efafe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- event_time: timestamp (nullable = true)\n |-- event_type: string (nullable = true)\n |-- user_id: string (nullable = true)\n |-- amount: double (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "#7. Analyzing and Visualizing Streaming Data with Databricks\n",
    "#Task: Analyzing Streaming Data\n",
    "\n",
    "df5 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_1.csv\")\n",
    "\n",
    "# Path to the saved CSV file\n",
    "streaming_path = \"/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_1.csv\"\n",
    "\n",
    "# Reading the dataset in streaming mode\n",
    "streaming_df = (spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(schema).load(streaming_path))\n",
    "\n",
    "# Show streaming schema\n",
    "streaming_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e1f40fc5-6d87-4547-9a94-ae22d2cd1ac5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window\n",
    "\n",
    "# Aggregating data by event type and time window\n",
    "aggregated_streaming_df = streaming_df \\\n",
    "    .groupBy(window(\"event_time\", \"1 minute\"), \"event_type\") \\\n",
    "    .agg({\"amount\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_amount\")\n",
    "\n",
    "# Write the results to the console to verify\n",
    "query = aggregated_streaming_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be7c6172-13b3-4355-9fec-63e98883bf8a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a7dee3d-a198-4087-bd43-24c7a77de1cb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data written to Delta format\n"
     ]
    }
   ],
   "source": [
    "#8. Introduction to Databricks Delta Lake\n",
    "#Task: Using Delta Lake for Data Versioning\n",
    "\n",
    "df6 = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_1.csv\")\n",
    "\n",
    "#Write DataFrame to Delta format \n",
    "df6.write.format(\"delta\").save(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_delta\")\n",
    "print(\"Data written to Delta format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af777c50-a6d6-4573-8050-4a103cd8298d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------+------------------+\n|         event_time|event_type|user_id|            amount|\n+-------------------+----------+-------+------------------+\n|2024-09-01 18:46:00|      view|    125|               0.0|\n|2024-09-01 14:20:00|      view|    590|               0.0|\n|2024-09-01 21:34:00|     click|    626|214.32524639559162|\n|2024-09-01 18:50:00|  purchase|    847|100.31318854589998|\n|2024-09-01 18:15:00|      view|    861|               0.0|\n|2024-09-01 17:24:00|     click|    338| 760.3444131051176|\n|2024-09-01 02:01:00|  purchase|    466| 857.7640567600539|\n|2024-09-01 07:46:00|  purchase|    503|1019.2739842176004|\n|2024-09-01 20:38:00|     click|    435|  887.214593409447|\n|2024-09-01 05:30:00|  purchase|     91|1025.9687203433857|\n|2024-09-01 01:27:00|  purchase|    375| 550.1249656164244|\n|2024-09-01 23:16:00|     click|    889|380.58625997139364|\n|2024-09-01 18:43:00|  purchase|     60|134.67489453008145|\n|2024-09-01 14:31:00|     click|    499| 993.1213927385505|\n|2024-09-01 02:10:00|      view|    836|               0.0|\n|2024-09-01 22:12:00|  purchase|    349| 612.5809093380254|\n|2024-09-01 12:49:00|      view|    564|               0.0|\n|2024-09-01 05:43:00|     click|    699|27.815051301618634|\n|2024-09-01 23:57:00|  purchase|    904| 628.8883308475426|\n|2024-09-01 13:25:00|  purchase|    612|  903.078549948274|\n+-------------------+----------+-------+------------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "delta_table_path = \"/FileStore/shared_uploads/varshinie.1006@gmail.com/event_data_delta\"\n",
    "# Perform updates by loading the Delta table\n",
    "delta_table = spark.read.format(\"delta\").load(delta_table_path)\n",
    "\n",
    "# Simulating an update: Changing 'amount' for 'purchase' event types\n",
    "from pyspark.sql.functions import expr\n",
    "updated_data = delta_table.withColumn(\"amount\", expr(\"case when event_type = 'purchase' then amount * 1.10 else amount end\"))\n",
    "\n",
    "# Overwrite the Delta table with the updated data\n",
    "updated_data.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "updated_data.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fed69cf8-079a-4b78-8e8c-44db3e3f99a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------+------------------+\n|         event_time|event_type|user_id|            amount|\n+-------------------+----------+-------+------------------+\n|2024-09-01 18:46:00|      view|    125|               0.0|\n|2024-09-01 14:20:00|      view|    590|               0.0|\n|2024-09-01 21:34:00|     click|    626|214.32524639559162|\n|2024-09-01 18:50:00|  purchase|    847| 82.90346160818179|\n|2024-09-01 18:15:00|      view|    861|               0.0|\n+-------------------+----------+-------+------------------+\nonly showing top 5 rows\n\n+-------------------+----------+-------+------------------+\n|         event_time|event_type|user_id|            amount|\n+-------------------+----------+-------+------------------+\n|2024-09-01 18:46:00|      view|    125|               0.0|\n|2024-09-01 14:20:00|      view|    590|               0.0|\n|2024-09-01 21:34:00|     click|    626|214.32524639559162|\n|2024-09-01 18:50:00|  purchase|    847| 91.19380776899997|\n|2024-09-01 18:15:00|      view|    861|               0.0|\n+-------------------+----------+-------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Reading the Delta table as of version 0 (before the update)\n",
    "df_version_0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).load(delta_table_path)\n",
    "df_version_0.show(5)\n",
    "\n",
    "# Reading the Delta table as of version 1 (after the update)\n",
    "df_version_1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).load(delta_table_path)\n",
    "df_version_1.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d27e1d0-c975-44c1-8e68-57294f9dd3e3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[51]: DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint>]"
     ]
    }
   ],
   "source": [
    "# Optimize the Delta table\n",
    "spark.sql(f\"OPTIMIZE delta.`{delta_table_path}`\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "285ec748-da27-478c-aea4-a1a480f47a9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1012568290716932>:2\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Vacuum the Delta table to delete old data versions\u001B[39;00m\n",
       "\u001B[0;32m----> 2\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mVACUUM delta.`\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdelta_table_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m` RETAIN 0 HOURS\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\n",
       "writers that are currently writing to this table, there is a risk that you may corrupt the\n",
       "state of your Delta table.\n",
       "\n",
       "If you are certain that there are no operations being performed on this table, such as\n",
       "insert/upsert/delete/optimize, then you may turn off this check by setting:\n",
       "spark.databricks.delta.retentionDurationCheck.enabled = false\n",
       "\n",
       "If you are not sure, please use a value not less than \"168 hours\".\n",
       "       "
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mIllegalArgumentException\u001B[0m                  Traceback (most recent call last)\nFile \u001B[0;32m<command-1012568290716932>:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# Vacuum the Delta table to delete old data versions\u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43mf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mVACUUM delta.`\u001B[39;49m\u001B[38;5;132;43;01m{\u001B[39;49;00m\u001B[43mdelta_table_path\u001B[49m\u001B[38;5;132;43;01m}\u001B[39;49;00m\u001B[38;5;124;43m` RETAIN 0 HOURS\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mIllegalArgumentException\u001B[0m: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
       "errorSummary": "<span class='ansi-red-fg'>IllegalArgumentException</span>: requirement failed: Are you sure you would like to vacuum files with such a low retention period? If you have\nwriters that are currently writing to this table, there is a risk that you may corrupt the\nstate of your Delta table.\n\nIf you are certain that there are no operations being performed on this table, such as\ninsert/upsert/delete/optimize, then you may turn off this check by setting:\nspark.databricks.delta.retentionDurationCheck.enabled = false\n\nIf you are not sure, please use a value not less than \"168 hours\".\n       ",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Vacuum the Delta table to delete old data versions\n",
    "spark.sql(f\"VACUUM delta.`{delta_table_path}` RETAIN 0 HOURS\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f628b4-f8bd-4df6-8f36-46fd603d1cd0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+----------+-------+------------------+\n|         event_time|event_type|user_id|            amount|\n+-------------------+----------+-------+------------------+\n|2024-09-01 18:46:00|      view|    125|               0.0|\n|2024-09-01 14:20:00|      view|    590|               0.0|\n|2024-09-01 21:34:00|     click|    626|214.32524639559162|\n|2024-09-01 18:50:00|  purchase|    847| 91.19380776899997|\n|2024-09-01 18:15:00|      view|    861|               0.0|\n+-------------------+----------+-------+------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# Query the optimized Delta table\n",
    "optimized_df = spark.read.format(\"delta\").load(delta_table_path)\n",
    "optimized_df.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd78c129-8dde-4f80-83a9-4f1be243a67b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#9. Managed and Unmanaged Tables\n",
    "#Task: Creating Managed and Unmanaged Tables\n",
    "# Save the sales data as a managed Delta table\n",
    "df4.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"managed_sales_table\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35c5e59b-17c6-4f89-99fd-ae3916d47df1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[56]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "# Define the external location to save the data for unmanaged table\n",
    "external_path = \"/FileStore/tables/unmanaged_sales_data\"\n",
    "\n",
    "# Save the sales data to the external location\n",
    "df4.write.format(\"delta\").mode(\"overwrite\").save(external_path)\n",
    "\n",
    "# Create an unmanaged Delta table pointing to this external location\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE unmanaged_sales_table\n",
    "    USING DELTA\n",
    "    LOCATION '{external_path}'\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "270122cb-bd48-42d4-9da7-3c8336694d41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n+----------+------+--------+--------+-----+\n\n+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n+----------+------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Select the first 5 records from the managed table\n",
    "spark.sql(\"SELECT * FROM managed_sales_table LIMIT 5\").show()\n",
    "\n",
    "# Select the first 5 records from the unmanaged table\n",
    "spark.sql(\"SELECT * FROM unmanaged_sales_table LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "65a7500a-7ef6-4435-8c24-f73cc75705f0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n+----------+------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#10. Views and Temporary Views\n",
    "#Task: Working with Views in Databricks\n",
    "\n",
    "# Create a view from the sales data\n",
    "df4.createOrReplaceTempView(\"sales_view\")\n",
    "\n",
    "# Query the view\n",
    "spark.sql(\"SELECT * FROM sales_view LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4dca8cf4-8ad9-4b41-9788-b0bdbee32f31",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+\n| product|total_sales|\n+--------+-----------+\n|Widget C|       45.0|\n|Widget B|      47.25|\n|Widget A|       76.5|\n|Widget D|       30.0|\n+--------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary view\n",
    "df4.createOrReplaceTempView(\"temp_sales_view\")\n",
    "\n",
    "# Query the temporary view\n",
    "spark.sql(\"SELECT product, SUM(price) AS total_sales FROM temp_sales_view GROUP BY product\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf7fb169-8681-4ce8-92a4-56ef22d306b9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+--------+--------+-----+\n|      Date|Region| Product|Quantity|Price|\n+----------+------+--------+--------+-----+\n|2024-09-01| North|Widget A|      10|25.50|\n|2024-09-01| South|Widget B|       5|15.75|\n|2024-09-02| North|Widget A|      12|25.50|\n|2024-09-02|  East|Widget C|       8|22.50|\n|2024-09-03|  West|Widget A|      15|25.50|\n+----------+------+--------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "# Create a global temporary view\n",
    "df4.createOrReplaceGlobalTempView(\"global_sales_view\")\n",
    "\n",
    "# Query the global temporary view\n",
    "spark.sql(\"SELECT * FROM global_temp.global_sales_view LIMIT 5\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9482c303-3e42-4eaa-a4ba-5385b830d17f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Assignment: Delta Lake Concepts\n",
    "#Task 1: Creating Delta Table using Three Methods\n",
    "\n",
    "#1. Load the given CSV and JSON datasets into Databricks.\n",
    "\n",
    "df_employees = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/employees.csv\")\n",
    "\n",
    "df_products = spark.read.format(\"json\").load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/products.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c13ae178-0839-4701-8266-e3de4141d4f5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Load the Employees CSV file into a DataFrame and cache it\n",
    "df_employees = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/employees.csv\").cache()\n",
    "\n",
    "# Load the Products JSON file into a DataFrame and cache it\n",
    "df_products = spark.read.format(\"json\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/products.json\").cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e220d82f-f4fe-4a83-9916-9a0dc4e3391b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1573338673244432>:6\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#2. Create a Delta table using the following three methods:\u001B[39;00m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Create a Delta table from a DataFrame.\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m \n",
       "\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#df_employees.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employees\")\u001B[39;00m\n",
       "\u001B[0;32m----> 6\u001B[0m \u001B[43mdf_products\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/delta/products\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n",
       "\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
       "referenced columns only include the internal corrupt record column\n",
       "(named _corrupt_record by default). For example:\n",
       "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\n",
       "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\n",
       "Instead, you can cache or save the parsed results and then send the same query.\n",
       "For example, val df = spark.read.schema(schema).csv(file).cache() and then\n",
       "df.filter($\"_corrupt_record\".isNotNull).count()."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1573338673244432>:6\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#2. Create a Delta table using the following three methods:\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m#Create a Delta table from a DataFrame.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \n\u001B[1;32m      4\u001B[0m \u001B[38;5;66;03m#df_employees.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employees\")\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[43mdf_products\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mformat\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdelta\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43moverwrite\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m/delta/products\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/readwriter.py:1397\u001B[0m, in \u001B[0;36mDataFrameWriter.save\u001B[0;34m(self, path, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m   1395\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39msave()\n\u001B[1;32m   1396\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1397\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msave\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpath\u001B[49m\u001B[43m)\u001B[49m\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2. Create a Delta table using the following three methods:\n",
    "#Create a Delta table from a DataFrame.\n",
    "\n",
    "df_employees.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d130aeff-57df-40f6-8ea0-43746ac67808",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create temporary view for Employees DataFrame\n",
    "df_employees.createOrReplaceTempView(\"df_employees\")\n",
    "\n",
    "# Create temporary view for Products DataFrame\n",
    "df_products.createOrReplaceTempView(\"df_products\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c59b801-357a-4a93-a1ce-9dc23b173fa3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1573338673244434>:7\u001B[0m\n",
       "\u001B[1;32m      5\u001B[0m     display(df)\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n",
       "\n",
       "File \u001B[0;32m<command-1573338673244434>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n",
       "\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n",
       "\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n",
       "\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS1Vc2UgU1FMIHRvIGNyZWF0ZSBhIERlbHRhIHRhYmxlLgotLSBFbXBsb3llZXMgCkNSRUFURSBUQUJMRSBkZWx0YV9lbXBsb3llZXMKVVNJTkcgZGVsdGEKQVMgU0VMRUNUICogRlJPTSBkZl9lbXBsb3llZXM=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m      5\u001B[0m   display(df)\n",
       "\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n",
       "\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n",
       "\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n",
       "\u001B[1;32m     51\u001B[0m     )\n",
       "\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n",
       "\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n",
       "\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n",
       "\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n",
       "\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\n",
       "referenced columns only include the internal corrupt record column\n",
       "(named _corrupt_record by default). For example:\n",
       "spark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\n",
       "and spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\n",
       "Instead, you can cache or save the parsed results and then send the same query.\n",
       "For example, val df = spark.read.schema(schema).csv(file).cache() and then\n",
       "df.filter($\"_corrupt_record\".isNotNull).count()."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1573338673244434>:7\u001B[0m\n\u001B[1;32m      5\u001B[0m     display(df)\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m df\n\u001B[0;32m----> 7\u001B[0m   _sqldf \u001B[38;5;241m=\u001B[39m \u001B[43m____databricks_percent_sql\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m      9\u001B[0m   \u001B[38;5;28;01mdel\u001B[39;00m ____databricks_percent_sql\n\nFile \u001B[0;32m<command-1573338673244434>:4\u001B[0m, in \u001B[0;36m____databricks_percent_sql\u001B[0;34m()\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m____databricks_percent_sql\u001B[39m():\n\u001B[1;32m      3\u001B[0m   \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mbase64\u001B[39;00m\n\u001B[0;32m----> 4\u001B[0m   df \u001B[38;5;241m=\u001B[39m \u001B[43mspark\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbase64\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstandard_b64decode\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mLS1Vc2UgU1FMIHRvIGNyZWF0ZSBhIERlbHRhIHRhYmxlLgotLSBFbXBsb3llZXMgCkNSRUFURSBUQUJMRSBkZWx0YV9lbXBsb3llZXMKVVNJTkcgZGVsdGEKQVMgU0VMRUNUICogRlJPTSBkZl9lbXBsb3llZXM=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdecode\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m   display(df)\n\u001B[1;32m      6\u001B[0m   \u001B[38;5;28;01mreturn\u001B[39;00m df\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/instrumentation_utils.py:48\u001B[0m, in \u001B[0;36m_wrap_function.<locals>.wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     46\u001B[0m start \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mperf_counter()\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 48\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     49\u001B[0m     logger\u001B[38;5;241m.\u001B[39mlog_success(\n\u001B[1;32m     50\u001B[0m         module_name, class_name, function_name, time\u001B[38;5;241m.\u001B[39mperf_counter() \u001B[38;5;241m-\u001B[39m start, signature\n\u001B[1;32m     51\u001B[0m     )\n\u001B[1;32m     52\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m res\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/session.py:1387\u001B[0m, in \u001B[0;36mSparkSession.sql\u001B[0;34m(self, sqlQuery, args, **kwargs)\u001B[0m\n\u001B[1;32m   1385\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m   1386\u001B[0m     litArgs \u001B[38;5;241m=\u001B[39m {k: _to_java_column(lit(v)) \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m (args \u001B[38;5;129;01mor\u001B[39;00m {})\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[0;32m-> 1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m DataFrame(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jsparkSession\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msql\u001B[49m\u001B[43m(\u001B[49m\u001B[43msqlQuery\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlitArgs\u001B[49m\u001B[43m)\u001B[49m, \u001B[38;5;28mself\u001B[39m)\n\u001B[1;32m   1388\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[1;32m   1389\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(kwargs) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Since Spark 2.3, the queries from raw JSON/CSV files are disallowed when the\nreferenced columns only include the internal corrupt record column\n(named _corrupt_record by default). For example:\nspark.read.schema(schema).csv(file).filter($\"_corrupt_record\".isNotNull).count()\nand spark.read.schema(schema).csv(file).select(\"_corrupt_record\").show().\nInstead, you can cache or save the parsed results and then send the same query.\nFor example, val df = spark.read.schema(schema).csv(file).cache() and then\ndf.filter($\"_corrupt_record\".isNotNull).count().",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Use SQL to create a Delta table.\n",
    "-- Employees \n",
    "CREATE TABLE employees_delta\n",
    "USING delta\n",
    "AS SELECT * FROM df_employees;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "189e4025-0f58-46bf-9623-2e9aae21a2bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Convert both the CSV and JSON files into Delta format.\n",
    "\n",
    "df_employees.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/employees_converted\")\n",
    "\n",
    "df_products.write.format(\"delta\").mode(\"overwrite\").save(\"/delta/products_converted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "903f85f9-ef55-4e27-8bbf-9aef62cf0b27",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 2: Merge and Upsert (Slowly Changing Dimension - SCD)\n",
    "#1. Load the Delta table for employees created in Task 1.\n",
    "employees_delta_df = spark.read.format(\"delta\").load(\"/delta/employees\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "68945f6a-6cce-4342-bf6e-27a95d9c7ffe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Merge the new employee data into the employees Delta table.\n",
    "new_employee_data = [\n",
    "    (102, \"Alice\", \"Finance\", \"2023-02-15\", 75000),  # Updated Salary\n",
    "    (106, \"Olivia\", \"HR\", \"2023-06-10\", 65000)       # New Employee\n",
    "]\n",
    "\n",
    "columns = [\"EmployeeID\", \"EmployeeName\", \"Department\", \"JoiningDate\", \"Salary\"]\n",
    "\n",
    "new_employees_df = spark.createDataFrame(new_employee_data, columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ca7b1050-cfd5-4e97-9546-41ceeaecba34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[27]: DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "#3. If an employee exists, update their salary. If the employee is new, insert their details.\n",
    "\n",
    "#new_employees_df.createOrReplaceTempView(\"new_employees_view\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO delta.`/delta/employees` AS target\n",
    "USING new_employees_view AS source\n",
    "ON target.EmployeeID = source.EmployeeID\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.Salary = source.Salary\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (EmployeeID, EmployeeName, Department, JoiningDate, Salary)\n",
    "  VALUES (source.EmployeeID, source.EmployeeName, source.Department, source.JoiningDate, source.Salary);\n",
    "\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b681eb2d-eccb-4ed6-9a55-270c63359639",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------+-----------+-----------+------+\n|EmployeeID|EmployeeName| Department|JoiningDate|Salary|\n+----------+------------+-----------+-----------+------+\n|       101|        John|         HR| 2023-01-10| 50000|\n|       103|        Mark|Engineering| 2023-03-20| 85000|\n|       104|        Emma|      Sales| 2023-04-01| 55000|\n|       105|        Liam|  Marketing| 2023-05-12| 60000|\n|       102|       Alice|    Finance| 2023-02-15| 75000|\n|       106|      Olivia|         HR| 2023-06-10| 65000|\n+----------+------------+-----------+-----------+------+\n\n"
     ]
    }
   ],
   "source": [
    "# Verify the updated Delta table\n",
    "updated_employees_df = spark.read.format(\"delta\").load(\"/delta/employees\")\n",
    "updated_employees_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "35cca378-d621-4ffb-bbbd-be16f76dc6e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------+----+-----------+---------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+----------+\n|format|id                                  |name|description|location             |createdAt              |lastModified       |partitionColumns|numFiles|sizeInBytes|properties|minReaderVersion|minWriterVersion|tableFeatures           |statistics|\n+------+------------------------------------+----+-----------+---------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+----------+\n|delta |06f06877-5161-4c56-9546-5e301a90923f|null|null       |dbfs:/delta/employees|2024-09-17 04:11:26.246|2024-09-17 05:02:16|[]              |3       |5059       |{}        |1               |2               |[appendOnly, invariants]|{}        |\n+------+------------------------------------+----+-----------+---------------------+-----------------------+-------------------+----------------+--------+-----------+----------+----------------+----------------+------------------------+----------+\n\n+------------+---------+-------+\n|    col_name|data_type|comment|\n+------------+---------+-------+\n|  EmployeeID|   string|   null|\n|EmployeeName|   string|   null|\n|  Department|   string|   null|\n| JoiningDate|   string|   null|\n|      Salary|   string|   null|\n+------------+---------+-------+\n\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|version|          timestamp|          userId|            userName|operation| operationParameters| job|          notebook|           clusterId|readVersion|   isolationLevel|isBlindAppend|    operationMetrics|userMetadata|          engineInfo|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n|      2|2024-09-17 05:02:16|3973167559214203|varshinie.1006@gm...|    MERGE|{predicate -> [\"(...|null|{1638296729183879}|0917-032946-bu5gwzay|          1|WriteSerializable|        false|{numTargetRowsCop...|        null|Databricks-Runtim...|\n|      1|2024-09-17 04:14:23|3973167559214203|varshinie.1006@gm...|    WRITE|{mode -> Overwrit...|null|{1638296729183879}|0917-032946-bu5gwzay|          0|WriteSerializable|        false|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n|      0|2024-09-17 04:11:39|3973167559214203|varshinie.1006@gm...|    WRITE|{mode -> Overwrit...|null|{1638296729183879}|0917-032946-bu5gwzay|       null|WriteSerializable|        false|{numFiles -> 1, n...|        null|Databricks-Runtim...|\n+-------+-------------------+----------------+--------------------+---------+--------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------+------------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#Task 3: Internals of Delta Table\n",
    "#1. Explore the internals of the employees Delta table using Delta Lake features.\n",
    "# Describe the Delta table to see its metadata and internals\n",
    "spark.sql(\"DESCRIBE DETAIL delta.`/delta/employees`\").show(truncate=False)\n",
    "\n",
    "# Show the schema of the Delta table\n",
    "spark.sql(\"DESCRIBE delta.`/delta/employees`\").show()\n",
    "\n",
    "# Check the number of files in the Delta table\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/delta/employees`\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01e8768a-91f2-4e4c-9144-59425e05211e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|version|timestamp          |userId          |userName                |operation|operationParameters                                                                                                                                                                                           |job |notebook          |clusterId           |readVersion|isolationLevel   |isBlindAppend|operationMetrics                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    |userMetadata|engineInfo                         |\n+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n|2      |2024-09-17 05:02:16|3973167559214203|varshinie.1006@gmail.com|MERGE    |{predicate -> [\"(cast(EmployeeID#4792 as bigint) = EmployeeID#4777L)\"], matchedPredicates -> [{\"actionType\":\"update\"}], notMatchedPredicates -> [{\"actionType\":\"insert\"}], notMatchedBySourcePredicates -> []}|null|{1638296729183879}|0917-032946-bu5gwzay|1          |WriteSerializable|false        |{numTargetRowsCopied -> 4, numTargetRowsDeleted -> 0, numTargetFilesAdded -> 3, numTargetBytesAdded -> 5059, numTargetBytesRemoved -> 1774, numTargetDeletionVectorsAdded -> 0, numTargetRowsMatchedUpdated -> 1, executionTimeMs -> 19680, numTargetRowsInserted -> 1, numTargetRowsMatchedDeleted -> 0, scanTimeMs -> 12877, numTargetRowsUpdated -> 1, numOutputRows -> 6, numTargetDeletionVectorsRemoved -> 0, numTargetRowsNotMatchedBySourceUpdated -> 0, numTargetChangeFilesAdded -> 0, numSourceRows -> 2, numTargetFilesRemoved -> 1, numTargetRowsNotMatchedBySourceDeleted -> 0, rewriteTimeMs -> 4279}|null        |Databricks-Runtime/12.2.x-scala2.12|\n|1      |2024-09-17 04:14:23|3973167559214203|varshinie.1006@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                                                        |null|{1638296729183879}|0917-032946-bu5gwzay|0          |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1774}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |null        |Databricks-Runtime/12.2.x-scala2.12|\n|0      |2024-09-17 04:11:39|3973167559214203|varshinie.1006@gmail.com|WRITE    |{mode -> Overwrite, partitionBy -> []}                                                                                                                                                                        |null|{1638296729183879}|0917-032946-bu5gwzay|null       |WriteSerializable|false        |{numFiles -> 1, numOutputRows -> 5, numOutputBytes -> 1774}                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         |null        |Databricks-Runtime/12.2.x-scala2.12|\n+-------+-------------------+----------------+------------------------+---------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----+------------------+--------------------+-----------+-----------------+-------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------+-----------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "#2. Check the transaction history of the table.\n",
    "spark.sql(\"DESCRIBE HISTORY delta.`/delta/employees`\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b3e895e2-91a5-43a7-afb5-b1f3a1c10fe5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>EmployeeID</th><th>EmployeeName</th><th>Department</th><th>JoiningDate</th><th>Salary</th></tr></thead><tbody><tr><td>101</td><td>John</td><td>HR</td><td>2023-01-10</td><td>50000</td></tr><tr><td>102</td><td>Alice</td><td>Finance</td><td>2023-02-15</td><td>70000</td></tr><tr><td>103</td><td>Mark</td><td>Engineering</td><td>2023-03-20</td><td>85000</td></tr><tr><td>104</td><td>Emma</td><td>Sales</td><td>2023-04-01</td><td>55000</td></tr><tr><td>105</td><td>Liam</td><td>Marketing</td><td>2023-05-12</td><td>60000</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "John",
         "HR",
         "2023-01-10",
         "50000"
        ],
        [
         "102",
         "Alice",
         "Finance",
         "2023-02-15",
         "70000"
        ],
        [
         "103",
         "Mark",
         "Engineering",
         "2023-03-20",
         "85000"
        ],
        [
         "104",
         "Emma",
         "Sales",
         "2023-04-01",
         "55000"
        ],
        [
         "105",
         "Liam",
         "Marketing",
         "2023-05-12",
         "60000"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "EmployeeID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "EmployeeName",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Department",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "JoiningDate",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Salary",
         "type": "\"string\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--3. Perform Time Travel and retrieve the table before the previous merge operation.\n",
    "-- Retrieve the Delta table using a version number\n",
    "SELECT * FROM delta.`/delta/employees` VERSION AS OF 1;\n",
    "\n",
    "-- Retrieve the Delta table using a valid timestamp\n",
    "SELECT * FROM delta.`/delta/employees` TIMESTAMP AS OF '2024-09-17T04:30:00.000Z';\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a75f3bf4-3a5e-484f-998c-b88081e6c3fa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[37]: DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint>]"
     ]
    }
   ],
   "source": [
    "#Task 4: Optimize Delta Table\n",
    "#1. Optimize the employees Delta table for better performance.\n",
    "spark.sql(\"\"\"\n",
    "OPTIMIZE delta.`/delta/employees`;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5747d4ab-217f-4467-9291-6c87a9499678",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[38]: DataFrame[path: string, metrics: struct<numFilesAdded:bigint,numFilesRemoved:bigint,filesAdded:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,filesRemoved:struct<min:bigint,max:bigint,avg:double,totalFiles:bigint,totalSize:bigint>,partitionsOptimized:bigint,zOrderStats:struct<strategyName:string,inputCubeFiles:struct<num:bigint,size:bigint>,inputOtherFiles:struct<num:bigint,size:bigint>,inputNumCubes:bigint,mergedFiles:struct<num:bigint,size:bigint>,numOutputCubes:bigint,mergedNumCubes:bigint>,numBatches:bigint,totalConsideredFiles:bigint,totalFilesSkipped:bigint,preserveInsertionOrder:boolean,numFilesSkippedToReduceWriteAmplification:bigint,numBytesSkippedToReduceWriteAmplification:bigint,startTimeMs:bigint,endTimeMs:bigint,totalClusterParallelism:bigint,totalScheduledTasks:bigint,autoCompactParallelismStats:struct<maxClusterActiveParallelism:bigint,minClusterActiveParallelism:bigint,maxSessionActiveParallelism:bigint,minSessionActiveParallelism:bigint>,deletionVectorStats:struct<numDeletionVectorsRemoved:bigint,numDeletionVectorRowsRemoved:bigint>,numTableColumns:bigint,numTableColumnsWithStats:bigint,totalTaskExecutionTimeMs:bigint>]"
     ]
    }
   ],
   "source": [
    "#2. Use Z-ordering on the Department column for improved query performance.\n",
    "spark.sql(\"\"\"\n",
    "OPTIMIZE delta.`/delta/employees` ZORDER BY (Department);\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dab9588-5843-4e96-8ac4-395760320b2d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[39]: DataFrame[version: bigint, timestamp: timestamp, userId: string, userName: string, operation: string, operationParameters: map<string,string>, job: struct<jobId:string,jobName:string,jobRunId:string,runId:string,jobOwnerId:string,triggerType:string>, notebook: struct<notebookId:string>, clusterId: string, readVersion: bigint, isolationLevel: string, isBlindAppend: boolean, operationMetrics: map<string,string>, userMetadata: string, engineInfo: string]"
     ]
    }
   ],
   "source": [
    "#Task 5: Time Travel with Delta Table\n",
    "#1. Retrieve the employees Delta table as it was before the last merge.\n",
    "spark.sql(\"\"\"\n",
    "DESCRIBE HISTORY delta.`/delta/employees`;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67810b16-5470-4cb1-9074-f8c0b7c7d292",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[41]: DataFrame[EmployeeID: string, EmployeeName: string, Department: string, JoiningDate: string, Salary: string]"
     ]
    }
   ],
   "source": [
    "#2. Query the table at a specific version to view the older records.\n",
    "spark.sql(\"\"\"\n",
    "SELECT * FROM delta.`/delta/employees` VERSION AS OF 2;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "21b2d9ad-c31c-464f-a0a6-3e6d3d742859",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[42]: DataFrame[path: string]"
     ]
    }
   ],
   "source": [
    "#Task 6: Vacuum Delta Table\n",
    "#1. Use the vacuum operation on the employees Delta table to remove old versions and free up disk space.\n",
    "spark.sql(\"\"\"\n",
    "VACUUM delta.`/delta/employees`;\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e0e7a486-d717-4a31-925b-e5fb4c957168",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[43]: DataFrame[path: string]"
     ]
    }
   ],
   "source": [
    "#2. Set the retention period to 7 days and ensure that old files are deleted.\n",
    "spark.sql(\"\"\"\n",
    "VACUUM delta.`/delta/employees` RETAIN 168 HOURS;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "742af18d-ac7e-4334-8442-a8e58f19cd57",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[44]: True"
     ]
    }
   ],
   "source": [
    "#Assignment: Structured Streaming and Transformations on Streams\n",
    "#Task 1: Ingest Streaming Data from CSV Files\n",
    "streaming_folder_path = \"/mnt/streaming_csv_data\"\n",
    "\n",
    "dbutils.fs.mkdirs(streaming_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1c2d3ab4-7344-49b7-ab4a-000362e2b52d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Set up a structured streaming source to continuously read CSV data from this folder.\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DateType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"TransactionDate\", DateType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "streaming_df = spark.readStream \\\n",
    "    .schema(schema) \\\n",
    "    .csv(streaming_folder_path) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "11d7f552-2d28-45fd-9f85-a83df447cb28",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. Ensure that the streaming query reads the data continuously in append mode and\n",
    "displays the results in the console.\n",
    "query = streaming_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .option(\"truncate\", False) \\\n",
    "    .start()\n",
    "\n",
    "#query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "459a6150-bcf7-4db8-9432-a2660ef436d8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 2: Stream Transformations\n",
    "#Add a new column for the TotalAmount ( Quantity * Price ).\n",
    "#Filter records where the Quantity is greater than 1.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "transformed_df = streaming_df \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .filter(col(\"Quantity\") > 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7838e10e-7efd-4c25-b004-d28aa06cff2e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Write the transformed stream to a memory sink to see the updated results continuously.\n",
    "query = transformed_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"transformed_stream\") \\\n",
    "    .start()\n",
    "\n",
    "#query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "eaa63786-8ea8-4909-bfd6-9206a555d562",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>TransactionID</th><th>TransactionDate</th><th>ProductID</th><th>Quantity</th><th>Price</th><th>TotalAmount</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "TransactionID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TransactionDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "TotalAmount",
         "type": "\"integer\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display the results from the memory table\n",
    "display(spark.table(\"transformed_stream\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee990128-ac12-4dd7-8623-056791963abe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 3: Aggregations on Streaming Data\n",
    "#1. Implement an aggregation on the streaming data:\n",
    "#Group the data by ProductID and calculate the total sales for each product\n",
    "from pyspark.sql.functions import col, sum as sum_\n",
    "\n",
    "# Aggregate data: sum of TotalAmount grouped by ProductID\n",
    "aggregated_df = transformed_df \\\n",
    "    .groupBy(\"ProductID\") \\\n",
    "    .agg(\n",
    "        sum_(\"TotalAmount\").alias(\"TotalSales\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b1e624d3-bc8a-443c-aaf7-2be5f16cb458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>ProductID</th><th>TotalSales</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "ProductID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "TotalSales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 2. Ensure the stream runs in update mode, so only updated results are output to the sink.\n",
    "\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"aggregated_sales_stream\") \\\n",
    "    .start()\n",
    "\n",
    "#query.awaitTermination()\n",
    "\n",
    "display(spark.table(\"aggregated_sales_stream\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74fce513-e5f6-472a-acbb-3b62c87953e9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected exception formatting exception. Falling back to standard exception\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 3378, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<command-1573338673244458>\", line 28, in <module>\n    query = aggregated_df.writeStream \\\n  File \"/databricks/spark/python/pyspark/sql/streaming/readwriter.py\", line 1385, in start\n    return self._sq(self._jwrite.start())\n  File \"/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1321, in __call__\n    return_value = get_return_value(\n  File \"/databricks/spark/python/pyspark/errors/exceptions.py\", line 234, in deco\n    raise converted from None\npyspark.errors.exceptions.AnalysisException: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9676L]\n+- ~EventTimeWatermark TransactionDate#9662: timestamp, 1 days\n   +- ~Project [TransactionID#9529, to_timestamp(TransactionDate#9530, None, TimestampType, Some(Etc/UTC), false) AS TransactionDate#9662, ProductID#9531, Quantity#9532, Price#9533, TotalAmount#9549]\n      +- ~Filter (Quantity#9532 > 1)\n         +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/interactiveshell.py\", line 1997, in showtraceback\n    stb = self.InteractiveTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1112, in structured_traceback\n    return FormattedTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 1006, in structured_traceback\n    return VerboseTB.structured_traceback(\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 859, in structured_traceback\n    formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 812, in format_exception_as_a_whole\n    frames.append(self.format_record(r))\n  File \"/databricks/python/lib/python3.9/site-packages/IPython/core/ultratb.py\", line 730, in format_record\n    result += ''.join(_format_traceback_lines(frame_info.lines, Colors, self.has_colors, lvals))\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 698, in lines\n    pieces = self.included_pieces\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 649, in included_pieces\n    pos = scope_pieces.index(self.executing_piece)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/utils.py\", line 145, in cached_property_wrapper\n    value = obj.__dict__[self.func.__name__] = self.func(obj)\n  File \"/databricks/python/lib/python3.9/site-packages/stack_data/core.py\", line 628, in executing_piece\n    return only(\n  File \"/databricks/python/lib/python3.9/site-packages/executing/executing.py\", line 164, in only\n    raise NotOneValueFound('Expected one value, found 0')\nexecuting.executing.NotOneValueFound: Expected one value, found 0\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       ""
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9676L]\n+- ~EventTimeWatermark TransactionDate#9662: timestamp, 1 days\n   +- ~Project [TransactionID#9529, to_timestamp(TransactionDate#9530, None, TimestampType, Some(Etc/UTC), false) AS TransactionDate#9662, ProductID#9531, Quantity#9532, Price#9533, TotalAmount#9549]\n      +- ~Filter (Quantity#9532 > 1)\n         +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Task 4: Writing Streaming Data to File Sinks\n",
    "#1. After transforming and aggregating the data, write the streaming results to a Parquet sink.\n",
    "#2. Ensure that you configure a checkpoint location to store progress and ensure recovery in case of failure.\n",
    "parquet_sink_path = \"/mnt/streaming_parquet_data\"\n",
    "checkpoint_location = \"/mnt/checkpoints/aggregated_sales\"\n",
    "\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", parquet_sink_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3b2250eb-87db-4b13-8bdb-8cee43704f72",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1573338673244460>:28\u001B[0m\n",
       "\u001B[1;32m     25\u001B[0m checkpoint_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/checkpoints/aggregated_sales\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Write the aggregated data to a Parquet sink with checkpointing\u001B[39;00m\n",
       "\u001B[0;32m---> 28\u001B[0m query \u001B[38;5;241m=\u001B[39m aggregated_df\u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     30\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     31\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, parquet_sink_path) \\\n",
       "\u001B[1;32m     32\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, checkpoint_location) \\\n",
       "\u001B[1;32m     33\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Await termination to keep the streaming query running\u001B[39;00m\n",
       "\u001B[1;32m     36\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1385\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m   1383\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqueryName(queryName)\n",
       "\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m   1386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mstart(path))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n",
       "~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9695L]\n",
       "+- ~EventTimeWatermark TransactionDate#9681: timestamp, 1 days\n",
       "   +- ~Project [TransactionID#9529, to_timestamp(TransactionDate#9530, None, TimestampType, Some(Etc/UTC), false) AS TransactionDate#9681, ProductID#9531, Quantity#9532, Price#9533, TotalAmount#9549]\n",
       "      +- ~Filter (Quantity#9532 > 1)\n",
       "         +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n",
       "            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1573338673244460>:28\u001B[0m\n\u001B[1;32m     25\u001B[0m checkpoint_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/checkpoints/aggregated_sales\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# Write the aggregated data to a Parquet sink with checkpointing\u001B[39;00m\n\u001B[0;32m---> 28\u001B[0m query \u001B[38;5;241m=\u001B[39m aggregated_df\u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     30\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     31\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, parquet_sink_path) \\\n\u001B[1;32m     32\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, checkpoint_location) \\\n\u001B[1;32m     33\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\u001B[1;32m     35\u001B[0m \u001B[38;5;66;03m# Await termination to keep the streaming query running\u001B[39;00m\n\u001B[1;32m     36\u001B[0m query\u001B[38;5;241m.\u001B[39mawaitTermination()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1385\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m   1383\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqueryName(queryName)\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mstart(path))\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9695L]\n+- ~EventTimeWatermark TransactionDate#9681: timestamp, 1 days\n   +- ~Project [TransactionID#9529, to_timestamp(TransactionDate#9530, None, TimestampType, Some(Etc/UTC), false) AS TransactionDate#9681, ProductID#9531, Quantity#9532, Price#9533, TotalAmount#9549]\n      +- ~Filter (Quantity#9532 > 1)\n         +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9695L]\n+- ~EventTimeWatermark TransactionDate#9681: timestamp, 1 days\n   +- ~Project [TransactionID#9529, to_timestamp(TransactionDate#9530, None, TimestampType, Some(Etc/UTC), false) AS TransactionDate#9681, ProductID#9531, Quantity#9532, Price#9533, TotalAmount#9549]\n      +- ~Filter (Quantity#9532 > 1)\n         +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n            +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Task 5: Handling Late Data using Watermarks\n",
    "#1. Introduce a watermark on the TransactionDate column to handle late data arriving in the stream.\n",
    "#2. Set the watermark to 1 day to allow late data within a 24-hour period and discard data that is older.\n",
    "from pyspark.sql.functions import col, to_timestamp, sum as sum_\n",
    "\n",
    "transformed_df_with_timestamp = transformed_df \\\n",
    "    .withColumn(\"TransactionDate\", to_timestamp(col(\"TransactionDate\")))\n",
    "\n",
    "watermarked_df = transformed_df_with_timestamp \\\n",
    "    .withWatermark(\"TransactionDate\", \"1 day\")\n",
    "\n",
    "aggregated_df = watermarked_df \\\n",
    "    .groupBy(\"ProductID\") \\\n",
    "    .agg(\n",
    "        sum_(\"TotalAmount\").alias(\"TotalSales\")\n",
    "    )\n",
    "\n",
    "parquet_sink_path = \"/mnt/streaming_parquet_data\"\n",
    "\n",
    "checkpoint_location = \"/mnt/checkpoints/aggregated_sales\"\n",
    "\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", parquet_sink_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37be1653-68eb-4deb-bc00-9ae7636e7083",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Task 6: Streaming from Multiple Sources\n",
    "#1. Simulate a scenario where two streams of data are being ingested:\n",
    "#Stream 1: Incoming transaction data (same as Task 1).\n",
    "#Stream 2: Product information (CSV with columns: ProductID, ProductName, Category).\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, TimestampType\n",
    "\n",
    "transaction_schema = StructType([\n",
    "    StructField(\"TransactionID\", StringType(), True),\n",
    "    StructField(\"TransactionDate\", TimestampType(), True),\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Price\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "product_schema = StructType([\n",
    "    StructField(\"ProductID\", StringType(), True),\n",
    "    StructField(\"ProductName\", StringType(), True),\n",
    "    StructField(\"Category\", StringType(), True)\n",
    "])\n",
    "\n",
    "transaction_stream_df = spark.readStream \\\n",
    "    .schema(transaction_schema) \\\n",
    "    .csv(\"/mnt/streaming_csv_data\")  # Update with the path to your CSV folder\n",
    "\n",
    "product_stream_df = spark.readStream \\\n",
    "    .schema(product_schema) \\\n",
    "    .csv(\"/mnt/streaming_csv_data\")  # Update with the path to your CSV folder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f93cb6e-b1b9-4000-83a9-407973507bd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Perform a join on the two streams using the ProductID column and display the combined stream results.\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "joined_stream_df = transaction_stream_df \\\n",
    "    .join(product_stream_df, on=\"ProductID\", how=\"inner\")\n",
    "\n",
    "display_df = joined_stream_df \\\n",
    "    .select(\n",
    "        col(\"TransactionID\"),\n",
    "        col(\"TransactionDate\"),\n",
    "        col(\"ProductID\"),\n",
    "        col(\"ProductName\"),\n",
    "        col(\"Category\"),\n",
    "        col(\"Quantity\"),\n",
    "        col(\"Price\"),\n",
    "        (col(\"Quantity\") * col(\"Price\")).alias(\"TotalAmount\")\n",
    "    )\n",
    "\n",
    "query = display_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "#query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ab1c2839-dd7b-4a14-be44-fd5cedd18f81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming Query Stopped. Exploring Results...\n"
     ]
    }
   ],
   "source": [
    "#Task 7: Stopping and Restarting Streaming Queries\n",
    "#1. Stop the streaming query and explore the results.\n",
    "query.stop()\n",
    "\n",
    "print(\"Streaming Query Stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81e21129-8ff9-498c-b45d-60efe182d9d6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-1573338673244464>:5\u001B[0m\n",
       "\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#2. Restart the query and ensure that it continues from the last processed data by utilizing the checkpoint.\u001B[39;00m\n",
       "\u001B[1;32m      3\u001B[0m checkpoint_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/checkpoints/aggregated_sales\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m----> 5\u001B[0m query \u001B[38;5;241m=\u001B[39m aggregated_df\u001B[38;5;241m.\u001B[39mwriteStream \\\n",
       "\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, parquet_sink_path) \\\n",
       "\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, checkpoint_location) \\\n",
       "\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1385\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n",
       "\u001B[1;32m   1383\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqueryName(queryName)\n",
       "\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
       "\u001B[0;32m-> 1385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n",
       "\u001B[1;32m   1386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m   1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mstart(path))\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n",
       "\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n",
       "\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n",
       "\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n",
       "\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n",
       "\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n",
       "\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n",
       "\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n",
       "\n",
       "File \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n",
       "\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n",
       "\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n",
       "\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n",
       "\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n",
       "\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n",
       "\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n",
       "~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9733L]\n",
       "+- ~Filter (Quantity#9532 > 1)\n",
       "   +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n",
       "      +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\nFile \u001B[0;32m<command-1573338673244464>:5\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m#2. Restart the query and ensure that it continues from the last processed data by utilizing the checkpoint.\u001B[39;00m\n\u001B[1;32m      3\u001B[0m checkpoint_location \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m/mnt/checkpoints/aggregated_sales\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 5\u001B[0m query \u001B[38;5;241m=\u001B[39m aggregated_df\u001B[38;5;241m.\u001B[39mwriteStream \\\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;241m.\u001B[39moutputMode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mappend\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparquet\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m      8\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpath\u001B[39m\u001B[38;5;124m\"\u001B[39m, parquet_sink_path) \\\n\u001B[1;32m      9\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcheckpointLocation\u001B[39m\u001B[38;5;124m\"\u001B[39m, checkpoint_location) \\\n\u001B[1;32m     10\u001B[0m     \u001B[38;5;241m.\u001B[39mstart()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/sql/streaming/readwriter.py:1385\u001B[0m, in \u001B[0;36mDataStreamWriter.start\u001B[0;34m(self, path, format, outputMode, partitionBy, queryName, **options)\u001B[0m\n\u001B[1;32m   1383\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mqueryName(queryName)\n\u001B[1;32m   1384\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m path \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m-> 1385\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_jwrite\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstart\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m   1386\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1387\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sq(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_jwrite\u001B[38;5;241m.\u001B[39mstart(path))\n\nFile \u001B[0;32m/databricks/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1321\u001B[0m, in \u001B[0;36mJavaMember.__call__\u001B[0;34m(self, *args)\u001B[0m\n\u001B[1;32m   1315\u001B[0m command \u001B[38;5;241m=\u001B[39m proto\u001B[38;5;241m.\u001B[39mCALL_COMMAND_NAME \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1316\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcommand_header \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1317\u001B[0m     args_command \u001B[38;5;241m+\u001B[39m\\\n\u001B[1;32m   1318\u001B[0m     proto\u001B[38;5;241m.\u001B[39mEND_COMMAND_PART\n\u001B[1;32m   1320\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgateway_client\u001B[38;5;241m.\u001B[39msend_command(command)\n\u001B[0;32m-> 1321\u001B[0m return_value \u001B[38;5;241m=\u001B[39m \u001B[43mget_return_value\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1322\u001B[0m \u001B[43m    \u001B[49m\u001B[43manswer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgateway_client\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtarget_id\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1324\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m temp_arg \u001B[38;5;129;01min\u001B[39;00m temp_args:\n\u001B[1;32m   1325\u001B[0m     temp_arg\u001B[38;5;241m.\u001B[39m_detach()\n\nFile \u001B[0;32m/databricks/spark/python/pyspark/errors/exceptions.py:234\u001B[0m, in \u001B[0;36mcapture_sql_exception.<locals>.deco\u001B[0;34m(*a, **kw)\u001B[0m\n\u001B[1;32m    230\u001B[0m converted \u001B[38;5;241m=\u001B[39m convert_exception(e\u001B[38;5;241m.\u001B[39mjava_exception)\n\u001B[1;32m    231\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(converted, UnknownException):\n\u001B[1;32m    232\u001B[0m     \u001B[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001B[39;00m\n\u001B[1;32m    233\u001B[0m     \u001B[38;5;66;03m# JVM exception message.\u001B[39;00m\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m converted \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28mNone\u001B[39m\n\u001B[1;32m    235\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    236\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m\n\n\u001B[0;31mAnalysisException\u001B[0m: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9733L]\n+- ~Filter (Quantity#9532 > 1)\n   +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n      +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n",
       "errorSummary": "<span class='ansi-red-fg'>AnalysisException</span>: Append output mode not supported when there are streaming aggregations on streaming DataFrames/DataSets without watermark;\n~Aggregate [ProductID#9531], [ProductID#9531, sum(TotalAmount#9549) AS TotalSales#9733L]\n+- ~Filter (Quantity#9532 > 1)\n   +- ~Project [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533, (Quantity#9532 * Price#9533) AS TotalAmount#9549]\n      +- ~StreamingRelation DataSource(org.apache.spark.sql.SparkSession@ac26223,csv,List(),Some(StructType(StructField(TransactionID,StringType,true),StructField(TransactionDate,DateType,true),StructField(ProductID,StringType,true),StructField(Quantity,IntegerType,true),StructField(Price,IntegerType,true))),List(),None,Map(path -> /mnt/streaming_csv_data),None), FileSource[/mnt/streaming_csv_data], [TransactionID#9529, TransactionDate#9530, ProductID#9531, Quantity#9532, Price#9533]\n",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#2. Restart the query and ensure that it continues from the last processed data by utilizing the checkpoint.\n",
    "\n",
    "checkpoint_location = \"/mnt/checkpoints/aggregated_sales\"\n",
    "\n",
    "query = aggregated_df.writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"path\", parquet_sink_path) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_location) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "834ad6c9-70d3-42cf-951d-e9679f3e46be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Assignment: Creating a Complete ETL Pipeline using Delta Live Tables (DLT)\n",
    "\n",
    "#Task 1: Create an ETL Pipeline using DLT (Python)\n",
    "#1. Create a Delta Live Table pipeline using PySpark to perform the following:\n",
    "#Read the source data from a CSV or Parquet file.\n",
    "#Transform the data by performing the following:\n",
    "#Add a new column for TotalAmount which is the result of\n",
    "#multiplying Quantity by Price .\n",
    "#Filter records where the Quantity is greater than 1.\n",
    "#Load the transformed data into a Delta table.\n",
    "#2. Ensure the pipeline is repeatable and can handle incremental loads by re-running with new data.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ETL Pipeline with DLT\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "source_df = spark.read.csv(\"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/orders.csv\", header=True, inferSchema=True)\n",
    "\n",
    "transformed_df = source_df \\\n",
    "    .withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\")) \\\n",
    "    .filter(col(\"Quantity\") > 1)\n",
    "\n",
    "transformed_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"delta_table_orders\")\n",
    "\n",
    "existing_df = spark.read.format(\"delta\").table(\"delta_table_orders\")\n",
    "\n",
    "incremental_df = transformed_df.join(\n",
    "    existing_df.select(\"OrderID\"),\n",
    "    on=\"OrderID\",\n",
    "    how=\"left_anti\"  \n",
    ")\n",
    "\n",
    "incremental_df.write.format(\"delta\").mode(\"append\").saveAsTable(\"delta_table_orders\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "53a9a165-5f36-4df2-ba25-636a8146189e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[3]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "#Task 2: Create an ETL Pipeline using DLT (SQL)\n",
    "#1. Create a similar Delta Live Table pipeline using SQL:\n",
    "#Use SQL to read the source data, perform the same transformations (as above), and write the data into a Delta table.\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS source_orders (\n",
    "    OrderID INT,\n",
    "    OrderDate DATE,\n",
    "    CustomerID STRING,\n",
    "    Product STRING,\n",
    "    Quantity INT,\n",
    "    Price DECIMAL(10, 2)\n",
    ")\n",
    "USING DELTA;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f0b049e1-e452-49d2-b7e1-0274d907db2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>5</td><td>5</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         5,
         5
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "\n",
    "-- Load data from CSV into a temporary view\n",
    "CREATE OR REPLACE TEMPORARY VIEW temp_source_orders\n",
    "USING csv\n",
    "OPTIONS (\n",
    "    path 'dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/orders.csv',\n",
    "    header 'true',\n",
    "    inferSchema 'true'\n",
    ");\n",
    "\n",
    "-- Insert data from the temporary view into the Delta table\n",
    "INSERT INTO source_orders\n",
    "SELECT * FROM temp_source_orders;\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "96cc4718-f5c3-41c7-8a12-e507acaffecb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_inserted_rows</th></tr></thead><tbody></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "-- Create or replace the target Delta table for transformed data\n",
    "CREATE OR REPLACE TABLE transformed_orders AS\n",
    "SELECT\n",
    "    OrderID,\n",
    "    OrderDate,\n",
    "    CustomerID,\n",
    "    Product,\n",
    "    Quantity,\n",
    "    Price,\n",
    "    Quantity * Price AS TotalAmount\n",
    "FROM source_orders\n",
    "WHERE Quantity > 1;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3d35dd7-9444-458b-b8d8-095b2657aeab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th><th>num_updated_rows</th><th>num_deleted_rows</th><th>num_inserted_rows</th></tr></thead><tbody><tr><td>3</td><td>3</td><td>0</td><td>0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         3,
         3,
         0,
         0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_updated_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_deleted_rows",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "num_inserted_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Ensure the pipeline can process incremental data without losing records or creating duplicates.\n",
    "-- Upsert new records into the transformed Delta table\n",
    "MERGE INTO transformed_orders AS target\n",
    "USING (\n",
    "    SELECT\n",
    "        OrderID,\n",
    "        OrderDate,\n",
    "        CustomerID,\n",
    "        Product,\n",
    "        Quantity,\n",
    "        Price,\n",
    "        Quantity * Price AS TotalAmount\n",
    "    FROM source_orders\n",
    "    WHERE Quantity > 1\n",
    ") AS source\n",
    "ON target.OrderID = source.OrderID\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET\n",
    "        target.OrderDate = source.OrderDate,\n",
    "        target.CustomerID = source.CustomerID,\n",
    "        target.Product = source.Product,\n",
    "        target.Quantity = source.Quantity,\n",
    "        target.Price = source.Price,\n",
    "        target.TotalAmount = source.TotalAmount\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount)\n",
    "    VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price, source.TotalAmount);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89f09f8a-b544-4340-a337-5dfb78d2d2fe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>OrderID</th><th>OrderDate</th><th>CustomerID</th><th>Product</th><th>Quantity</th><th>Price</th><th>TotalAmount</th></tr></thead><tbody><tr><td>101</td><td>2024-01-01</td><td>C001</td><td>Laptop</td><td>2</td><td>1000.00</td><td>2000.00</td></tr><tr><td>103</td><td>2024-01-03</td><td>C003</td><td>Tablet</td><td>3</td><td>300.00</td><td>900.00</td></tr><tr><td>105</td><td>2024-01-05</td><td>C005</td><td>Mouse</td><td>5</td><td>20.00</td><td>100.00</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         101,
         "2024-01-01",
         "C001",
         "Laptop",
         2,
         "1000.00",
         "2000.00"
        ],
        [
         103,
         "2024-01-03",
         "C003",
         "Tablet",
         3,
         "300.00",
         "900.00"
        ],
        [
         105,
         "2024-01-05",
         "C005",
         "Mouse",
         5,
         "20.00",
         "100.00"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "OrderID",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "OrderDate",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "CustomerID",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Product",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "Quantity",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "Price",
         "type": "\"decimal(10,2)\""
        },
        {
         "metadata": "{}",
         "name": "TotalAmount",
         "type": "\"decimal(21,2)\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--Task 3: Perform Read, Write, Update, and Delete Operations on Delta Table (SQL + PySpark)\n",
    "--1. Read the data from the Delta table created in Task 1 and Task 2.\n",
    "SELECT * FROM transformed_orders;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "50ab459b-32a8-4d9d-b99e-711a207a57bd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-------+-----------+\n|OrderID| OrderDate|CustomerID|Product|Quantity|  Price|TotalAmount|\n+-------+----------+----------+-------+--------+-------+-----------+\n|    101|2024-01-01|      C001| Laptop|       2|1000.00|    2000.00|\n|    103|2024-01-03|      C003| Tablet|       3| 300.00|     900.00|\n|    105|2024-01-05|      C005|  Mouse|       5|  20.00|     100.00|\n+-------+----------+----------+-------+--------+-------+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta Table Operations\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data from the Delta table using PySpark\n",
    "df = spark.read.format(\"delta\").table(\"transformed_orders\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "304c16fc-e1e5-4534-825e-bf9b6457bca8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>num_affected_rows</th></tr></thead><tbody><tr><td>1</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         1
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "num_affected_rows",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "--2. Update the table by changing the price of a product (e.g., increase the price of laptops by 10%).\n",
    "\n",
    "UPDATE transformed_orders\n",
    "SET Price = Price * 1.10\n",
    "WHERE Product = 'Laptop';\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6085e5cc-3b4f-4168-8b63-794ea5096294",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[15]: DataFrame[num_affected_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "#3. Delete rows from the Delta table where the quantity is less than 2.\n",
    "spark.sql(\"\"\"          \n",
    "DELETE FROM transformed_orders\n",
    "WHERE Quantity < 2;\n",
    " \"\"\")\n",
    "#4. Insert a new record into the Delta table using PySpark or SQL.\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO transformed_orders\n",
    "VALUES (106, '2024-01-06', 'C006', 'Keyboard', 3, 50, 150);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fd088b3-e508-4390-ab33-8dbde96896aa",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: DataFrame[]"
     ]
    }
   ],
   "source": [
    "#Task 4: Merge Data (Slowly Changing Dimension - SCD Type 2)\n",
    "#1. Create a new dataset representing updated orders with new prices and products.\n",
    "spark.sql(\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW updated_orders AS\n",
    "SELECT * FROM (\n",
    "    VALUES\n",
    "    (101, '2024-01-10', 'C001', 'Laptop', 2, 1200),\n",
    "    (106, '2024-01-12', 'C006', 'Keyboard', 3, 50)\n",
    ") AS updated_orders(OrderID, OrderDate, CustomerID, Product, Quantity, Price);\n",
    "\n",
    "          \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "771e5255-5a15-4490-80da-d7c035d2d38a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[17]: DataFrame[num_affected_rows: bigint, num_updated_rows: bigint, num_deleted_rows: bigint, num_inserted_rows: bigint]"
     ]
    }
   ],
   "source": [
    "#Implement a MERGE operation to simulate a Slowly Changing Dimension Type 2 (SCD2) scenario. Ensure that:\n",
    "#The Quantity , Price , and TotalAmount columns are updated if there is a match on OrderID .\n",
    "#If no match is found, insert the new record into the Delta table.\n",
    "spark.sql(\"\"\"\n",
    "MERGE INTO transformed_orders AS target\n",
    "USING updated_orders AS source\n",
    "ON target.OrderID = source.OrderID\n",
    "WHEN MATCHED THEN\n",
    "    UPDATE SET \n",
    "        target.Quantity = source.Quantity,\n",
    "        target.Price = source.Price,\n",
    "        target.TotalAmount = source.Quantity * source.Price,\n",
    "        target.OrderDate = source.OrderDate,\n",
    "        target.CustomerID = source.CustomerID,\n",
    "        target.Product = source.Product\n",
    "WHEN NOT MATCHED THEN\n",
    "    INSERT (OrderID, OrderDate, CustomerID, Product, Quantity, Price, TotalAmount)\n",
    "    VALUES (source.OrderID, source.OrderDate, source.CustomerID, source.Product, source.Quantity, source.Price, source.Quantity * source.Price);\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c0185de8-cdc3-406b-bac2-0f9de3414e6d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[18]: DataFrame[format: string, id: string, name: string, description: string, location: string, createdAt: timestamp, lastModified: timestamp, partitionColumns: array<string>, numFiles: bigint, sizeInBytes: bigint, properties: map<string,string>, minReaderVersion: int, minWriterVersion: int, tableFeatures: array<string>, statistics: map<string,bigint>]"
     ]
    }
   ],
   "source": [
    "#Task 5: Explore Delta Table Internals\n",
    "#1. Inspect the Delta table's transaction logs and explore the metadata using SQL queries:\n",
    "#Display the history of changes to the Delta table using the DESCRIBE HISTORY command.\n",
    "spark.sql(\"\"\"\n",
    "         DESCRIBE HISTORY transformed_orders; \n",
    "          \"\"\")\n",
    "#Check the file size and modification times using DESCRIBE DETAIL .\n",
    "spark.sql(\"\"\"\n",
    "          DESCRIBE DETAIL transformed_orders;\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26b45606-758d-4450-9a86-2a614f6a6b06",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[23]: DataFrame[OrderID: int, OrderDate: date, CustomerID: string, Product: string, Quantity: int, Price: decimal(10,2), TotalAmount: decimal(21,2)]"
     ]
    }
   ],
   "source": [
    "#Task 6: Time Travel in Delta Tables\n",
    "#1. Use time travel to query the Delta table as it existed at a previous point in time.\n",
    "#Query the table as it existed before the last merge operation.\n",
    "spark.sql(\"\"\"\n",
    "DESCRIBE HISTORY transformed_orders;\n",
    "\"\"\")\n",
    "#Demonstrate time travel by using both the version of the table and the timestamp.\n",
    "spark.sql(\"\"\"\n",
    "          SELECT * FROM transformed_orders VERSION AS OF 5;\n",
    "          \"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "         SELECT * FROM transformed_orders TIMESTAMP AS OF '2024-09-17T08:56:56';\n",
    "          \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24a5cfbc-0f9e-4d66-979f-fb922bbde458",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[24]: DataFrame[path: string]"
     ]
    }
   ],
   "source": [
    "#Task 7: Optimize Delta Table\n",
    "#1. Optimize the Delta table for faster queries using Z-Ordering.\n",
    "#Optimize the table on the Product column to reduce I/O and improve query performance.\n",
    "spark.sql(\"\"\"\n",
    "         OPTIMIZE transformed_orders\n",
    "ZORDER BY (Product); \n",
    "          \"\"\")\n",
    "#2. Use vacuum to remove any old files that are no longer necessary after the optimization process.\n",
    "spark.sql(\"\"\"\n",
    "          VACUUM transformed_orders;\n",
    "      \"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2fcf123b-1b3d-4431-9f9b-8a47872339c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+----------+-------+--------+-----+\n|OrderID| OrderDate|CustomerID|Product|Quantity|Price|\n+-------+----------+----------+-------+--------+-----+\n|    101|2024-01-01|      C001| Laptop|       2| 1000|\n|    102|2024-01-02|      C002|  Phone|       1|  500|\n|    103|2024-01-03|      C003| Tablet|       3|  300|\n|    104|2024-01-04|      C004|Monitor|       1|  150|\n|    105|2024-01-05|      C005|  Mouse|       5|   20|\n+-------+----------+----------+-------+--------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "#Task 8: Converting Parquet Files to Delta Format\n",
    "#1. You are provided with Parquet files containing historical order data. Convert these files into a Delta table format using either PySpark or SQL.\n",
    "\n",
    "\n",
    "# Save DataFrame as Parquet\n",
    "source_df.write.format(\"parquet\").mode(\"overwrite\").save(\"/mnt/delta/historical_orders_parquet\")\n",
    "\n",
    "source_df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"historical_orders_delta\")\n",
    "\n",
    "#Perform a simple query on the converted Delta table to verify the conversion.\n",
    "spark.sql(\"SELECT * FROM historical_orders_delta\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6919f339-062c-45e1-b823-723d0dbdf354",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+----------+-------+--------+-----+-----------+\n|OrderID|OrderDate|CustomerID|Product|Quantity|Price|TotalAmount|\n+-------+---------+----------+-------+--------+-----+-----------+\n+-------+---------+----------+-------+--------+-----+-----------+\n\n+-------+---------+----------+-------+--------+-----+-----------+\n|OrderID|OrderDate|CustomerID|Product|Quantity|Price|TotalAmount|\n+-------+---------+----------+-------+--------+-----+-----------+\n+-------+---------+----------+-------+--------+-----+-----------+\n\n"
     ]
    }
   ],
   "source": [
    "#Assignment: Creating and Scheduling a Job on Databricks using Notebooks\n",
    "#Task 1: Prepare Your Notebook\n",
    "#1. Create a new Notebook in your Databricks workspace.\n",
    "#Use PySpark for data processing.\n",
    "#In the notebook, read a CSV file (use the provided sample data), perform a transformation, and write the transformed data into a Delta table.\n",
    "#The transformation should include:\n",
    "#Adding a new column ( TotalAmount ) which is the product of Quantity and Price .\n",
    "#Filtering rows where Quantity is greater than 5.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "spark = SparkSession.builder.appName(\"TransformData\").getOrCreate()\n",
    "\n",
    "csv_file_path = \"dbfs:/FileStore/shared_uploads/varshinie.1006@gmail.com/orders.csv\" \n",
    "\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_file_path)\n",
    "\n",
    "df_transformed = df.withColumn(\"TotalAmount\", col(\"Quantity\") * col(\"Price\"))\n",
    "\n",
    "df_filtered = df_transformed.filter(col(\"Quantity\") > 5)\n",
    "df_filtered.show()\n",
    "\n",
    "delta_table_path = \"/mnt/delta/transformed_orders_delta\"  \n",
    "\n",
    "df_filtered.write.format(\"delta\").mode(\"overwrite\").save(delta_table_path)\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS transformed_orders_delta\n",
    "USING DELTA\n",
    "LOCATION '{}'\n",
    "\"\"\".format(delta_table_path))\n",
    "\n",
    "spark.sql(\"SELECT * FROM transformed_orders_delta\").show()\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 2289039131618891,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Hands-on2",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
