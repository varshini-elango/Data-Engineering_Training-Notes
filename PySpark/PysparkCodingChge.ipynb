{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kIAUF4MMG3GU",
        "outputId": "2084d722-8db7-47ba-e7e5-b04713eec89c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=6dc7ec6bf3085bf8d7cacd16df4e02ca27ba5eb0b084ef33f043506ad77debca\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ],
      "source": [
        "pip install pyspark"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# E-commerce Transactions\n",
        "\n",
        "#Exercises:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum,avg,col\n",
        "\n",
        "spark = SparkSession.builder.appName(\"E-commerce Transactions\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/sample_data/Ecommercedata.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#1. Calculate the Total Revenue per Category\n",
        "transactions = df.withColumn(\"revenue\", col(\"price\") * col(\"quantity\") * (1 - col(\"discount_percentage\") / 100))\n",
        "total_revenue_per_category = transactions.groupBy(\"category\").agg(sum(\"revenue\").alias(\"total_revenue\"))\n",
        "print(\"Total Revenue:\")\n",
        "total_revenue_per_category.show()\n",
        "\n",
        "#2. Filter Transactions with a Discount Greater Than 10%\n",
        "transactions_with_discount = df.filter(col(\"discount_percentage\") > 10)\n",
        "print(\"Transactions with Discount Greater Than 10%:\")\n",
        "transactions_with_discount.show()\n",
        "\n",
        "#3. Find the Most Expensive Product Sold\n",
        "most_expensive_product = transactions.orderBy(col(\"price\").desc()).limit(1)\n",
        "print(\"Most Expensive Product Sold:\")\n",
        "most_expensive_product.show()\n",
        "\n",
        "#4. Calculate the Average Quantity of Products Sold per Category\n",
        "average_quantity_per_category = transactions.groupBy(\"category\").agg(avg(\"quantity\").alias(\"average_quantity\"))\n",
        "print(\"Average Quantity of Products Sold per Category:\")\n",
        "average_quantity_per_category.show()\n",
        "\n",
        "#5. Identify Customers Who Purchased More Than One Product\n",
        "customers_multiple_purchases = transactions.filter(col(\"quantity\") > 1)\n",
        "print(\"Customers Who Purchased More Than One Product:\")\n",
        "customers_multiple_purchases.show()\n",
        "\n",
        "#6. Find the Top 3 Highest Revenue Transactions\n",
        "top_3_revenue = transactions.withColumn(\"revenue\", col(\"price\") * col(\"quantity\") * (1 - col(\"discount_percentage\") / 100))\\\n",
        "                                         .orderBy(col(\"revenue\").desc())\\\n",
        "                                         .limit(3)\n",
        "print(\"Top 3 Highest Revenue Transactions:\")\n",
        "top_3_revenue.show()\n",
        "\n",
        "#7. Calculate the Total Number of Transactions per Day\n",
        "transactions_per_day = transactions.groupBy(\"transaction_date\").count().orderBy(\"transaction_date\")\n",
        "print(\"Total Number of Transactions per Day:\")\n",
        "transactions_per_day.show()\n",
        "\n",
        "#8. Find the Customer Who Spent the Most Money\n",
        "customer_total_spent = transactions.groupBy(\"customer_id\").agg(sum(\"revenue\").alias(\"total_spent\"))\n",
        "customer_most_spent = customer_total_spent.orderBy(col(\"total_spent\").desc()).limit(1)\n",
        "print(\"Customer Who Spent the Most Money:\")\n",
        "customer_most_spent.show()\n",
        "\n",
        "#9. Calculate the Average Discount Given per Product Category\n",
        "average_discount_per_category = transactions.groupBy(\"category\").agg(avg(\"discount_percentage\").alias(\"average_discount\"))\n",
        "print(\"Average Discount Given per Product Category:\")\n",
        "average_discount_per_category.show()\n",
        "\n",
        "#10. Create a New Column for Final Price After Discount\n",
        "transactions_with_final_price = transactions.withColumn(\"final_price\", col(\"price\") * (1 - col(\"discount_percentage\") / 100))\n",
        "print(\"New Column for Final Price After Discount:\")\n",
        "transactions_with_final_price.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "HgI7G6_bHMvW",
        "outputId": "c1e83f49-08ca-459c-e163-f2fc1222723d"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue:\n",
            "+--------------+-------------+\n",
            "|      category|total_revenue|\n",
            "+--------------+-------------+\n",
            "|       Fashion|        168.0|\n",
            "|   Electronics|       2950.0|\n",
            "|         Books|         80.0|\n",
            "|Home Appliance|        756.0|\n",
            "+--------------+-------------+\n",
            "\n",
            "Transactions with Discount Greater Than 10%:\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+\n",
            "\n",
            "Most Expensive Product Sold:\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|product|   category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|             1|        101| Laptop|Electronics| 1000|       1|                 10|      2023-08-01|  900.0|\n",
            "+--------------+-----------+-------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Average Quantity of Products Sold per Category:\n",
            "+--------------+----------------+\n",
            "|      category|average_quantity|\n",
            "+--------------+----------------+\n",
            "|       Fashion|             2.0|\n",
            "|   Electronics|            1.75|\n",
            "|         Books|             4.0|\n",
            "|Home Appliance|             1.0|\n",
            "+--------------+----------------+\n",
            "\n",
            "Customers Who Purchased More Than One Product:\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|   product|   category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "|             2|        102|Smartphone|Electronics|  700|       2|                  5|      2023-08-01| 1330.0|\n",
            "|             3|        103|     Shirt|    Fashion|   40|       3|                  0|      2023-08-02|  120.0|\n",
            "|             5|        101|Headphones|Electronics|  100|       2|                 10|      2023-08-03|  180.0|\n",
            "|             8|        107|      Book|      Books|   20|       4|                  0|      2023-08-05|   80.0|\n",
            "|            10|        102|    Tablet|Electronics|  300|       2|                 10|      2023-08-06|  540.0|\n",
            "+--------------+-----------+----------+-----------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Top 3 Highest Revenue Transactions:\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01| 1330.0|\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|  900.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|  600.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+\n",
            "\n",
            "Total Number of Transactions per Day:\n",
            "+----------------+-----+\n",
            "|transaction_date|count|\n",
            "+----------------+-----+\n",
            "|      2023-08-01|    2|\n",
            "|      2023-08-02|    1|\n",
            "|      2023-08-03|    2|\n",
            "|      2023-08-04|    1|\n",
            "|      2023-08-05|    2|\n",
            "|      2023-08-06|    2|\n",
            "+----------------+-----+\n",
            "\n",
            "Customer Who Spent the Most Money:\n",
            "+-----------+-----------+\n",
            "|customer_id|total_spent|\n",
            "+-----------+-----------+\n",
            "|        102|     1870.0|\n",
            "+-----------+-----------+\n",
            "\n",
            "Average Discount Given per Product Category:\n",
            "+--------------+----------------+\n",
            "|      category|average_discount|\n",
            "+--------------+----------------+\n",
            "|       Fashion|            10.0|\n",
            "|   Electronics|            8.75|\n",
            "|         Books|             0.0|\n",
            "|Home Appliance|            15.0|\n",
            "+--------------+----------------+\n",
            "\n",
            "New Column for Final Price After Discount:\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+-----------+\n",
            "|transaction_id|customer_id|     product|      category|price|quantity|discount_percentage|transaction_date|revenue|final_price|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+-----------+\n",
            "|             1|        101|      Laptop|   Electronics| 1000|       1|                 10|      2023-08-01|  900.0|      900.0|\n",
            "|             2|        102|  Smartphone|   Electronics|  700|       2|                  5|      2023-08-01| 1330.0|      665.0|\n",
            "|             3|        103|       Shirt|       Fashion|   40|       3|                  0|      2023-08-02|  120.0|       40.0|\n",
            "|             4|        104|     Blender|Home Appliance|  150|       1|                 15|      2023-08-03|  127.5|      127.5|\n",
            "|             5|        101|  Headphones|   Electronics|  100|       2|                 10|      2023-08-03|  180.0|       90.0|\n",
            "|             6|        105|       Shoes|       Fashion|   60|       1|                 20|      2023-08-04|   48.0|       48.0|\n",
            "|             7|        106|Refrigerator|Home Appliance|  800|       1|                 25|      2023-08-05|  600.0|      600.0|\n",
            "|             8|        107|        Book|         Books|   20|       4|                  0|      2023-08-05|   80.0|       20.0|\n",
            "|             9|        108|     Toaster|Home Appliance|   30|       1|                  5|      2023-08-06|   28.5|       28.5|\n",
            "|            10|        102|      Tablet|   Electronics|  300|       2|                 10|      2023-08-06|  540.0|      270.0|\n",
            "+--------------+-----------+------------+--------------+-----+--------+-------------------+----------------+-------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Banking Transactions\n",
        "\n",
        "#Exercises:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum,avg,col,when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Banking Transactions\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/sample_data/bankdata.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#1. Calculate the Total Deposit and Withdrawal Amounts\n",
        "total_amounts = df.groupBy(\"transaction_type\").agg(sum(\"amount\").alias(\"total_amount\"))\n",
        "print(\"Total Deposit and Withdrawal Amounts:\")\n",
        "total_amounts.show()\n",
        "\n",
        "#2. Filter Transactions Greater Than $3,000\n",
        "transactions_above_3000 = df.filter(col(\"amount\") > 3000)\n",
        "print(\"Transactions Greater Than $3,000:\")\n",
        "transactions_above_3000.show()\n",
        "\n",
        "#3. Find the Largest Deposit Made\n",
        "largest_deposit = df.filter(col(\"transaction_type\") == \"Deposit\").orderBy(col(\"amount\").desc()).limit(1)\n",
        "print(\"Largest Deposit Made:\")\n",
        "largest_deposit.show()\n",
        "\n",
        "#4. Calculate the Average Transaction Amount for Each Transaction Type\n",
        "average_amount_per_type = df.groupBy(\"transaction_type\").agg(avg(\"amount\").alias(\"average_amount\"))\n",
        "print(\"Average Transaction Amount for Each Transaction Type:\")\n",
        "average_amount_per_type.show()\n",
        "\n",
        "#5. Find Customers Who Made Both Deposits and Withdrawals\n",
        "customers_deposits_withdrawals = df.groupBy(\"customer_id\").pivot(\"transaction_type\").agg(sum(\"amount\"))\n",
        "customers_deposits_withdrawals = customers_deposits_withdrawals.filter((col(\"Deposit\") > 0) & (col(\"Withdrawal\") > 0))\n",
        "print(\"Customers Who Made Both Deposits and Withdrawals:\")\n",
        "customers_deposits_withdrawals.show()\n",
        "\n",
        "#6. Calculate the Total Amount of Transactions per Day\n",
        "transactions_per_day = df.groupBy(\"transaction_date\").agg(sum(\"amount\").alias(\"total_amount\"))\n",
        "print(\"Total Amount of Transactions per Day:\")\n",
        "transactions_per_day.show()\n",
        "\n",
        "#7. Find the Customer with the Highest Total Withdrawal\n",
        "total_withdrawals = df.filter(col(\"transaction_type\") == \"Withdrawal\").groupBy(\"customer_id\").agg(sum(\"amount\").alias(\"total_withdrawn\"))\n",
        "customer_with_max_withdrawal = total_withdrawals.orderBy(col(\"total_withdrawn\").desc()).limit(1)\n",
        "print(\"Customer with the Highest Total Withdrawal:\")\n",
        "customer_with_max_withdrawal.show()\n",
        "\n",
        "#8. Calculate the Number of Transactions for Each Customer\n",
        "transactions_per_customer = df.groupBy(\"customer_id\").count()\n",
        "print(\"Number of Transactions for Each Customer:\")\n",
        "transactions_per_customer.show()\n",
        "\n",
        "#9. Find All Transactions That Occurred on the Same Day as a Withdrawal Greater than $1,000\n",
        "withdrawal_dates = df.filter((col(\"transaction_type\") == \"Withdrawal\") & (col(\"amount\") > 1000))\\\n",
        "                               .select(\"transaction_date\").distinct()\n",
        "\n",
        "same_day_transactions = df.join(withdrawal_dates, \"transaction_date\", \"inner\")\n",
        "print(\"All Transactions That Occurred on the Same Day as a Withdrawal Greater than $1,000:\")\n",
        "same_day_transactions.show()\n",
        "\n",
        "#10. Create a New Column to Classify Transactions as \"High\" or \"Low\" Value\n",
        "transactions_with_value_class = df.withColumn(\"transaction_value\", when(col(\"amount\") > 5000, \"High\").otherwise(\"Low\"))\n",
        "print(\"New Column to Classify Transactions as High or Low Value:\")\n",
        "transactions_with_value_class.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "04ckly98SyTB",
        "outputId": "73b1b5a2-ac5d-4d41-88c7-808f0016392d"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Deposit and Withdrawal Amounts:\n",
            "+----------------+------------+\n",
            "|transaction_type|total_amount|\n",
            "+----------------+------------+\n",
            "|         Deposit|       24500|\n",
            "|      Withdrawal|        7700|\n",
            "+----------------+------------+\n",
            "\n",
            "Transactions Greater Than $3,000:\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|             1|        201|         Deposit|  5000|      2023-09-01|\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|\n",
            "|             9|        203|         Deposit|  4000|      2023-09-05|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "\n",
            "Largest Deposit Made:\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|\n",
            "+--------------+-----------+----------------+------+----------------+\n",
            "\n",
            "Average Transaction Amount for Each Transaction Type:\n",
            "+----------------+--------------+\n",
            "|transaction_type|average_amount|\n",
            "+----------------+--------------+\n",
            "|         Deposit|        4900.0|\n",
            "|      Withdrawal|        1540.0|\n",
            "+----------------+--------------+\n",
            "\n",
            "Customers Who Made Both Deposits and Withdrawals:\n",
            "+-----------+-------+----------+\n",
            "|customer_id|Deposit|Withdrawal|\n",
            "+-----------+-------+----------+\n",
            "|        202|   2500|      2000|\n",
            "|        204|  10000|      3000|\n",
            "|        201|   5000|      1500|\n",
            "+-----------+-------+----------+\n",
            "\n",
            "Total Amount of Transactions per Day:\n",
            "+----------------+------------+\n",
            "|transaction_date|total_amount|\n",
            "+----------------+------------+\n",
            "|      2023-09-03|       10500|\n",
            "|      2023-09-01|        7000|\n",
            "|      2023-09-05|        7000|\n",
            "|      2023-09-02|        4500|\n",
            "|      2023-09-04|        3200|\n",
            "+----------------+------------+\n",
            "\n",
            "Customer with the Highest Total Withdrawal:\n",
            "+-----------+---------------+\n",
            "|customer_id|total_withdrawn|\n",
            "+-----------+---------------+\n",
            "|        204|           3000|\n",
            "+-----------+---------------+\n",
            "\n",
            "Number of Transactions for Each Customer:\n",
            "+-----------+-----+\n",
            "|customer_id|count|\n",
            "+-----------+-----+\n",
            "|        206|    1|\n",
            "|        205|    1|\n",
            "|        202|    2|\n",
            "|        203|    2|\n",
            "|        204|    2|\n",
            "|        201|    2|\n",
            "+-----------+-----+\n",
            "\n",
            "All Transactions That Occurred on the Same Day as a Withdrawal Greater than $1,000:\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "|transaction_date|transaction_id|customer_id|transaction_type|amount|\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "|      2023-09-01|             1|        201|         Deposit|  5000|\n",
            "|      2023-09-01|             2|        202|      Withdrawal|  2000|\n",
            "|      2023-09-02|             3|        203|         Deposit|  3000|\n",
            "|      2023-09-02|             4|        201|      Withdrawal|  1500|\n",
            "|      2023-09-05|             9|        203|         Deposit|  4000|\n",
            "|      2023-09-05|            10|        204|      Withdrawal|  3000|\n",
            "+----------------+--------------+-----------+----------------+------+\n",
            "\n",
            "New Column to Classify Transactions as High or Low Value:\n",
            "+--------------+-----------+----------------+------+----------------+-----------------+\n",
            "|transaction_id|customer_id|transaction_type|amount|transaction_date|transaction_value|\n",
            "+--------------+-----------+----------------+------+----------------+-----------------+\n",
            "|             1|        201|         Deposit|  5000|      2023-09-01|              Low|\n",
            "|             2|        202|      Withdrawal|  2000|      2023-09-01|              Low|\n",
            "|             3|        203|         Deposit|  3000|      2023-09-02|              Low|\n",
            "|             4|        201|      Withdrawal|  1500|      2023-09-02|              Low|\n",
            "|             5|        204|         Deposit| 10000|      2023-09-03|             High|\n",
            "|             6|        205|      Withdrawal|   500|      2023-09-03|              Low|\n",
            "|             7|        202|         Deposit|  2500|      2023-09-04|              Low|\n",
            "|             8|        206|      Withdrawal|   700|      2023-09-04|              Low|\n",
            "|             9|        203|         Deposit|  4000|      2023-09-05|              Low|\n",
            "|            10|        204|      Withdrawal|  3000|      2023-09-05|              Low|\n",
            "+--------------+-----------+----------------+------+----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Health & Fitness Tracker Data\n",
        "\n",
        "#Exercises:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum,avg,col,row_number,collect_set,size,count,when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Fitness Tracker\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/sample_data/healthdata.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#1. Find the Total Steps Taken by Each User\n",
        "total_steps_per_user = df.groupBy(\"user_id\").agg(sum(\"steps\").alias(\"total_steps\"))\n",
        "print(\"Total Steps Taken by Each User:\")\n",
        "total_steps_per_user.show()\n",
        "\n",
        "#2. Filter Days with More Than 10,000 Steps\n",
        "high_activity_days = df.filter(col(\"steps\") > 10000)\n",
        "print(\"Days with More Than 10,000 Steps:\")\n",
        "high_activity_days.show()\n",
        "\n",
        "#3. Calculate the Average Calories Burned by Workout Type\n",
        "average_calories_per_workout = df.groupBy(\"workout_type\").agg(avg(\"calories_burned\").alias(\"average_calories\"))\n",
        "print(\"Average Calories Burned by Workout Type:\")\n",
        "average_calories_per_workout.show()\n",
        "\n",
        "#4. Identify the Day with the Most Steps for Each User\n",
        "from pyspark.sql.window import Window\n",
        "window = Window.partitionBy(\"user_id\").orderBy(col(\"steps\").desc())\n",
        "\n",
        "most_steps_per_user = df.withColumn(\"row_num\", row_number().over(window)).filter(col(\"row_num\") == 1)\n",
        "print(\"Day with the Most Steps for Each User:\")\n",
        "most_steps_per_user.select(\"user_id\", \"date\", \"steps\").show()\n",
        "\n",
        "#5. Find Users Who Burned More Than 600 Calories on Any Day\n",
        "high_calorie_users = df.filter(col(\"calories_burned\") > 600)\n",
        "print(\"Users Who Burned More Than 600 Calories on Any Day:\")\n",
        "high_calorie_users.show()\n",
        "\n",
        "#6. Calculate the Average Hours of Sleep per User\n",
        "average_sleep_per_user = df.groupBy(\"user_id\").agg(avg(\"hours_of_sleep\").alias(\"average_sleep\"))\n",
        "print(\"Average Hours of Sleep per User:\")\n",
        "average_sleep_per_user.show()\n",
        "\n",
        "#7. Find the Total Calories Burned per Day\n",
        "total_calories_per_day = df.groupBy(\"date\").agg(sum(\"calories_burned\").alias(\"total_calories\"))\n",
        "print(\"Total Calories Burned per Day:\")\n",
        "total_calories_per_day.show()\n",
        "\n",
        "#8. Identify Users Who Did Different Types of Workouts\n",
        "users_multiple_workouts = df.groupBy(\"user_id\").agg(collect_set(\"workout_type\").alias(\"workout_types\"))\n",
        "\n",
        "users_multiple_workouts = users_multiple_workouts.filter(size(col(\"workout_types\")) > 1)\n",
        "print(\"Users Who Did Different Types of Workouts:\")\n",
        "users_multiple_workouts.show()\n",
        "\n",
        "#9. Calculate the Total Number of Workouts per User\n",
        "total_workouts_per_user = df.groupBy(\"user_id\").agg(count(\"workout_type\").alias(\"total_workouts\"))\n",
        "print(\"Total Number of Workouts per User:\")\n",
        "total_workouts_per_user.show()\n",
        "\n",
        "#10. Create a New Column for \"Active\" Days\n",
        "tracker_data_with_active_days = df.withColumn(\"active_day\", when(col(\"steps\") > 10000, \"Active\").otherwise(\"Inactive\"))\n",
        "print(\"New Column for Active Days:\")\n",
        "tracker_data_with_active_days.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "uQcqG0F7cyDC",
        "outputId": "bfa510cf-5b26-4981-dd07-538625242fc7"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Steps Taken by Each User:\n",
            "+-------+-----------+\n",
            "|user_id|total_steps|\n",
            "+-------+-----------+\n",
            "|      1|      35000|\n",
            "|      3|      45000|\n",
            "|      2|      29500|\n",
            "+-------+-----------+\n",
            "\n",
            "Days with More Than 10,000 Steps:\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n",
            "Average Calories Burned by Workout Type:\n",
            "+------------+-----------------+\n",
            "|workout_type| average_calories|\n",
            "+------------+-----------------+\n",
            "|    Strength|            500.0|\n",
            "|        Yoga|573.3333333333334|\n",
            "|      Cardio|            537.5|\n",
            "+------------+-----------------+\n",
            "\n",
            "Day with the Most Steps for Each User:\n",
            "+-------+----------+-----+\n",
            "|user_id|      date|steps|\n",
            "+-------+----------+-----+\n",
            "|      1|2023-09-03|13000|\n",
            "|      2|2023-09-03|12000|\n",
            "|      3|2023-09-03|16000|\n",
            "+-------+----------+-----+\n",
            "\n",
            "Users Who Burned More Than 600 Calories on Any Day:\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|\n",
            "+-------+----------+-----+---------------+--------------+------------+\n",
            "\n",
            "Average Hours of Sleep per User:\n",
            "+-------+-----------------+\n",
            "|user_id|    average_sleep|\n",
            "+-------+-----------------+\n",
            "|      1|              7.0|\n",
            "|      3|              7.5|\n",
            "|      2|6.666666666666667|\n",
            "+-------+-----------------+\n",
            "\n",
            "Total Calories Burned per Day:\n",
            "+----------+--------------+\n",
            "|      date|total_calories|\n",
            "+----------+--------------+\n",
            "|2023-09-03|          1770|\n",
            "|2023-09-01|          1550|\n",
            "|2023-09-02|          1550|\n",
            "+----------+--------------+\n",
            "\n",
            "Users Who Did Different Types of Workouts:\n",
            "+-------+--------------------+\n",
            "|user_id|       workout_types|\n",
            "+-------+--------------------+\n",
            "|      1|      [Yoga, Cardio]|\n",
            "|      3|[Yoga, Cardio, St...|\n",
            "|      2|[Yoga, Cardio, St...|\n",
            "+-------+--------------------+\n",
            "\n",
            "Total Number of Workouts per User:\n",
            "+-------+--------------+\n",
            "|user_id|total_workouts|\n",
            "+-------+--------------+\n",
            "|      1|             3|\n",
            "|      3|             3|\n",
            "|      2|             3|\n",
            "+-------+--------------+\n",
            "\n",
            "New Column for Active Days:\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|user_id|      date|steps|calories_burned|hours_of_sleep|workout_type|active_day|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "|      1|2023-09-01|12000|            500|           7.0|      Cardio|    Active|\n",
            "|      2|2023-09-01| 8000|            400|           6.5|    Strength|  Inactive|\n",
            "|      3|2023-09-01|15000|            650|           8.0|        Yoga|    Active|\n",
            "|      1|2023-09-02|10000|            450|           6.0|      Cardio|  Inactive|\n",
            "|      2|2023-09-02| 9500|            500|           7.0|      Cardio|  Inactive|\n",
            "|      3|2023-09-02|14000|            600|           7.5|    Strength|    Active|\n",
            "|      1|2023-09-03|13000|            550|           8.0|        Yoga|    Active|\n",
            "|      2|2023-09-03|12000|            520|           6.5|        Yoga|    Active|\n",
            "|      3|2023-09-03|16000|            700|           7.0|      Cardio|    Active|\n",
            "+-------+----------+-----+---------------+--------------+------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Music Streaming\n",
        "\n",
        "#Exercises:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum,avg,col,row_number,collect_set,size,count,when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Music Streaming\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/sample_data/musicdata.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#1. Calculate the Total Listening Time for Each User\n",
        "total_listening_time = df.groupBy(\"user_id\").agg(sum(\"duration_seconds\").alias(\"total_listening_time\"))\n",
        "print(\"Total Listening Time for Each User:\")\n",
        "total_listening_time.show()\n",
        "\n",
        "#2. Filter Songs Streamed for More Than 200 Seconds\n",
        "long_songs = df.filter(col(\"duration_seconds\") > 200)\n",
        "print(\"Songs Streamed for More Than 200 Seconds:\")\n",
        "long_songs.show()\n",
        "\n",
        "#3. Find the Most Popular Artist (by Total Streams)\n",
        "most_popular_artist = df.groupBy(\"artist\").agg(count(\"song_title\").alias(\"total_streams\")).orderBy(col(\"total_streams\").desc())\n",
        "print(\"Most Popular Artist:\")\n",
        "most_popular_artist.show(1)\n",
        "\n",
        "#4. Identify the Song with the Longest Duration\n",
        "longest_song = df.orderBy(col(\"duration_seconds\").desc()).limit(1)\n",
        "print(\"Song with the Longest Duration:\")\n",
        "longest_song.show()\n",
        "\n",
        "#5. Calculate the Average Song Duration by Artist\n",
        "average_duration_per_artist = df.groupBy(\"artist\").agg(avg(\"duration_seconds\").alias(\"average_duration\"))\n",
        "print(\"Average Song Duration by Artist:\")\n",
        "average_duration_per_artist.show()\n",
        "\n",
        "#6. Find the Top 3 Most Streamed Songs per User\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "window = Window.partitionBy(\"user_id\").orderBy(col(\"duration_seconds\").desc())\n",
        "\n",
        "top_songs_per_user = df.withColumn(\"rank\", row_number().over(window)).filter(col(\"rank\") <= 3)\n",
        "print(\"Top 3 Most Streamed Songs per User:\")\n",
        "top_songs_per_user.show()\n",
        "\n",
        "#7. Calculate the Total Number of Streams per Day\n",
        "from pyspark.sql.functions import to_date\n",
        "\n",
        "streams_per_day = df.withColumn(\"streaming_date\", to_date(\"streaming_time\")).groupBy(\"streaming_date\").count()\n",
        "print(\"Total Number of Streams per Day:\")\n",
        "streams_per_day.show()\n",
        "\n",
        "#8. Identify Users Who Streamed Songs from More Than One Artist\n",
        "users_multiple_artists = df.groupBy(\"user_id\").agg(collect_set(\"artist\").alias(\"artists_list\"))\n",
        "users_multiple_artists = users_multiple_artists.filter(size(col(\"artists_list\")) > 1)\n",
        "print(\"Users Who Streamed Songs from More Than One Artist:\")\n",
        "users_multiple_artists.show()\n",
        "\n",
        "#9. Calculate the Total Streams for Each Location\n",
        "streams_per_location = df.groupBy(\"location\").count().alias(\"total_streams\")\n",
        "print(\"Total Streams for Each Location:\")\n",
        "streams_per_location.show()\n",
        "\n",
        "#10. Create a New Column to Classify Long and Short Songs\n",
        "df_with_song_length = df.withColumn(\"song_length\", when(col(\"duration_seconds\") > 200, \"Long\").otherwise(\"Short\"))\n",
        "print(\"New Column to Classify Long and Short Songs:\")\n",
        "df_with_song_length.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "78Sml9YRkYB4",
        "outputId": "06f12cf7-931b-4dc5-a773-0c4c72fba9b3"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Listening Time for Each User:\n",
            "+-------+--------------------+\n",
            "|user_id|total_listening_time|\n",
            "+-------+--------------------+\n",
            "|      1|                 630|\n",
            "|      3|                 610|\n",
            "|      2|                 680|\n",
            "+-------+--------------------+\n",
            "\n",
            "Songs Streamed for More Than 200 Seconds:\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "Most Popular Artist:\n",
            "+--------+-------------+\n",
            "|  artist|total_streams|\n",
            "+--------+-------------+\n",
            "|Dua Lipa|            3|\n",
            "+--------+-------------+\n",
            "only showing top 1 row\n",
            "\n",
            "Song with the Longest Duration:\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "|user_id|song_title|    artist|duration_seconds|     streaming_time|   location|\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "|      2|   Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|\n",
            "+-------+----------+----------+----------------+-------------------+-----------+\n",
            "\n",
            "Average Song Duration by Artist:\n",
            "+----------+------------------+\n",
            "|    artist|  average_duration|\n",
            "+----------+------------------+\n",
            "|  Dua Lipa|203.33333333333334|\n",
            "|Ed Sheeran|226.66666666666666|\n",
            "|The Weeknd|             210.0|\n",
            "+----------+------------------+\n",
            "\n",
            "Top 3 Most Streamed Songs per User:\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+----+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|rank|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+----+\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|   1|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|   2|\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|   3|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|   1|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|   2|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|   3|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|   1|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|   2|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|   3|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+----+\n",
            "\n",
            "Total Number of Streams per Day:\n",
            "+--------------+-----+\n",
            "|streaming_date|count|\n",
            "+--------------+-----+\n",
            "|    2023-09-01|    5|\n",
            "|    2023-09-02|    4|\n",
            "+--------------+-----+\n",
            "\n",
            "Users Who Streamed Songs from More Than One Artist:\n",
            "+-------+------------+\n",
            "|user_id|artists_list|\n",
            "+-------+------------+\n",
            "+-------+------------+\n",
            "\n",
            "Total Streams for Each Location:\n",
            "+-----------+-----+\n",
            "|   location|count|\n",
            "+-----------+-----+\n",
            "|Los Angeles|    3|\n",
            "|     London|    3|\n",
            "|   New York|    3|\n",
            "+-----------+-----+\n",
            "\n",
            "New Column to Classify Long and Short Songs:\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|user_id|     song_title|    artist|duration_seconds|     streaming_time|   location|song_length|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "|      1|Blinding Lights|The Weeknd|             200|2023-09-01 08:15:00|   New York|      Short|\n",
            "|      2|   Shape of You|Ed Sheeran|             240|2023-09-01 09:20:00|Los Angeles|       Long|\n",
            "|      3|     Levitating|  Dua Lipa|             180|2023-09-01 10:30:00|     London|      Short|\n",
            "|      1|        Starboy|The Weeknd|             220|2023-09-01 11:00:00|   New York|       Long|\n",
            "|      2|        Perfect|Ed Sheeran|             250|2023-09-01 12:15:00|Los Angeles|       Long|\n",
            "|      3|Don't Start Now|  Dua Lipa|             200|2023-09-02 08:10:00|     London|      Short|\n",
            "|      1|Save Your Tears|The Weeknd|             210|2023-09-02 09:00:00|   New York|       Long|\n",
            "|      2|    Galway Girl|Ed Sheeran|             190|2023-09-02 10:00:00|Los Angeles|      Short|\n",
            "|      3|      New Rules|  Dua Lipa|             230|2023-09-02 11:00:00|     London|       Long|\n",
            "+-------+---------------+----------+----------------+-------------------+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Retail Store Sales\n",
        "\n",
        "#Exercises:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import sum,avg,col,row_number,collect_set,size,count,when\n",
        "\n",
        "spark = SparkSession.builder.appName(\"Retail Store Sales\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"/content/sample_data/retailsalesdata.csv\", header=True, inferSchema=True)\n",
        "\n",
        "#1. Calculate the Total Revenue per Category\n",
        "print(\"Total Revenue per Category:\")\n",
        "df.withColumn('revenue', col('price') * col('quantity')) \\\n",
        "  .groupBy('category') \\\n",
        "  .agg(sum('revenue').alias('total_revenue')) \\\n",
        "  .show()\n",
        "\n",
        "#2. Filter Transactions Where the Total Sales Amount is Greater Than $100\n",
        "print(\"Transactions Where the Total Sales Amount is Greater Than $100:\")\n",
        "df.withColumn('total_sales', col('price') * col('quantity')) \\\n",
        "  .filter(col('total_sales') > 100) \\\n",
        "  .show()\n",
        "\n",
        "#3. Find the Most Sold Product\n",
        "print(\"Most Sold Product:\")\n",
        "df.groupBy('product_name') \\\n",
        "  .agg(sum('quantity').alias('total_quantity')) \\\n",
        "  .orderBy(col('total_quantity').desc()) \\\n",
        "  .limit(1) \\\n",
        "  .show()\n",
        "\n",
        "#4. Calculate the Average Price per Product Category\n",
        "print(\"Average Price per Product Category:\")\n",
        "df.groupBy('category') \\\n",
        "  .agg(avg('price').alias('average_price')) \\\n",
        "  .show()\n",
        "\n",
        "#5. Find the Top 3 Highest Grossing Products\n",
        "print(\"Top 3 Highest Grossing Products:\")\n",
        "df.withColumn('revenue', col('price') * col('quantity')) \\\n",
        "  .groupBy('product_name') \\\n",
        "  .agg(sum('revenue').alias('total_revenue')) \\\n",
        "  .orderBy(col('total_revenue').desc()) \\\n",
        "  .show(3)\n",
        "\n",
        "#6. Calculate the Total Number of Items Sold per Day\n",
        "print(\"Total Number of Items Sold per Day:\")\n",
        "df.groupBy('sales_date') \\\n",
        "  .agg(sum('quantity').alias('total_quantity')) \\\n",
        "  .show()\n",
        "\n",
        "#7. Identify the Product with the Lowest Price in Each Category\n",
        "window = Window.partitionBy('category').orderBy(col('price'))\n",
        "print(\"Product with the Lowest Price in Each Category:\")\n",
        "df.withColumn('rank', row_number().over(window)) \\\n",
        "  .filter(col('rank') == 1) \\\n",
        "  .select('category', 'product_name', 'price') \\\n",
        "  .show()\n",
        "\n",
        "#8. Calculate the Total Revenue for Each Product\n",
        "print(\"Total Revenue for Each Product:\")\n",
        "df.withColumn('revenue', col('price') * col('quantity')) \\\n",
        "  .groupBy('product_name') \\\n",
        "  .agg(sum('revenue').alias('total_revenue')) \\\n",
        "  .show()\n",
        "\n",
        "#9. Find the Total Sales per Day for Each Category\n",
        "print(\"Total Sales per Day for Each Category:\")\n",
        "df.withColumn('revenue', col('price') * col('quantity')) \\\n",
        "  .groupBy('sales_date', 'category') \\\n",
        "  .agg(sum('revenue').alias('total_sales')) \\\n",
        "  .show()\n",
        "\n",
        "#10. Create a New Column for Discounted Price\n",
        "print(\"New Column for Discounted Price:\")\n",
        "df.withColumn('discounted_price', col('price') * 0.9) \\\n",
        "  .show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWZWK1LItXvd",
        "outputId": "58a24aad-de8d-49ad-fb5f-3fb2034566f9"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Revenue per Category:\n",
            "+-----------+------------------+\n",
            "|   category|     total_revenue|\n",
            "+-----------+------------------+\n",
            "| Stationery|              20.0|\n",
            "|  Groceries|13.399999999999999|\n",
            "|Electronics|            1000.0|\n",
            "|   Clothing|             155.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "Transactions Where the Total Sales Amount is Greater Than $100:\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|total_sales|\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|      800.0|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|      200.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+-----------+\n",
            "\n",
            "Most Sold Product:\n",
            "+------------+--------------+\n",
            "|product_name|total_quantity|\n",
            "+------------+--------------+\n",
            "|      Banana|            12|\n",
            "+------------+--------------+\n",
            "\n",
            "Average Price per Product Category:\n",
            "+-----------+------------------+\n",
            "|   category|     average_price|\n",
            "+-----------+------------------+\n",
            "| Stationery|               1.5|\n",
            "|  Groceries|0.4666666666666666|\n",
            "|Electronics|             450.0|\n",
            "|   Clothing|              30.0|\n",
            "+-----------+------------------+\n",
            "\n",
            "Top 3 Highest Grossing Products:\n",
            "+------------+-------------+\n",
            "|product_name|total_revenue|\n",
            "+------------+-------------+\n",
            "|      Laptop|        800.0|\n",
            "|  Headphones|        200.0|\n",
            "|       Pants|         75.0|\n",
            "+------------+-------------+\n",
            "only showing top 3 rows\n",
            "\n",
            "Total Number of Items Sold per Day:\n",
            "+----------+--------------+\n",
            "|sales_date|total_quantity|\n",
            "+----------+--------------+\n",
            "|2023-09-03|             4|\n",
            "|2023-09-01|            12|\n",
            "|2023-09-05|             9|\n",
            "|2023-09-02|            17|\n",
            "|2023-09-04|            12|\n",
            "+----------+--------------+\n",
            "\n",
            "Product with the Lowest Price in Each Category:\n",
            "+-----------+------------+-----+\n",
            "|   category|product_name|price|\n",
            "+-----------+------------+-----+\n",
            "|   Clothing|     T-shirt| 15.0|\n",
            "|Electronics|  Headphones|100.0|\n",
            "|  Groceries|      Banana|  0.3|\n",
            "| Stationery|         Pen|  1.0|\n",
            "+-----------+------------+-----+\n",
            "\n",
            "Total Revenue for Each Product:\n",
            "+------------+------------------+\n",
            "|product_name|     total_revenue|\n",
            "+------------+------------------+\n",
            "|     T-shirt|              30.0|\n",
            "|    Sneakers|              50.0|\n",
            "|      Orange|               4.8|\n",
            "|      Banana|3.5999999999999996|\n",
            "|         Pen|              10.0|\n",
            "|       Pants|              75.0|\n",
            "|      Laptop|             800.0|\n",
            "|    Notebook|              10.0|\n",
            "|       Apple|               5.0|\n",
            "|  Headphones|             200.0|\n",
            "+------------+------------------+\n",
            "\n",
            "Total Sales per Day for Each Category:\n",
            "+----------+-----------+------------------+\n",
            "|sales_date|   category|       total_sales|\n",
            "+----------+-----------+------------------+\n",
            "|2023-09-03|Electronics|             800.0|\n",
            "|2023-09-01|  Groceries|               5.0|\n",
            "|2023-09-01|   Clothing|              30.0|\n",
            "|2023-09-02| Stationery|              10.0|\n",
            "|2023-09-04| Stationery|              10.0|\n",
            "|2023-09-02|  Groceries|3.5999999999999996|\n",
            "|2023-09-05|  Groceries|               4.8|\n",
            "|2023-09-05|   Clothing|              50.0|\n",
            "|2023-09-03|   Clothing|              75.0|\n",
            "|2023-09-04|Electronics|             200.0|\n",
            "+----------+-----------+------------------+\n",
            "\n",
            "New Column for Discounted Price:\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|transaction_id|product_name|   category|price|quantity|sales_date|discounted_price|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "|             1|       Apple|  Groceries|  0.5|      10|2023-09-01|            0.45|\n",
            "|             2|     T-shirt|   Clothing| 15.0|       2|2023-09-01|            13.5|\n",
            "|             3|    Notebook| Stationery|  2.0|       5|2023-09-02|             1.8|\n",
            "|             4|      Banana|  Groceries|  0.3|      12|2023-09-02|            0.27|\n",
            "|             5|      Laptop|Electronics|800.0|       1|2023-09-03|           720.0|\n",
            "|             6|       Pants|   Clothing| 25.0|       3|2023-09-03|            22.5|\n",
            "|             7|  Headphones|Electronics|100.0|       2|2023-09-04|            90.0|\n",
            "|             8|         Pen| Stationery|  1.0|      10|2023-09-04|             0.9|\n",
            "|             9|      Orange|  Groceries|  0.6|       8|2023-09-05|            0.54|\n",
            "|            10|    Sneakers|   Clothing| 50.0|       1|2023-09-05|            45.0|\n",
            "+--------------+------------+-----------+-----+--------+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}