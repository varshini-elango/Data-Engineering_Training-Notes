{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4etoQP2U4P-",
        "outputId": "e851057a-232a-440e-ce04-83a0a2f44aa4",
        "collapsed": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=dbd6f15e5ddccc6216cb42a0e7872525f49f38597a395ee433abb64700c120b7\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#initialize sparkSession\n",
        "spark =  SparkSession.builder\\\n",
        "    .appName(\"Pyspark notebook example\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkK3pUlYU3fl",
        "outputId": "7b77b65a-900c-4ec0-8e25-6557ce4d01cc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x792c90adf0d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "#Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"PySpark DataFrame Example\") \\\n",
        ".getOrCreate()\n",
        "#Sample data representing employees\n",
        "data = [\n",
        "(\"John Doe\", \"Engineering\", 75000),\n",
        "(\"Jane Smith\", \"Marketing\", 60000),\n",
        "(\"Sam Brown\", \"Engineering\", 80000),\n",
        "(\"Emily Davis\", \"HR\", 50000),\n",
        "(\"Michael Johnson\", \"Marketing\", 70000),\n",
        "]\n",
        "#Define schema for DataFrame\n",
        "columns= [\"Name\", \"Department\", \"Salary\"]\n",
        "#Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "#Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF4S7VbSV6RP",
        "outputId": "12582b34-9d09-4d2f-c8ff-675c21519c04",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|     Jane Smith|  Marketing| 60000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|    Emily Davis|         HR| 50000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter: Select employees with a salary greater than 65,000\n",
        "high_salary_df = df.filter (col (\"Salary\") > 65000)\n",
        "print(\"Employees with Salary > 65,000:\")\n",
        "high_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Aip9uf5Yjm4",
        "outputId": "39ef1cba-4788-41cc-8028-18a05062dfac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Salary > 65,000:\n",
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Department and calculate the average salary\n",
        "avg_salary_df = df.groupBy(\"Department\").avg(\"Salary\")\n",
        "print(\"Average Salary by Department:\")\n",
        "avg_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8abCBdVlZZrS",
        "outputId": "0e7462a1-10f1-49c1-ecdf-d525c918cd8c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Salary by Department:\n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|Engineering|    77500.0|\n",
            "|  Marketing|    65000.0|\n",
            "|         HR|    50000.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "#Initialize SparkSession\n",
        "spark = SparkSession.builder\\\n",
        ".appName(\"Customer Transactions Analysis\") \\\n",
        ".getOrCreate ()\n",
        "# Sample data for customers\n",
        "customers = [\n",
        "(1, \"Ravi\", \"Mumbai\"),\n",
        "(2, \"Priya\", \"Delhi\"),\n",
        "(3, \"Vijay\", \"Bangalore\"),\n",
        "(4, \"Anita\", \"Chennai\"),\n",
        "(5, \"Raj\", \"Hyderabad\"),\n",
        "]\n",
        "# Sample data for transactions\n",
        "transactions = [\n",
        "(1, 1, 10000.50),\n",
        "(2, 2, 20000.75),\n",
        "(3, 1, 15000.25),\n",
        "(4, 3, 30000.00),\n",
        "(5, 2, 40000.50),\n",
        "(6, 4, 25000.00),\n",
        "(7, 5, 18000.75),\n",
        "(8, 1, 5000.00),\n",
        "]\n",
        "\n",
        "#Define schema for DataFrames\n",
        "customer_columns = [\"CustomerID\", \"Name\", \"City\"]\n",
        "transaction_columns = [\"TransactionID\", \"CustomerID\", \"Amount\"]\n",
        "#Create DataFrames\n",
        "customer_df = spark.createDataFrame (customers, schema=customer_columns)\n",
        "transaction_df = spark.createDataFrame (transactions, schema=transaction_columns)\n",
        "\n",
        "#Show the DataFrames\n",
        "print(\"Customers DataFrame:\")\n",
        "customer_df.show()\n",
        "print(\"Transactions DataFrame:\")\n",
        "transaction_df.show()"
      ],
      "metadata": {
        "id": "0O6Jq2L08rB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "df328db9-edab-4eb5-ea4e-6ca579bdf8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers DataFrame:\n",
            "+----------+-----+---------+\n",
            "|CustomerID| Name|     City|\n",
            "+----------+-----+---------+\n",
            "|         1| Ravi|   Mumbai|\n",
            "|         2|Priya|    Delhi|\n",
            "|         3|Vijay|Bangalore|\n",
            "|         4|Anita|  Chennai|\n",
            "|         5|  Raj|Hyderabad|\n",
            "+----------+-----+---------+\n",
            "\n",
            "Transactions DataFrame:\n",
            "+-------------+----------+--------+\n",
            "|TransactionID|CustomerID|  Amount|\n",
            "+-------------+----------+--------+\n",
            "|            1|         1| 10000.5|\n",
            "|            2|         2|20000.75|\n",
            "|            3|         1|15000.25|\n",
            "|            4|         3| 30000.0|\n",
            "|            5|         2| 40000.5|\n",
            "|            6|         4| 25000.0|\n",
            "|            7|         5|18000.75|\n",
            "|            8|         1|  5000.0|\n",
            "+-------------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Join the DataFrames on CustomerID\n",
        "customer_transactions_df = customer_df.join(transaction_df, on=\"CustomerID\")\n",
        "print(\"Customer Transactions DataFrame:\")\n",
        "customer_transactions_df.show()\n",
        "\n",
        "#Calculate the total amount spent by each customer\n",
        "total_spent_df = customer_transactions_df.groupBy(\"Name\").sum(\"Amount\").withColumnRenamed (\"sum (Amount)\", \"TotalSpent\")\n",
        "print(\"Total Amount Spent by Each Customer:\")\n",
        "total_spent_df.show()\n",
        "\n",
        "## Find customers who have spent more than ₹30,000\n",
        "big_spenders_df = total_spent_df.filter (col (\"TotalSpent\") > 30000)\n",
        "print(\"Customers Who Spent More Than ₹30,000:\")\n",
        "big_spenders_df.show()\n",
        "\n",
        "## Count the number of transactions per customer\n",
        "transactions_count_df = customer_transactions_df.groupBy(\"Name\").count().withColumnRenamed (\"count\", \"TransactionCount\")\n",
        "print(\"Number of Transactions Per Customer:\")\n",
        "transactions_count_df.show()\n",
        "\n",
        "## Sort customers by total amount spent in descending order\n",
        "sorted_spenders_df = total_spent_df.orderBy (col (\"TotalSpent\").desc())\n",
        "print(\"Customers Sorted by Total Spent (Descending):\")\n",
        "sorted_spenders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "collapsed": true,
        "id": "SACY9KBpE5Ky",
        "outputId": "60c1ec01-aff4-4ed1-b678-14c9c8a04204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Transactions DataFrame:\n",
            "+----------+-----+---------+-------------+--------+\n",
            "|CustomerID| Name|     City|TransactionID|  Amount|\n",
            "+----------+-----+---------+-------------+--------+\n",
            "|         1| Ravi|   Mumbai|            1| 10000.5|\n",
            "|         1| Ravi|   Mumbai|            3|15000.25|\n",
            "|         1| Ravi|   Mumbai|            8|  5000.0|\n",
            "|         2|Priya|    Delhi|            2|20000.75|\n",
            "|         2|Priya|    Delhi|            5| 40000.5|\n",
            "|         3|Vijay|Bangalore|            4| 30000.0|\n",
            "|         4|Anita|  Chennai|            6| 25000.0|\n",
            "|         5|  Raj|Hyderabad|            7|18000.75|\n",
            "+----------+-----+---------+-------------+--------+\n",
            "\n",
            "Total Amount Spent by Each Customer:\n",
            "+-----+-----------+\n",
            "| Name|sum(Amount)|\n",
            "+-----+-----------+\n",
            "| Ravi|   30000.75|\n",
            "|Priya|   60001.25|\n",
            "|Vijay|    30000.0|\n",
            "|Anita|    25000.0|\n",
            "|  Raj|   18000.75|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TotalSpent` cannot be resolved. Did you mean one of the following? [`Name`, `sum(Amount)`].;\n'Filter ('TotalSpent > 30000)\n+- Aggregate [Name#55], [Name#55, sum(Amount#62) AS sum(Amount)#226]\n   +- Project [CustomerID#54L, Name#55, City#56, TransactionID#60L, Amount#62]\n      +- Join Inner, (CustomerID#54L = CustomerID#61L)\n         :- LogicalRDD [CustomerID#54L, Name#55, City#56], false\n         +- LogicalRDD [TransactionID#60L, CustomerID#61L, Amount#62], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ab289b7fcfdb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m## Find customers who have spent more than ₹30,000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mbig_spenders_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_spent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"TotalSpent\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customers Who Spent More Than ₹30,000:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mbig_spenders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3329\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3330\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3331\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m             raise PySparkTypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TotalSpent` cannot be resolved. Did you mean one of the following? [`Name`, `sum(Amount)`].;\n'Filter ('TotalSpent > 30000)\n+- Aggregate [Name#55], [Name#55, sum(Amount#62) AS sum(Amount)#226]\n   +- Project [CustomerID#54L, Name#55, City#56, TransactionID#60L, Amount#62]\n      +- Join Inner, (CustomerID#54L = CustomerID#61L)\n         :- LogicalRDD [CustomerID#54L, Name#55, City#56], false\n         +- LogicalRDD [TransactionID#60L, CustomerID#61L, Amount#62], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Exercise: Product Sales Analysis**\n",
        "#### **Step 1: Create DataFrames**\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Product Sales Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data for products\n",
        "products = [\n",
        "    (1, \"Laptop\", \"Electronics\", 50000),\n",
        "    (2, \"Smartphone\", \"Electronics\", 30000),\n",
        "    (3, \"Table\", \"Furniture\", 15000),\n",
        "    (4, \"Chair\", \"Furniture\", 5000),\n",
        "    (5, \"Headphones\", \"Electronics\", 2000),\n",
        "]\n",
        "\n",
        "# Sample data for sales transactions\n",
        "sales = [\n",
        "    (1, 1, 2),\n",
        "    (2, 2, 1),\n",
        "    (3, 3, 3),\n",
        "    (4, 1, 1),\n",
        "    (5, 4, 5),\n",
        "    (6, 2, 2),\n",
        "    (7, 5, 10),\n",
        "    (8, 3, 1),\n",
        "]\n",
        "\n",
        "#Define schema for DataFrames\n",
        "product_columns = [\"ProductID\", \"Name\", \"Category\", \"Price\"]\n",
        "sales_columns = [\"SaleID\", \"ProductID\", \"Quantity\"]\n",
        "#Create DataFrames\n",
        "product_df = spark.createDataFrame (products, schema=product_columns)\n",
        "transaction_columns = [\"TransactionID\", \"CustomerID\", \"Amount\"]\n",
        "#Create DataFrames\n",
        "product_df = spark.createDataFrame (products, schema=product_columns)\n",
        "sales_df = spark.createDataFrame (sales, schema=sales_columns)\n",
        "\n",
        "#Show the DataFrames\n",
        "print(\"Products DataFrame:\")\n",
        "product_df.show()\n",
        "print(\"Sales DataFrame:\")\n",
        "sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3maMjzXL4RW",
        "outputId": "ac5f2879-1e31-41f2-a16a-05611802eb1b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame:\n",
            "+---------+----------+-----------+-----+\n",
            "|ProductID|      Name|   Category|Price|\n",
            "+---------+----------+-----------+-----+\n",
            "|        1|    Laptop|Electronics|50000|\n",
            "|        2|Smartphone|Electronics|30000|\n",
            "|        3|     Table|  Furniture|15000|\n",
            "|        4|     Chair|  Furniture| 5000|\n",
            "|        5|Headphones|Electronics| 2000|\n",
            "+---------+----------+-----------+-----+\n",
            "\n",
            "Sales DataFrame:\n",
            "+------+---------+--------+\n",
            "|SaleID|ProductID|Quantity|\n",
            "+------+---------+--------+\n",
            "|     1|        1|       2|\n",
            "|     2|        2|       1|\n",
            "|     3|        3|       3|\n",
            "|     4|        1|       1|\n",
            "|     5|        4|       5|\n",
            "|     6|        2|       2|\n",
            "|     7|        5|      10|\n",
            "|     8|        3|       1|\n",
            "+------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 2: Perform the Following Tasks**\n",
        "#1. **Join the DataFrames:**\n",
        "\n",
        "product_sales_df = product_df.join(sales_df, on=\"ProductID\")\n",
        "print(\"Product Sales DataFrame:\")\n",
        "product_sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTEIC4rOT46f",
        "outputId": "24873e51-150d-4fb1-9393-8509d9b745cb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product Sales DataFrame:\n",
            "+---------+----------+-----------+-----+------+--------+\n",
            "|ProductID|      Name|   Category|Price|SaleID|Quantity|\n",
            "+---------+----------+-----------+-----+------+--------+\n",
            "|        1|    Laptop|Electronics|50000|     1|       2|\n",
            "|        1|    Laptop|Electronics|50000|     4|       1|\n",
            "|        2|Smartphone|Electronics|30000|     2|       1|\n",
            "|        2|Smartphone|Electronics|30000|     6|       2|\n",
            "|        3|     Table|  Furniture|15000|     3|       3|\n",
            "|        3|     Table|  Furniture|15000|     8|       1|\n",
            "|        4|     Chair|  Furniture| 5000|     5|       5|\n",
            "|        5|Headphones|Electronics| 2000|     7|      10|\n",
            "+---------+----------+-----------+-----+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. **Calculate Total Sales Value:**\n",
        "combined_df = product_sales_df.withColumn(\"TotalSalesValue\", col(\"Price\") * col(\"Quantity\"))\n",
        "print(\"Combined DataFrame with Total Sales Value:\")\n",
        "combined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRHCf0vwVDuT",
        "outputId": "c3909c53-03be-4163-ff98-d2ee548c74db",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame with Total Sales Value:\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|ProductID|      Name|   Category|Price|SaleID|Quantity|TotalSalesValue|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|        1|    Laptop|Electronics|50000|     1|       2|         100000|\n",
            "|        1|    Laptop|Electronics|50000|     4|       1|          50000|\n",
            "|        2|Smartphone|Electronics|30000|     2|       1|          30000|\n",
            "|        2|Smartphone|Electronics|30000|     6|       2|          60000|\n",
            "|        3|     Table|  Furniture|15000|     3|       3|          45000|\n",
            "|        3|     Table|  Furniture|15000|     8|       1|          15000|\n",
            "|        4|     Chair|  Furniture| 5000|     5|       5|          25000|\n",
            "|        5|Headphones|Electronics| 2000|     7|      10|          20000|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. **Find the Total Sales for Each Product Category:**\n",
        "category_sales_df = combined_df.groupBy(\"Category\").agg({\"TotalSalesValue\": \"sum\"}).withColumnRenamed(\"sum(TotalSalesValue)\", \"TotalCategorySales\")\n",
        "print(\"Total Sales for Each Product Category:\")\n",
        "category_sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsZyaBZ4WNNB",
        "outputId": "d6ccbe3c-f25f-4fdf-fc9d-9a04c9d7c6cd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sales for Each Product Category:\n",
            "+-----------+------------------+\n",
            "|   Category|TotalCategorySales|\n",
            "+-----------+------------------+\n",
            "|Electronics|            260000|\n",
            "|  Furniture|             85000|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. **Identify the Top-Selling Product:**\n",
        "product_sales_df = combined_df.groupBy(\"ProductID\", \"Name\").agg({\"TotalSalesValue\": \"sum\"}).withColumnRenamed(\"sum(TotalSalesValue)\", \"TotalProductSales\")\n",
        "top_selling_product_df = product_sales_df.orderBy(col(\"TotalProductSales\").desc()).limit(1)\n",
        "print(\"Top-Selling Product:\")\n",
        "top_selling_product_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR_-1kV_WlHm",
        "outputId": "65fdd1e6-5acb-4e97-f6a8-7ad18a160fd6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-Selling Product:\n",
            "+---------+------+-----------------+\n",
            "|ProductID|  Name|TotalProductSales|\n",
            "+---------+------+-----------------+\n",
            "|        1|Laptop|           150000|\n",
            "+---------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. **Sort the Products by Total Sales Value:**\n",
        "sorted_products_df = combined_df.orderBy(col(\"TotalSalesValue\").desc())\n",
        "print(\"Products Sorted by Total Sales Value:\")\n",
        "sorted_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4xOC3ipX3nw",
        "outputId": "f64214cb-3b7a-4c6c-9400-93633600e33a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products Sorted by Total Sales Value:\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|ProductID|      Name|   Category|Price|SaleID|Quantity|TotalSalesValue|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|        1|    Laptop|Electronics|50000|     1|       2|         100000|\n",
            "|        2|Smartphone|Electronics|30000|     6|       2|          60000|\n",
            "|        1|    Laptop|Electronics|50000|     4|       1|          50000|\n",
            "|        3|     Table|  Furniture|15000|     3|       3|          45000|\n",
            "|        2|Smartphone|Electronics|30000|     2|       1|          30000|\n",
            "|        4|     Chair|  Furniture| 5000|     5|       5|          25000|\n",
            "|        5|Headphones|Electronics| 2000|     7|      10|          20000|\n",
            "|        3|     Table|  Furniture|15000|     8|       1|          15000|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. **Count the Number of Sales for Each Product:**\n",
        "sales_count_df = sales_df.groupBy(\"ProductID\").count().withColumnRenamed(\"count\", \"NumberOfSales\")\n",
        "print(\"Number of Sales for Each Product:\")\n",
        "sales_count_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjzt7BB9YaOt",
        "outputId": "99397079-048a-4cc7-ba4c-0ce58f528b35",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sales for Each Product:\n",
            "+---------+-------------+\n",
            "|ProductID|NumberOfSales|\n",
            "+---------+-------------+\n",
            "|        1|            2|\n",
            "|        3|            2|\n",
            "|        2|            2|\n",
            "|        5|            1|\n",
            "|        4|            1|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. **Filter the Products with Total Sales Value Greater Than ₹50,000:**\n",
        "product_sales_df = product_sales_df.withColumnRenamed(\"sum(TotalSalesValue)\", \"TotalProductSales\")\n",
        "filtered_products_df = product_sales_df.filter(col(\"TotalProductSales\") > 50000)\n",
        "print(\"Products with Total Sales Value > ₹50,000:\")\n",
        "filtered_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM8YyYyfYnNu",
        "outputId": "7812e675-cd46-422e-e23c-ea963af2200c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with Total Sales Value > ₹50,000:\n",
            "+---------+----------+-----------------+\n",
            "|ProductID|      Name|TotalProductSales|\n",
            "+---------+----------+-----------------+\n",
            "|        1|    Laptop|           150000|\n",
            "|        2|Smartphone|            90000|\n",
            "|        3|     Table|            60000|\n",
            "+---------+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Exercise: Analyzing a Sample Sales Dataset Using PySpark**\n",
        "### **Part 1: Dataset Preparation**\n",
        "\n",
        "#### **Step 1: Generate the Sample Sales Dataset**\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Sample sales data\n",
        "data = {\n",
        "       \"TransactionID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "       \"CustomerID\": [101, 102, 103, 101, 104, 102, 103, 104, 101, 105],\n",
        "       \"ProductID\": [501, 502, 501, 503, 504, 502, 503, 504, 501, 505],\n",
        "       \"Quantity\": [2, 1, 4, 3, 1, 2, 5, 1, 2, 1],\n",
        "       \"Price\": [150.0, 250.0, 150.0, 300.0, 450.0, 250.0, 300.0, 450.0, 150.0, 550.0],\n",
        "       \"Date\": [\n",
        "           datetime(2024, 9, 1),\n",
        "           datetime(2024, 9, 1),\n",
        "           datetime(2024, 9, 2),\n",
        "           datetime(2024, 9, 2),\n",
        "           datetime(2024, 9, 3),\n",
        "           datetime(2024, 9, 3),\n",
        "           datetime(2024, 9, 4),\n",
        "           datetime(2024, 9, 4),\n",
        "           datetime(2024, 9, 5),\n",
        "           datetime(2024, 9, 5)\n",
        "       ]\n",
        "   }\n",
        "\n",
        "# Create a DataFrame\n",
        "pandas_df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "pandas_df.to_csv('sales_data.csv', index=False)\n",
        "\n",
        "print(\"Sample sales dataset has been created and saved as 'sales_data.csv'.\")\n",
        "\n",
        "#2. **Verify the Dataset:**\n",
        "pandas_df = pd.read_csv('sales_data.csv')\n",
        "print(pandas_df)\n"
      ],
      "metadata": {
        "id": "8niuLH-Abrw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1fd124-b606-4980-c8f8-e2e585127739",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample sales dataset has been created and saved as 'sales_data.csv'.\n",
            "   TransactionID  CustomerID  ProductID  Quantity  Price        Date\n",
            "0              1         101        501         2  150.0  2024-09-01\n",
            "1              2         102        502         1  250.0  2024-09-01\n",
            "2              3         103        501         4  150.0  2024-09-02\n",
            "3              4         101        503         3  300.0  2024-09-02\n",
            "4              5         104        504         1  450.0  2024-09-03\n",
            "5              6         102        502         2  250.0  2024-09-03\n",
            "6              7         103        503         5  300.0  2024-09-04\n",
            "7              8         104        504         1  450.0  2024-09-04\n",
            "8              9         101        501         2  150.0  2024-09-05\n",
            "9             10         105        505         1  550.0  2024-09-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 2: Load the Dataset into PySpark**\n",
        "\n",
        "#1. **Initialize the SparkSession:**\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sales Dataset Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#2. **Load the CSV File into a PySpark DataFrame:**\n",
        "df = spark.read.csv('sales_data.csv', header=True, inferSchema=True)\n",
        "#Display the first few rows\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kYRMJt2n6EK",
        "outputId": "f43425f4-6e1d-400f-9c56-a99391fa4d15",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|            1|       101|      501|       2|150.0|2024-09-01|\n",
            "|            2|       102|      502|       1|250.0|2024-09-01|\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|\n",
            "|            5|       104|      504|       1|450.0|2024-09-03|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 3: Explore the Data**\n",
        "#1. **Print the Schema:**\n",
        "df.printSchema()\n",
        "\n",
        "#2. **Show the First Few Rows:**\n",
        "df.show(5)\n",
        "\n",
        "#3. **Get Summary Statistics:**\n",
        "df.describe(\"Quantity\", \"Price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCJ_DmMHpnfT",
        "outputId": "ff28f22f-7bfd-4d5b-e3e1-804d6ff7b5ae",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TransactionID: integer (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- ProductID: integer (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            "\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|            1|       101|      501|       2|150.0|2024-09-01|\n",
            "|            2|       102|      502|       1|250.0|2024-09-01|\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|\n",
            "|            5|       104|      504|       1|450.0|2024-09-03|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+-----------------+-----------------+\n",
            "|summary|         Quantity|            Price|\n",
            "+-------+-----------------+-----------------+\n",
            "|  count|               10|               10|\n",
            "|   mean|              2.2|            300.0|\n",
            "| stddev|1.398411797560202|141.4213562373095|\n",
            "|    min|                1|            150.0|\n",
            "|    max|                5|            550.0|\n",
            "+-------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 4: Perform Data Transformations and Analysis**\n",
        "#1. **Calculate the Total Sales Value for Each Transaction:**\n",
        "df = df.withColumn(\"TotalSales\", col(\"Quantity\") * col(\"Price\"))\n",
        "df.show()\n",
        "\n",
        "#2. **Group By ProductID and Calculate Total Sales Per Product:**\n",
        "df.groupBy(\"ProductID\").sum(\"TotalSales\").alias(\"TotalProductSales\").show()\n",
        "\n",
        "#3. **Identify the Top-Selling Product:**\n",
        "from pyspark.sql.functions import desc\n",
        "df.groupBy(\"ProductID\").sum(\"TotalSales\").alias(\"TotalProductSales\").orderBy(desc(\"sum(TotalSales)\")).show(1)\n",
        "\n",
        "#4. **Calculate the Total Sales by Date:**\n",
        "df.groupBy(\"Date\").sum(\"TotalSales\").alias(\"TotalSalesByDate\").orderBy(\"Date\").show()\n",
        "\n",
        "#5. **Filter High-Value Transactions:**\n",
        "df.filter(col(\"TotalSales\") > 500).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ggm1_8eqKV5",
        "outputId": "989e88b6-2f65-4036-842d-ccaa2bf90f58",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|TotalSalesValue|TotalSales|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|            1|       101|      501|       2|150.0|2024-09-01|          300.0|     300.0|\n",
            "|            2|       102|      502|       1|250.0|2024-09-01|          250.0|     250.0|\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|          600.0|     600.0|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|          900.0|     900.0|\n",
            "|            5|       104|      504|       1|450.0|2024-09-03|          450.0|     450.0|\n",
            "|            6|       102|      502|       2|250.0|2024-09-03|          500.0|     500.0|\n",
            "|            7|       103|      503|       5|300.0|2024-09-04|         1500.0|    1500.0|\n",
            "|            8|       104|      504|       1|450.0|2024-09-04|          450.0|     450.0|\n",
            "|            9|       101|      501|       2|150.0|2024-09-05|          300.0|     300.0|\n",
            "|           10|       105|      505|       1|550.0|2024-09-05|          550.0|     550.0|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "\n",
            "+---------+---------------+\n",
            "|ProductID|sum(TotalSales)|\n",
            "+---------+---------------+\n",
            "|      501|         1200.0|\n",
            "|      504|          900.0|\n",
            "|      502|          750.0|\n",
            "|      505|          550.0|\n",
            "|      503|         2400.0|\n",
            "+---------+---------------+\n",
            "\n",
            "+---------+---------------+\n",
            "|ProductID|sum(TotalSales)|\n",
            "+---------+---------------+\n",
            "|      503|         2400.0|\n",
            "+---------+---------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+----------+---------------+\n",
            "|      Date|sum(TotalSales)|\n",
            "+----------+---------------+\n",
            "|2024-09-01|          550.0|\n",
            "|2024-09-02|         1500.0|\n",
            "|2024-09-03|          950.0|\n",
            "|2024-09-04|         1950.0|\n",
            "|2024-09-05|          850.0|\n",
            "+----------+---------------+\n",
            "\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|TotalSalesValue|TotalSales|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|          600.0|     600.0|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|          900.0|     900.0|\n",
            "|            7|       103|      503|       5|300.0|2024-09-04|         1500.0|    1500.0|\n",
            "|           10|       105|      505|       1|550.0|2024-09-05|          550.0|     550.0|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Additional Challenge (Optional):**\n",
        "#1. **Identify Repeat Customers:**\n",
        "df.groupBy(\"CustomerID\").count().filter(col(\"count\") > 1).show()\n",
        "\n",
        "#2. **Calculate the Average Sale Price Per Product:**\n",
        "df.groupBy(\"ProductID\").avg(\"Price\").alias(\"AvgPricePerProduct\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RpqqJlw_uA9V",
        "outputId": "294f7e3e-1c69-445b-fe7f-8beea39b4932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|CustomerID|count|\n",
            "+----------+-----+\n",
            "|       101|    3|\n",
            "|       103|    2|\n",
            "|       102|    2|\n",
            "|       104|    2|\n",
            "+----------+-----+\n",
            "\n",
            "+---------+----------+\n",
            "|ProductID|avg(Price)|\n",
            "+---------+----------+\n",
            "|      501|     150.0|\n",
            "|      504|     450.0|\n",
            "|      502|     250.0|\n",
            "|      505|     550.0|\n",
            "|      503|     300.0|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"RDD Transformation Example\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "# Get the SparkContext from the SparkSession\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark Session Created\")\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Print the original RDD\n",
        "print(\"Original RDD:\", rdd.collect())\n",
        "\n",
        "rdd2 =  rdd.map(lambda x: x* 2)\n",
        "\n",
        "#Print the transformed RDD\n",
        "print(\"RDD after map transformation (x2):\", rdd2.collect())\n",
        "\n"
      ],
      "metadata": {
        "id": "ii5Kd-L-y601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7917e0e3-3fd8-4686-fdce-0881670c771a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Created\n",
            "Original RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "RDD after map transformation (x2): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = rdd2.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "#Print the filtered RDD\n",
        "print(\"RDD after filter transformation (even numbers):\", rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DeBe5bJQNm3N",
        "outputId": "864c6a56-aa44-4a1b-95bf-2d9e7c649182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD after filter transformation (even numbers): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Hello world\", \"PySpark is great\", \"RDD transformations\"]\n",
        "rdd4 = sc.parallelize (sentences)\n",
        "words_rdd = rdd4.flatMap(lambda sentence: sentence.split(\" \"))\n",
        "\n",
        "# Print the flatMapped RDD\n",
        "print(\"RDD after flatMap transformation (split into words):\", words_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VMnsYOFbNz43",
        "outputId": "9730efeb-ca25-4f17-b682-616236523b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD after flatMap transformation (split into words): ['Hello', 'world', 'PySpark', 'is', 'great', 'RDD', 'transformations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rdd3.collect()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-4AXZhi6O4S-",
        "outputId": "f1b9b909-a359-4446-bc14-2bd679e5f361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count =  rdd3.count()\n",
        "print(f\"Number of Elements: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p2Edy_ZyRv8F",
        "outputId": "d10d7bfc-da80-4610-dd75-2be07f739141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Elements: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_sum = rdd.reduce(lambda x, y: x + y)\n",
        "print(f\"Total Sum: {total_sum}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9wERJSw3R-dM",
        "outputId": "13e5c609-cc47-4822-f20b-8be66e95cfa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sum: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Exercise: Working with Key-Value Pair RDDs in PySpark**\n",
        "### **Step 1: Initialize Spark Context**\n",
        "\n",
        "#1. **Initialize SparkSession and SparkContext:**\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"Key-Value Pair RDD Transformations\") \\\n",
        ".getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark Session Created\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QP8qISTGTZyT",
        "outputId": "98020faa-01dd-468b-9bef-1f8a0674896f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 2: Create and Explore the RDD**\n",
        "#2. **Task 1: Create an RDD from the Sales Data**\n",
        "sales_data = [\n",
        "    (\"ProductA\", 100),\n",
        "    (\"ProductB\", 150),\n",
        "    (\"ProductA\", 200),\n",
        "    (\"ProductC\", 300),\n",
        "    (\"ProductB\", 250),\n",
        "    (\"ProductC\", 100)\n",
        "]\n",
        "\n",
        "sales_rdd = sc.parallelize(sales_data)\n",
        "\n",
        "# Print the first few elements of the RDD\n",
        "print(sales_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OwrwG9p8WvDd",
        "outputId": "dc7e4aef-a102-46e7-de54-c56e51c20d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ProductA', 100), ('ProductB', 150), ('ProductA', 200), ('ProductC', 300), ('ProductB', 250), ('ProductC', 100)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 3: Grouping and Aggregating Data**\n",
        "\n",
        "#3. **Task 2: Group Data by Product Name**\n",
        "grouped_sales_rdd = sales_rdd.groupByKey()\n",
        "\n",
        "# To see the grouped data\n",
        "grouped_sales = grouped_sales_rdd.mapValues(list).collect()\n",
        "print(\"grouped sales:\")\n",
        "print(grouped_sales)\n",
        "\n",
        "#4. **Task 3: Calculate Total Sales by Product**\n",
        "total_sales_rdd = sales_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the total sales for each product\n",
        "print(\"Total Sales by Product:\")\n",
        "print(total_sales_rdd.collect())\n",
        "\n",
        "#5. **Task 4: Sort Products by Total Sales**\n",
        "sorted_sales_rdd = total_sales_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Print the sorted list of products along with their sales amounts\n",
        "print(\"Sorted Products by Total Sales:\")\n",
        "print(sorted_sales_rdd.collect())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sGachWqPXP-m",
        "outputId": "44483df9-b31f-4bbb-a11f-f4254cdaa59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grouped sales:\n",
            "[('ProductA', [100, 200]), ('ProductB', [150, 250]), ('ProductC', [300, 100])]\n",
            "Total Sales by Product:\n",
            "[('ProductA', 300), ('ProductB', 400), ('ProductC', 400)]\n",
            "Sorted Products by Total Sales:\n",
            "[('ProductB', 400), ('ProductC', 400), ('ProductA', 300)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 4: Additional Transformations**\n",
        "\n",
        "#6. **Task 5: Filter Products with High Sales**\n",
        "high_sales_rdd = total_sales_rdd.filter(lambda x: x[1] > 200)\n",
        "\n",
        "# Print the products with high sales\n",
        "print(\"Products with High Sales:\")\n",
        "print(high_sales_rdd.collect())\n",
        "\n",
        "#7. **Task 6: Combine Regional Sales Data**\n",
        "regional_sales_data = [\n",
        "    (\"ProductA\", 50),\n",
        "    (\"ProductC\", 150)\n",
        "]\n",
        "\n",
        "regional_sales_rdd = sc.parallelize(regional_sales_data)\n",
        "\n",
        "combined_rdd = sales_rdd.union(regional_sales_rdd)\n",
        "combined_total_sales_rdd = combined_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the combined sales data\n",
        "print(\"Combined Sales Data:\")\n",
        "print(combined_total_sales_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KKSfITawX3f9",
        "outputId": "6de19926-79f7-453f-c31e-5071c046294e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with High Sales:\n",
            "[('ProductA', 300), ('ProductB', 400), ('ProductC', 400)]\n",
            "Combined Sales Data:\n",
            "[('ProductA', 350), ('ProductC', 550), ('ProductB', 400)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 5: Perform Actions on the RDD**\n",
        "\n",
        "#8. **Task 7: Count the Number of Distinct Products**\n",
        "distinct_products_count = sales_rdd.map(lambda x: x[0]).distinct().count()\n",
        "\n",
        "# Print the count of distinct products\n",
        "print(\"Number of Distinct Products:\", distinct_products_count)\n",
        "\n",
        "#9. **Task 8: Identify the Product with Maximum Sales**\n",
        "total_sales_rdd = sales_rdd.reduceByKey(lambda x, y: x + y)\n",
        "max_sales_product = total_sales_rdd.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
        "\n",
        "print(f\"Product with maximum sales: {max_sales_product[0]} with sales amount: {max_sales_product[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RsWXSnzMaZ39",
        "outputId": "af34d49e-64a1-485f-a240-3c7e6bf561c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Distinct Products: 3\n",
            "Product with maximum sales: ProductC with sales amount: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Challenge Task: Calculate the Average Sales per Product**\n",
        "\n",
        "#10. *Challenge Task:**\n",
        "#Calculate the average sales amount per product using the key-value pair RDD.\n",
        "average_sales_per_product = total_sales_rdd.mapValues(lambda x: x / distinct_products_count).collect()\n",
        "\n",
        "# Print the average sales per product\n",
        "print(\"Average Sales per Product:\")\n",
        "print(average_sales_per_product)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Elbd_XoHb3L_",
        "outputId": "aa97366b-91d2-4da0-812e-e572d0a747d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sales per Product:\n",
            "[('ProductA', 100.0), ('ProductB', 133.33333333333334), ('ProductC', 133.33333333333334)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#Create a DataFrames\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, 'John Doe', 'New York', 28),\n",
        "    (2, 'Jane Smith', 'Los Angeles', 34),\n",
        "    (3, 'Sam Brown', 'Chicago', 22),\n",
        "    (4, 'Lisa Ray', 'Houston', 45)\n",
        "]\n",
        "\n",
        "# Creating a DataFrame\n",
        "columns = ['CustomerID', 'Name', 'City', 'Age']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "#Selecting, Renaming, Filtering Data in a Pandas DataFrame\n",
        "# Selecting a single column\n",
        "df.select('Name').show()\n",
        "\n",
        "# Selecting multiple columns\n",
        "df.select('Name', 'City').show()\n",
        "\n",
        "# Renaming a column\n",
        "df.withColumnRenamed('City', 'Location').show()\n",
        "\n",
        "# Filtering data\n",
        "df.filter(col('Age') > 30).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hei2BaO0xakA",
        "outputId": "72385b3a-e90b-47b1-fb26-0ac6abef019b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------+---+\n",
            "|CustomerID|      Name|       City|Age|\n",
            "+----------+----------+-----------+---+\n",
            "|         1|  John Doe|   New York| 28|\n",
            "|         2|Jane Smith|Los Angeles| 34|\n",
            "|         3| Sam Brown|    Chicago| 22|\n",
            "|         4|  Lisa Ray|    Houston| 45|\n",
            "+----------+----------+-----------+---+\n",
            "\n",
            "+----------+\n",
            "|      Name|\n",
            "+----------+\n",
            "|  John Doe|\n",
            "|Jane Smith|\n",
            "| Sam Brown|\n",
            "|  Lisa Ray|\n",
            "+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|      Name|       City|\n",
            "+----------+-----------+\n",
            "|  John Doe|   New York|\n",
            "|Jane Smith|Los Angeles|\n",
            "| Sam Brown|    Chicago|\n",
            "|  Lisa Ray|    Houston|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+----------+-----------+---+\n",
            "|CustomerID|      Name|   Location|Age|\n",
            "+----------+----------+-----------+---+\n",
            "|         1|  John Doe|   New York| 28|\n",
            "|         2|Jane Smith|Los Angeles| 34|\n",
            "|         3| Sam Brown|    Chicago| 22|\n",
            "|         4|  Lisa Ray|    Houston| 45|\n",
            "+----------+----------+-----------+---+\n",
            "\n",
            "+----------+----------+-----------+---+\n",
            "|CustomerID|      Name|       City|Age|\n",
            "+----------+----------+-----------+---+\n",
            "|         2|Jane Smith|Los Angeles| 34|\n",
            "|         4|  Lisa Ray|    Houston| 45|\n",
            "+----------+----------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample employee data\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, 'Shalini', 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', 50000),\n",
        "    (5, 'Rahul', 'Finance', 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "# Create DataFrame\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MIRveRcKFpfQ",
        "outputId": "344ddb9b-76b2-4e2c-c7e3-e3e75aaf290f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. **Task 1: Filter Employees by Salary**\n",
        "filtered_df = employee_df.filter(col('Salary') > 60000)\n",
        "print(\"Employees with Salary > 60000:\")\n",
        "filtered_df.show()\n",
        "\n",
        "#2. **Task 2: Calculate the Average Salary by Department**\n",
        "#from pyspark.sql.functions import avg\n",
        "avg_salary_df = employee_df.groupBy('Department').avg('Salary').alias('AverageSalary')\n",
        "print(\"Average Salary by Department:\")\n",
        "avg_salary_df.show()\n",
        "\n",
        "#3. **Task 3: Sort Employees by Salary**\n",
        "sorted_salary_df = employee_df.orderBy(col(\"Salary\").desc())\n",
        "print(\"Employees sorted by salary:\")\n",
        "sorted_salary_df.show()\n",
        "\n",
        "#4. **Task 4: Add a Bonus Column**\n",
        "#from pyspark.sql.functions import expr\n",
        "bonus_df = employee_df.withColumn(\"Bonus\", col(\"Salary\") * 0.10)\n",
        "print(\"Employee DataFrame with Bonus Column:\")\n",
        "bonus_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rz_eWMjpKxQH",
        "outputId": "35acc7e4-1bac-4df8-9c5a-6b2dac61de4e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Salary > 60000:\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "Average Salary by Department:\n",
            "+----------+-----------------+\n",
            "|Department|      avg(Salary)|\n",
            "+----------+-----------------+\n",
            "|   Finance|          72500.0|\n",
            "|        IT|73333.33333333333|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "Employees sorted by salary:\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "Employee DataFrame with Bonus Column:\n",
            "+----------+------------+----------+------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary| Bonus|\n",
            "+----------+------------+----------+------+------+\n",
            "|         1|       Arjun|        IT| 75000|7500.0|\n",
            "|         2|       Vijay|   Finance| 85000|8500.0|\n",
            "|         3|     Shalini|        IT| 90000|9000.0|\n",
            "|         4|       Sneha|        HR| 50000|5000.0|\n",
            "|         5|       Rahul|   Finance| 60000|6000.0|\n",
            "|         6|        Amit|        IT| 55000|5500.0|\n",
            "+----------+------------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "#Initialize Spark session\n",
        "spark = SparkSession.builder\\\n",
        ".appName(\"Employee Data Handling\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "#Sample employee data with null values\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, None, 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', None),\n",
        "    (5, 'Rahul', None, 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "#Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "#Create DataFrame\n",
        "employee_df = spark.createDataFrame (data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()\n",
        "\n",
        "#Fill null values in 'EmployeeName' and 'Department' with 'Unknown'\n",
        "filled_df = employee_df.fillna({'EmployeeName': 'Unknown', 'Department': 'Unknown'})\n",
        "filled_df.show()\n",
        "\n",
        "#Drop rows where 'Salary' is null\n",
        "dropped_null_salary_df= employee_df.dropna (subset=['Salary'])\n",
        "dropped_null_salary_df.show()\n",
        "\n",
        "#Fill null values in 'Salary' with 50000\n",
        "salary_filled_df = employee_df.fillna({'Salary': 50000})\n",
        "salary_filled_df.show()\n",
        "\n",
        "#Check for null values in the entire DataFrame\n",
        "null_counts = employee_df.select([col(c).isNull().alias(c) for c in employee_df.columns]).show()\n",
        "\n",
        "#Replace all null values in the DataFrame with N/A\n",
        "na_filled_df = employee_df.na.fill('N/A')\n",
        "na_filled_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jG59SP9717bx",
        "outputId": "d3b529f9-c7df-4c4a-f2c1-9861adc3f858"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Unknown|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|   Unknown| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|     false|       false|     false| false|\n",
            "|     false|       false|     false| false|\n",
            "|     false|        true|     false| false|\n",
            "|     false|       false|     false|  true|\n",
            "|     false|       false|      true| false|\n",
            "|     false|       false|     false| false|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|         N/A|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|       N/A| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "#Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        ".appName (\"Advanced DataFrame Operations\") \\\n",
        ".getOrCreate()\n",
        "# Create two sample DataFrames\n",
        "datal = [\n",
        "    (1, 'Arjun', 'IT', 75000, '2022-01-15'),\n",
        "    (2, 'Vijay', 'Finance', 85000, '2022-03-12'),\n",
        "    (3, 'Shalini', 'IT', 90000, '2021-06-30')\n",
        "]\n",
        "data2 = [\n",
        "    (4, 'Sneha', 'HR', 50000, '2022-05-01'),\n",
        "    (5, 'Rahul', 'Finance', 60000, '2022-08-20'),\n",
        "    (6, 'Amit', 'IT', 55000, '2021-12-15')\n",
        "]\n",
        "\n",
        "#Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary', 'JoiningDate']\n",
        "\n",
        "#Create DataFrames\n",
        "employee_df1 = spark.createDataFrame (datal, columns)\n",
        "employee_df2 =  spark.createDataFrame (data2, columns)\n",
        "\n",
        "#Show the DataFrames\n",
        "employee_df1.show()\n",
        "employee_df2.show()\n",
        "\n",
        "# Union of two DataFrames (removes duplicates)\n",
        "union_df = employee_df1.union (employee_df2).dropDuplicates()\n",
        "print(\"Union of DataFrames (without duplicates):\")\n",
        "union_df.show()\n",
        "\n",
        "# Union of two DataFrames (includes duplicates)\n",
        "union_all_df = employee_df1.union(employee_df2)\n",
        "print(\"Union of DataFrames (with duplicates):\")\n",
        "union_all_df.show()\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "#Define a window specification to rank employees by salary within each department\n",
        "window_spec =  Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "\n",
        "#Add a rank column to the DataFrame\n",
        "ranked_df  = union_all_df.withColumn(\"Rank\", rank().over(window_spec))\n",
        "print(\"DataFrame with ranks:\")\n",
        "ranked_df.show()\n",
        "\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "#Define a window specification for cumulative sum of salaries within each department\n",
        "window_spec_sum =  Window.partitionBy(\"Department\").orderBy(\"JoiningDate\").rowsBetween(Window.unboundedPreceding, Window. currentRow)\n",
        "\n",
        "# Calculate the running total of salaries\n",
        "running_total_df = union_all_df.withColumn (\"RunningTotal\", sum(col (\"Salary\")).over(window_spec_sum))\n",
        "print(\"DataFrame with running total:\")\n",
        "running_total_df.show()\n",
        "\n",
        "# Convert JoiningDate from string to date type\n",
        "date_converted_df =  union_all_df.withColumn(\"JoiningDate\",F.to_date(col(\"JoiningDate\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
        "print(\"DataFrame with converted JoiningDate:\")\n",
        "date_converted_df.show()\n",
        "\n",
        "# Replace invalid dates with null\n",
        "#date_converted_df = date_converted_df.fillna({'JoiningDate': None})\n",
        "#date_converted_df.show()\n",
        "\n",
        "# Calculate the number of years since joining\n",
        "experience_df = date_converted_df.withColumn(\"YearsOfExperience\", F.round (F.datediff (F.current_date(), col (\"JoiningDate\")) /\n",
        "365, 2))\n",
        "print(\"DataFrame with years of experience:\")\n",
        "experience_df.show()\n",
        "\n",
        "#Add a new column for next evaluation date (one year after joining)\n",
        "eval_date_df =  date_converted_df.withColumn(\"NextEvaluationDate\", F.date_add(col(\"JoiningDate\"), 365))\n",
        "print(\"DataFrame with next evaluation date:\")\n",
        "eval_date_df.show()\n",
        "\n",
        "#Calculate average salary per department\n",
        "avg_salary_df = union_all_df.groupBy(\"Department\").agg(F.avg(\"Salary\").alias(\"AverageSalary\"))\n",
        "print(\"Average salary per department:\")\n",
        "avg_salary_df.show()\n",
        "\n",
        "# Calculate the total number of employees\n",
        "total_employees_df = union_all_df.agg (F.count(\"EmployeeID\").alias (\"TotalEmployees\"))\n",
        "print(\"Total number of employees:\")\n",
        "total_employees_df.show()\n",
        "\n",
        "# Convert employee names to uppercase\n",
        "upper_name_df = union_all_df.withColumn(\"EmployeeNameUpper\", F.upper(col(\"EmployeeName\")))\n",
        "print(\"DataFrame with employee names in uppercase:\")\n",
        "upper_name_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CW-yTPEaNART",
        "outputId": "ca27acfe-4ed1-47e9-d02a-8ac76228f310"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of DataFrames (without duplicates):\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of DataFrames (with duplicates):\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "DataFrame with ranks:\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|Rank|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|   1|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|   2|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|   1|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|   1|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|   2|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|   3|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "\n",
            "DataFrame with running total:\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "\n",
            "DataFrame with converted JoiningDate:\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "DataFrame with years of experience:\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|YearsOfExperience|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|             2.64|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|             2.48|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|             3.18|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|             2.35|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|             2.04|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|             2.72|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "\n",
            "DataFrame with next evaluation date:\n",
            "+----------+------------+----------+------+-----------+------------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|NextEvaluationDate|\n",
            "+----------+------------+----------+------+-----------+------------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|        2023-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|        2023-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|        2022-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|        2023-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|        2023-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|        2022-12-15|\n",
            "+----------+------------+----------+------+-----------+------------------+\n",
            "\n",
            "Average salary per department:\n",
            "+----------+-----------------+\n",
            "|Department|    AverageSalary|\n",
            "+----------+-----------------+\n",
            "|        IT|73333.33333333333|\n",
            "|   Finance|          72500.0|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "Total number of employees:\n",
            "+--------------+\n",
            "|TotalEmployees|\n",
            "+--------------+\n",
            "|             6|\n",
            "+--------------+\n",
            "\n",
            "DataFrame with employee names in uppercase:\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|EmployeeNameUpper|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|            ARJUN|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|            VIJAY|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|          SHALINI|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|            SNEHA|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|            RAHUL|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|             AMIT|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Data Setup:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Advanced DataFrame Operations - Different Dataset\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create two sample DataFrames for Product Sales\n",
        "data1 = [\n",
        "    (1, 'Product A', 'Electronics', 1200, '2022-05-10'),\n",
        "    (2, 'Product B', 'Clothing', 500, '2022-07-15'),\n",
        "    (3, 'Product C', 'Electronics', 1800, '2021-11-05')\n",
        "]\n",
        "\n",
        "data2 = [\n",
        "    (4, 'Product D', 'Furniture', 3000, '2022-03-25'),\n",
        "    (5, 'Product E', 'Clothing', 800, '2022-09-12'),\n",
        "    (6, 'Product F', 'Electronics', 1500, '2021-10-19')\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['ProductID', 'ProductName', 'Category', 'Price', 'SaleDate']\n",
        "\n",
        "# Create DataFrames\n",
        "sales_df1 = spark.createDataFrame(data1, columns)\n",
        "sales_df2 = spark.createDataFrame(data2, columns)\n",
        "\n",
        "#show the dataframes\n",
        "sales_df1.show()\n",
        "sales_df2.show()\n",
        "\n",
        "### Tasks:\n",
        "\n",
        "#1. **Union of DataFrames (removing duplicates)**:\n",
        "union_df = sales_df1.union(sales_df2).dropDuplicates()\n",
        "print(\"Union of DataFrames (without duplicates):\")\n",
        "union_df.show()\n",
        "\n",
        "#2. **Union of DataFrames (including duplicates)**:\n",
        "union_all_df = sales_df1.union(sales_df2)\n",
        "print(\"Union of DataFrames (with duplicates):\")\n",
        "union_all_df.show()\n",
        "\n",
        "#3. **Rank products by price within their category**:\n",
        "window_spec = Window.partitionBy(\"Category\").orderBy(F.desc(\"Price\"))\n",
        "ranked_df = union_df.withColumn(\"Rank\", F.row_number().over(window_spec))\n",
        "print(\"DataFrame with ranks:\")\n",
        "ranked_df.show()\n",
        "\n",
        "#4. **Calculate cumulative price per category**:\n",
        "cumulative_df = union_df.withColumn(\"CumulativePrice\", F.sum(\"Price\").over(window_spec))\n",
        "print(\"DataFrame with cumulative price:\")\n",
        "cumulative_df.show()\n",
        "\n",
        "#5. **Convert `SaleDate` from string to date type**:\n",
        "converted_df = union_df.withColumn(\"SaleDate\", F.to_date(\"SaleDate\", \"yyyy-MM-dd\"))\n",
        "print(\"DataFrame with converted SaleDate:\")\n",
        "converted_df.show()\n",
        "\n",
        "#6. **Calculate the number of days since each sale**:\n",
        "days_since_sale_df = converted_df.withColumn(\"DaysSinceSale\", F.datediff(F.current_date(), \"SaleDate\"))\n",
        "print(\"DataFrame with days since sale:\")\n",
        "days_since_sale_df.show()\n",
        "\n",
        "#7. **Add a column for the next sale deadline**:\n",
        "next_sale_deadline_df = converted_df.withColumn(\"NextSaleDeadline\", F.date_add(\"SaleDate\", 30))\n",
        "print(\"DataFrame with next sale deadline:\")\n",
        "next_sale_deadline_df.show()\n",
        "\n",
        "#8. **Calculate total revenue and average price per category**:\n",
        "revenue_df = union_df.groupBy(\"Category\").agg(\n",
        "    F.sum(\"Price\").alias(\"TotalRevenue\"),\n",
        "    F.avg(\"Price\").alias(\"AveragePrice\")\n",
        ")\n",
        "print(\"Total revenue and average price per category:\")\n",
        "revenue_df.show()\n",
        "\n",
        "#9. **Convert all product names to lowercase**:\n",
        "lower_case_df = union_df.withColumn(\"ProductNameLower\", F.lower(\"ProductName\"))\n",
        "print(\"DataFrame with product names in lowercase:\")\n",
        "lower_case_df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9BI4FhLz6ZQ",
        "outputId": "c96fe35d-eb83-4b52-b803-99d566550e17"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "Union of DataFrames (without duplicates):\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "Union of DataFrames (with duplicates):\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "DataFrame with ranks:\n",
            "+---------+-----------+-----------+-----+----------+----+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|Rank|\n",
            "+---------+-----------+-----------+-----+----------+----+\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|   1|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|   2|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|   1|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|   2|\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|   3|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|   1|\n",
            "+---------+-----------+-----------+-----+----------+----+\n",
            "\n",
            "DataFrame with cumulative price:\n",
            "+---------+-----------+-----------+-----+----------+---------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|CumulativePrice|\n",
            "+---------+-----------+-----------+-----+----------+---------------+\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|            800|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|           1300|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|           1800|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|           3300|\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|           4500|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|           3000|\n",
            "+---------+-----------+-----------+-----+----------+---------------+\n",
            "\n",
            "DataFrame with converted SaleDate:\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "DataFrame with days since sale:\n",
            "+---------+-----------+-----------+-----+----------+-------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|DaysSinceSale|\n",
            "+---------+-----------+-----------+-----+----------+-------------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|          848|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|          782|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|         1034|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|          894|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|         1051|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|          723|\n",
            "+---------+-----------+-----------+-----+----------+-------------+\n",
            "\n",
            "DataFrame with next sale deadline:\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|NextSaleDeadline|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|      2022-06-09|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|      2022-08-14|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|      2021-12-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|      2022-04-24|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|      2021-11-18|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|      2022-10-12|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "\n",
            "Total revenue and average price per category:\n",
            "+-----------+------------+------------+\n",
            "|   Category|TotalRevenue|AveragePrice|\n",
            "+-----------+------------+------------+\n",
            "|Electronics|        4500|      1500.0|\n",
            "|   Clothing|        1300|       650.0|\n",
            "|  Furniture|        3000|      3000.0|\n",
            "+-----------+------------+------------+\n",
            "\n",
            "DataFrame with product names in lowercase:\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|ProductNameLower|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|       product a|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|       product b|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|       product c|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|       product d|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|       product f|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|       product e|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}