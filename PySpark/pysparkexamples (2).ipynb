{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5c91802837fd42cbbfede951e4cb1687": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DropdownModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DropdownModel",
            "_options_labels": [
              "age",
              "salary"
            ],
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "DropdownView",
            "description": "Filter By:",
            "description_tooltip": null,
            "disabled": false,
            "index": 0,
            "layout": "IPY_MODEL_e78c39f851ed41b7990a4f3bdac75748",
            "style": "IPY_MODEL_278d67a682584ea598d5c385f1fbccb7"
          }
        },
        "e78c39f851ed41b7990a4f3bdac75748": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "278d67a682584ea598d5c385f1fbccb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b1cca2cfbf3c446e8b21903dfa88e909": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntSliderModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "IntSliderModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "IntSliderView",
            "continuous_update": false,
            "description": "Threshold:",
            "description_tooltip": null,
            "disabled": false,
            "layout": "IPY_MODEL_ef96b6950283408f9af118f9f2a173e4",
            "max": 100,
            "min": 20,
            "orientation": "horizontal",
            "readout": true,
            "readout_format": "d",
            "step": 5,
            "style": "IPY_MODEL_675d1b8bfbd146bd95989dad0ab52614",
            "value": 30
          }
        },
        "ef96b6950283408f9af118f9f2a173e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "675d1b8bfbd146bd95989dad0ab52614": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "SliderStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "SliderStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": "",
            "handle_color": null
          }
        },
        "d0e1ac04e00c4c449c9ec8c76080ae85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ButtonView",
            "button_style": "",
            "description": "Apply Filter",
            "disabled": false,
            "icon": "",
            "layout": "IPY_MODEL_6549c656e47348c8b9199338fe18c0d1",
            "style": "IPY_MODEL_deb3f94b74974c02a0193ee54248dcb8",
            "tooltip": ""
          }
        },
        "6549c656e47348c8b9199338fe18c0d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "deb3f94b74974c02a0193ee54248dcb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ButtonStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ButtonStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "button_color": null,
            "font_weight": ""
          }
        },
        "011b1dabb86a40ce8024f84c536e1587": {
          "model_module": "@jupyter-widgets/output",
          "model_name": "OutputModel",
          "model_module_version": "1.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/output",
            "_model_module_version": "1.0.0",
            "_model_name": "OutputModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/output",
            "_view_module_version": "1.0.0",
            "_view_name": "OutputView",
            "layout": "IPY_MODEL_43241bffd860421abf965f2d12684987",
            "msg_id": "",
            "outputs": []
          }
        },
        "43241bffd860421abf965f2d12684987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "pip install pyspark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H4etoQP2U4P-",
        "outputId": "23b901ac-5c70-4651-c64d-4caaafbae9b3",
        "collapsed": true
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyspark\n",
            "  Downloading pyspark-3.5.2.tar.gz (317.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: py4j==0.10.9.7 in /usr/local/lib/python3.10/dist-packages (from pyspark) (0.10.9.7)\n",
            "Building wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.5.2-py2.py3-none-any.whl size=317812365 sha256=cf06b19155612431f54b5220faec1cdf4fd461bdec8c42a7adbea8b4433c0a56\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/34/bd/03944534c44b677cd5859f248090daa9fb27b3c8f8e5f49574\n",
            "Successfully built pyspark\n",
            "Installing collected packages: pyspark\n",
            "Successfully installed pyspark-3.5.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "#initialize sparkSession\n",
        "spark =  SparkSession.builder\\\n",
        "    .appName(\"Pyspark notebook example\")\\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(spark)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JkK3pUlYU3fl",
        "outputId": "7b77b65a-900c-4ec0-8e25-6557ce4d01cc",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<pyspark.sql.session.SparkSession object at 0x792c90adf0d0>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "#Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"PySpark DataFrame Example\") \\\n",
        ".getOrCreate()\n",
        "#Sample data representing employees\n",
        "data = [\n",
        "(\"John Doe\", \"Engineering\", 75000),\n",
        "(\"Jane Smith\", \"Marketing\", 60000),\n",
        "(\"Sam Brown\", \"Engineering\", 80000),\n",
        "(\"Emily Davis\", \"HR\", 50000),\n",
        "(\"Michael Johnson\", \"Marketing\", 70000),\n",
        "]\n",
        "#Define schema for DataFrame\n",
        "columns= [\"Name\", \"Department\", \"Salary\"]\n",
        "#Create DataFrame\n",
        "df = spark.createDataFrame(data, schema=columns)\n",
        "#Show the DataFrame\n",
        "df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PF4S7VbSV6RP",
        "outputId": "12582b34-9d09-4d2f-c8ff-675c21519c04",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|     Jane Smith|  Marketing| 60000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|    Emily Davis|         HR| 50000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Filter: Select employees with a salary greater than 65,000\n",
        "high_salary_df = df.filter (col (\"Salary\") > 65000)\n",
        "print(\"Employees with Salary > 65,000:\")\n",
        "high_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Aip9uf5Yjm4",
        "outputId": "39ef1cba-4788-41cc-8028-18a05062dfac",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Salary > 65,000:\n",
            "+---------------+-----------+------+\n",
            "|           Name| Department|Salary|\n",
            "+---------------+-----------+------+\n",
            "|       John Doe|Engineering| 75000|\n",
            "|      Sam Brown|Engineering| 80000|\n",
            "|Michael Johnson|  Marketing| 70000|\n",
            "+---------------+-----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Group by Department and calculate the average salary\n",
        "avg_salary_df = df.groupBy(\"Department\").avg(\"Salary\")\n",
        "print(\"Average Salary by Department:\")\n",
        "avg_salary_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8abCBdVlZZrS",
        "outputId": "0e7462a1-10f1-49c1-ecdf-d525c918cd8c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Salary by Department:\n",
            "+-----------+-----------+\n",
            "| Department|avg(Salary)|\n",
            "+-----------+-----------+\n",
            "|Engineering|    77500.0|\n",
            "|  Marketing|    65000.0|\n",
            "|         HR|    50000.0|\n",
            "+-----------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "#Initialize SparkSession\n",
        "spark = SparkSession.builder\\\n",
        ".appName(\"Customer Transactions Analysis\") \\\n",
        ".getOrCreate ()\n",
        "# Sample data for customers\n",
        "customers = [\n",
        "(1, \"Ravi\", \"Mumbai\"),\n",
        "(2, \"Priya\", \"Delhi\"),\n",
        "(3, \"Vijay\", \"Bangalore\"),\n",
        "(4, \"Anita\", \"Chennai\"),\n",
        "(5, \"Raj\", \"Hyderabad\"),\n",
        "]\n",
        "# Sample data for transactions\n",
        "transactions = [\n",
        "(1, 1, 10000.50),\n",
        "(2, 2, 20000.75),\n",
        "(3, 1, 15000.25),\n",
        "(4, 3, 30000.00),\n",
        "(5, 2, 40000.50),\n",
        "(6, 4, 25000.00),\n",
        "(7, 5, 18000.75),\n",
        "(8, 1, 5000.00),\n",
        "]\n",
        "\n",
        "#Define schema for DataFrames\n",
        "customer_columns = [\"CustomerID\", \"Name\", \"City\"]\n",
        "transaction_columns = [\"TransactionID\", \"CustomerID\", \"Amount\"]\n",
        "#Create DataFrames\n",
        "customer_df = spark.createDataFrame (customers, schema=customer_columns)\n",
        "transaction_df = spark.createDataFrame (transactions, schema=transaction_columns)\n",
        "\n",
        "#Show the DataFrames\n",
        "print(\"Customers DataFrame:\")\n",
        "customer_df.show()\n",
        "print(\"Transactions DataFrame:\")\n",
        "transaction_df.show()"
      ],
      "metadata": {
        "id": "0O6Jq2L08rB3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "df328db9-edab-4eb5-ea4e-6ca579bdf8ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customers DataFrame:\n",
            "+----------+-----+---------+\n",
            "|CustomerID| Name|     City|\n",
            "+----------+-----+---------+\n",
            "|         1| Ravi|   Mumbai|\n",
            "|         2|Priya|    Delhi|\n",
            "|         3|Vijay|Bangalore|\n",
            "|         4|Anita|  Chennai|\n",
            "|         5|  Raj|Hyderabad|\n",
            "+----------+-----+---------+\n",
            "\n",
            "Transactions DataFrame:\n",
            "+-------------+----------+--------+\n",
            "|TransactionID|CustomerID|  Amount|\n",
            "+-------------+----------+--------+\n",
            "|            1|         1| 10000.5|\n",
            "|            2|         2|20000.75|\n",
            "|            3|         1|15000.25|\n",
            "|            4|         3| 30000.0|\n",
            "|            5|         2| 40000.5|\n",
            "|            6|         4| 25000.0|\n",
            "|            7|         5|18000.75|\n",
            "|            8|         1|  5000.0|\n",
            "+-------------+----------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Join the DataFrames on CustomerID\n",
        "customer_transactions_df = customer_df.join(transaction_df, on=\"CustomerID\")\n",
        "print(\"Customer Transactions DataFrame:\")\n",
        "customer_transactions_df.show()\n",
        "\n",
        "#Calculate the total amount spent by each customer\n",
        "total_spent_df = customer_transactions_df.groupBy(\"Name\").sum(\"Amount\").withColumnRenamed (\"sum (Amount)\", \"TotalSpent\")\n",
        "print(\"Total Amount Spent by Each Customer:\")\n",
        "total_spent_df.show()\n",
        "\n",
        "## Find customers who have spent more than ₹30,000\n",
        "big_spenders_df = total_spent_df.filter (col (\"TotalSpent\") > 30000)\n",
        "print(\"Customers Who Spent More Than ₹30,000:\")\n",
        "big_spenders_df.show()\n",
        "\n",
        "## Count the number of transactions per customer\n",
        "transactions_count_df = customer_transactions_df.groupBy(\"Name\").count().withColumnRenamed (\"count\", \"TransactionCount\")\n",
        "print(\"Number of Transactions Per Customer:\")\n",
        "transactions_count_df.show()\n",
        "\n",
        "## Sort customers by total amount spent in descending order\n",
        "sorted_spenders_df = total_spent_df.orderBy (col (\"TotalSpent\").desc())\n",
        "print(\"Customers Sorted by Total Spent (Descending):\")\n",
        "sorted_spenders_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 963
        },
        "collapsed": true,
        "id": "SACY9KBpE5Ky",
        "outputId": "60c1ec01-aff4-4ed1-b678-14c9c8a04204"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Customer Transactions DataFrame:\n",
            "+----------+-----+---------+-------------+--------+\n",
            "|CustomerID| Name|     City|TransactionID|  Amount|\n",
            "+----------+-----+---------+-------------+--------+\n",
            "|         1| Ravi|   Mumbai|            1| 10000.5|\n",
            "|         1| Ravi|   Mumbai|            3|15000.25|\n",
            "|         1| Ravi|   Mumbai|            8|  5000.0|\n",
            "|         2|Priya|    Delhi|            2|20000.75|\n",
            "|         2|Priya|    Delhi|            5| 40000.5|\n",
            "|         3|Vijay|Bangalore|            4| 30000.0|\n",
            "|         4|Anita|  Chennai|            6| 25000.0|\n",
            "|         5|  Raj|Hyderabad|            7|18000.75|\n",
            "+----------+-----+---------+-------------+--------+\n",
            "\n",
            "Total Amount Spent by Each Customer:\n",
            "+-----+-----------+\n",
            "| Name|sum(Amount)|\n",
            "+-----+-----------+\n",
            "| Ravi|   30000.75|\n",
            "|Priya|   60001.25|\n",
            "|Vijay|    30000.0|\n",
            "|Anita|    25000.0|\n",
            "|  Raj|   18000.75|\n",
            "+-----+-----------+\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AnalysisException",
          "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TotalSpent` cannot be resolved. Did you mean one of the following? [`Name`, `sum(Amount)`].;\n'Filter ('TotalSpent > 30000)\n+- Aggregate [Name#55], [Name#55, sum(Amount#62) AS sum(Amount)#226]\n   +- Project [CustomerID#54L, Name#55, City#56, TransactionID#60L, Amount#62]\n      +- Join Inner, (CustomerID#54L = CustomerID#61L)\n         :- LogicalRDD [CustomerID#54L, Name#55, City#56], false\n         +- LogicalRDD [TransactionID#60L, CustomerID#61L, Amount#62], false\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-ab289b7fcfdb>\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m## Find customers who have spent more than ₹30,000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mbig_spenders_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_spent_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcol\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"TotalSpent\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m30000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Customers Who Spent More Than ₹30,000:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mbig_spenders_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mfilter\u001b[0;34m(self, condition)\u001b[0m\n\u001b[1;32m   3329\u001b[0m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3330\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mColumn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3331\u001b[0;31m             \u001b[0mjdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcondition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3332\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3333\u001b[0m             raise PySparkTypeError(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    183\u001b[0m                 \u001b[0;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                 \u001b[0;31m# JVM exception message.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 185\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mconverted\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    186\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `TotalSpent` cannot be resolved. Did you mean one of the following? [`Name`, `sum(Amount)`].;\n'Filter ('TotalSpent > 30000)\n+- Aggregate [Name#55], [Name#55, sum(Amount#62) AS sum(Amount)#226]\n   +- Project [CustomerID#54L, Name#55, City#56, TransactionID#60L, Amount#62]\n      +- Join Inner, (CustomerID#54L = CustomerID#61L)\n         :- LogicalRDD [CustomerID#54L, Name#55, City#56], false\n         +- LogicalRDD [TransactionID#60L, CustomerID#61L, Amount#62], false\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Exercise: Product Sales Analysis**\n",
        "#### **Step 1: Create DataFrames**\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Product Sales Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample data for products\n",
        "products = [\n",
        "    (1, \"Laptop\", \"Electronics\", 50000),\n",
        "    (2, \"Smartphone\", \"Electronics\", 30000),\n",
        "    (3, \"Table\", \"Furniture\", 15000),\n",
        "    (4, \"Chair\", \"Furniture\", 5000),\n",
        "    (5, \"Headphones\", \"Electronics\", 2000),\n",
        "]\n",
        "\n",
        "# Sample data for sales transactions\n",
        "sales = [\n",
        "    (1, 1, 2),\n",
        "    (2, 2, 1),\n",
        "    (3, 3, 3),\n",
        "    (4, 1, 1),\n",
        "    (5, 4, 5),\n",
        "    (6, 2, 2),\n",
        "    (7, 5, 10),\n",
        "    (8, 3, 1),\n",
        "]\n",
        "\n",
        "#Define schema for DataFrames\n",
        "product_columns = [\"ProductID\", \"Name\", \"Category\", \"Price\"]\n",
        "sales_columns = [\"SaleID\", \"ProductID\", \"Quantity\"]\n",
        "#Create DataFrames\n",
        "product_df = spark.createDataFrame (products, schema=product_columns)\n",
        "transaction_columns = [\"TransactionID\", \"CustomerID\", \"Amount\"]\n",
        "#Create DataFrames\n",
        "product_df = spark.createDataFrame (products, schema=product_columns)\n",
        "sales_df = spark.createDataFrame (sales, schema=sales_columns)\n",
        "\n",
        "#Show the DataFrames\n",
        "print(\"Products DataFrame:\")\n",
        "product_df.show()\n",
        "print(\"Sales DataFrame:\")\n",
        "sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g3maMjzXL4RW",
        "outputId": "ac5f2879-1e31-41f2-a16a-05611802eb1b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products DataFrame:\n",
            "+---------+----------+-----------+-----+\n",
            "|ProductID|      Name|   Category|Price|\n",
            "+---------+----------+-----------+-----+\n",
            "|        1|    Laptop|Electronics|50000|\n",
            "|        2|Smartphone|Electronics|30000|\n",
            "|        3|     Table|  Furniture|15000|\n",
            "|        4|     Chair|  Furniture| 5000|\n",
            "|        5|Headphones|Electronics| 2000|\n",
            "+---------+----------+-----------+-----+\n",
            "\n",
            "Sales DataFrame:\n",
            "+------+---------+--------+\n",
            "|SaleID|ProductID|Quantity|\n",
            "+------+---------+--------+\n",
            "|     1|        1|       2|\n",
            "|     2|        2|       1|\n",
            "|     3|        3|       3|\n",
            "|     4|        1|       1|\n",
            "|     5|        4|       5|\n",
            "|     6|        2|       2|\n",
            "|     7|        5|      10|\n",
            "|     8|        3|       1|\n",
            "+------+---------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 2: Perform the Following Tasks**\n",
        "#1. **Join the DataFrames:**\n",
        "\n",
        "product_sales_df = product_df.join(sales_df, on=\"ProductID\")\n",
        "print(\"Product Sales DataFrame:\")\n",
        "product_sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nTEIC4rOT46f",
        "outputId": "24873e51-150d-4fb1-9393-8509d9b745cb",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Product Sales DataFrame:\n",
            "+---------+----------+-----------+-----+------+--------+\n",
            "|ProductID|      Name|   Category|Price|SaleID|Quantity|\n",
            "+---------+----------+-----------+-----+------+--------+\n",
            "|        1|    Laptop|Electronics|50000|     1|       2|\n",
            "|        1|    Laptop|Electronics|50000|     4|       1|\n",
            "|        2|Smartphone|Electronics|30000|     2|       1|\n",
            "|        2|Smartphone|Electronics|30000|     6|       2|\n",
            "|        3|     Table|  Furniture|15000|     3|       3|\n",
            "|        3|     Table|  Furniture|15000|     8|       1|\n",
            "|        4|     Chair|  Furniture| 5000|     5|       5|\n",
            "|        5|Headphones|Electronics| 2000|     7|      10|\n",
            "+---------+----------+-----------+-----+------+--------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#2. **Calculate Total Sales Value:**\n",
        "combined_df = product_sales_df.withColumn(\"TotalSalesValue\", col(\"Price\") * col(\"Quantity\"))\n",
        "print(\"Combined DataFrame with Total Sales Value:\")\n",
        "combined_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRHCf0vwVDuT",
        "outputId": "c3909c53-03be-4163-ff98-d2ee548c74db",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined DataFrame with Total Sales Value:\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|ProductID|      Name|   Category|Price|SaleID|Quantity|TotalSalesValue|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|        1|    Laptop|Electronics|50000|     1|       2|         100000|\n",
            "|        1|    Laptop|Electronics|50000|     4|       1|          50000|\n",
            "|        2|Smartphone|Electronics|30000|     2|       1|          30000|\n",
            "|        2|Smartphone|Electronics|30000|     6|       2|          60000|\n",
            "|        3|     Table|  Furniture|15000|     3|       3|          45000|\n",
            "|        3|     Table|  Furniture|15000|     8|       1|          15000|\n",
            "|        4|     Chair|  Furniture| 5000|     5|       5|          25000|\n",
            "|        5|Headphones|Electronics| 2000|     7|      10|          20000|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3. **Find the Total Sales for Each Product Category:**\n",
        "category_sales_df = combined_df.groupBy(\"Category\").agg({\"TotalSalesValue\": \"sum\"}).withColumnRenamed(\"sum(TotalSalesValue)\", \"TotalCategorySales\")\n",
        "print(\"Total Sales for Each Product Category:\")\n",
        "category_sales_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fsZyaBZ4WNNB",
        "outputId": "d6ccbe3c-f25f-4fdf-fc9d-9a04c9d7c6cd",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sales for Each Product Category:\n",
            "+-----------+------------------+\n",
            "|   Category|TotalCategorySales|\n",
            "+-----------+------------------+\n",
            "|Electronics|            260000|\n",
            "|  Furniture|             85000|\n",
            "+-----------+------------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#4. **Identify the Top-Selling Product:**\n",
        "product_sales_df = combined_df.groupBy(\"ProductID\", \"Name\").agg({\"TotalSalesValue\": \"sum\"}).withColumnRenamed(\"sum(TotalSalesValue)\", \"TotalProductSales\")\n",
        "top_selling_product_df = product_sales_df.orderBy(col(\"TotalProductSales\").desc()).limit(1)\n",
        "print(\"Top-Selling Product:\")\n",
        "top_selling_product_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR_-1kV_WlHm",
        "outputId": "65fdd1e6-5acb-4e97-f6a8-7ad18a160fd6",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-Selling Product:\n",
            "+---------+------+-----------------+\n",
            "|ProductID|  Name|TotalProductSales|\n",
            "+---------+------+-----------------+\n",
            "|        1|Laptop|           150000|\n",
            "+---------+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#5. **Sort the Products by Total Sales Value:**\n",
        "sorted_products_df = combined_df.orderBy(col(\"TotalSalesValue\").desc())\n",
        "print(\"Products Sorted by Total Sales Value:\")\n",
        "sorted_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4xOC3ipX3nw",
        "outputId": "f64214cb-3b7a-4c6c-9400-93633600e33a",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products Sorted by Total Sales Value:\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|ProductID|      Name|   Category|Price|SaleID|Quantity|TotalSalesValue|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "|        1|    Laptop|Electronics|50000|     1|       2|         100000|\n",
            "|        2|Smartphone|Electronics|30000|     6|       2|          60000|\n",
            "|        1|    Laptop|Electronics|50000|     4|       1|          50000|\n",
            "|        3|     Table|  Furniture|15000|     3|       3|          45000|\n",
            "|        2|Smartphone|Electronics|30000|     2|       1|          30000|\n",
            "|        4|     Chair|  Furniture| 5000|     5|       5|          25000|\n",
            "|        5|Headphones|Electronics| 2000|     7|      10|          20000|\n",
            "|        3|     Table|  Furniture|15000|     8|       1|          15000|\n",
            "+---------+----------+-----------+-----+------+--------+---------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#6. **Count the Number of Sales for Each Product:**\n",
        "sales_count_df = sales_df.groupBy(\"ProductID\").count().withColumnRenamed(\"count\", \"NumberOfSales\")\n",
        "print(\"Number of Sales for Each Product:\")\n",
        "sales_count_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qjzt7BB9YaOt",
        "outputId": "99397079-048a-4cc7-ba4c-0ce58f528b35",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Sales for Each Product:\n",
            "+---------+-------------+\n",
            "|ProductID|NumberOfSales|\n",
            "+---------+-------------+\n",
            "|        1|            2|\n",
            "|        3|            2|\n",
            "|        2|            2|\n",
            "|        5|            1|\n",
            "|        4|            1|\n",
            "+---------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#7. **Filter the Products with Total Sales Value Greater Than ₹50,000:**\n",
        "product_sales_df = product_sales_df.withColumnRenamed(\"sum(TotalSalesValue)\", \"TotalProductSales\")\n",
        "filtered_products_df = product_sales_df.filter(col(\"TotalProductSales\") > 50000)\n",
        "print(\"Products with Total Sales Value > ₹50,000:\")\n",
        "filtered_products_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM8YyYyfYnNu",
        "outputId": "7812e675-cd46-422e-e23c-ea963af2200c",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with Total Sales Value > ₹50,000:\n",
            "+---------+----------+-----------------+\n",
            "|ProductID|      Name|TotalProductSales|\n",
            "+---------+----------+-----------------+\n",
            "|        1|    Laptop|           150000|\n",
            "|        2|Smartphone|            90000|\n",
            "|        3|     Table|            60000|\n",
            "+---------+----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Exercise: Analyzing a Sample Sales Dataset Using PySpark**\n",
        "### **Part 1: Dataset Preparation**\n",
        "\n",
        "#### **Step 1: Generate the Sample Sales Dataset**\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "# Sample sales data\n",
        "data = {\n",
        "       \"TransactionID\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],\n",
        "       \"CustomerID\": [101, 102, 103, 101, 104, 102, 103, 104, 101, 105],\n",
        "       \"ProductID\": [501, 502, 501, 503, 504, 502, 503, 504, 501, 505],\n",
        "       \"Quantity\": [2, 1, 4, 3, 1, 2, 5, 1, 2, 1],\n",
        "       \"Price\": [150.0, 250.0, 150.0, 300.0, 450.0, 250.0, 300.0, 450.0, 150.0, 550.0],\n",
        "       \"Date\": [\n",
        "           datetime(2024, 9, 1),\n",
        "           datetime(2024, 9, 1),\n",
        "           datetime(2024, 9, 2),\n",
        "           datetime(2024, 9, 2),\n",
        "           datetime(2024, 9, 3),\n",
        "           datetime(2024, 9, 3),\n",
        "           datetime(2024, 9, 4),\n",
        "           datetime(2024, 9, 4),\n",
        "           datetime(2024, 9, 5),\n",
        "           datetime(2024, 9, 5)\n",
        "       ]\n",
        "   }\n",
        "\n",
        "# Create a DataFrame\n",
        "pandas_df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame to a CSV file\n",
        "pandas_df.to_csv('sales_data.csv', index=False)\n",
        "\n",
        "print(\"Sample sales dataset has been created and saved as 'sales_data.csv'.\")\n",
        "\n",
        "#2. **Verify the Dataset:**\n",
        "pandas_df = pd.read_csv('sales_data.csv')\n",
        "print(pandas_df)\n"
      ],
      "metadata": {
        "id": "8niuLH-Abrw9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d1fd124-b606-4980-c8f8-e2e585127739",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample sales dataset has been created and saved as 'sales_data.csv'.\n",
            "   TransactionID  CustomerID  ProductID  Quantity  Price        Date\n",
            "0              1         101        501         2  150.0  2024-09-01\n",
            "1              2         102        502         1  250.0  2024-09-01\n",
            "2              3         103        501         4  150.0  2024-09-02\n",
            "3              4         101        503         3  300.0  2024-09-02\n",
            "4              5         104        504         1  450.0  2024-09-03\n",
            "5              6         102        502         2  250.0  2024-09-03\n",
            "6              7         103        503         5  300.0  2024-09-04\n",
            "7              8         104        504         1  450.0  2024-09-04\n",
            "8              9         101        501         2  150.0  2024-09-05\n",
            "9             10         105        505         1  550.0  2024-09-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 2: Load the Dataset into PySpark**\n",
        "\n",
        "#1. **Initialize the SparkSession:**\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Sales Dataset Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "#2. **Load the CSV File into a PySpark DataFrame:**\n",
        "df = spark.read.csv('sales_data.csv', header=True, inferSchema=True)\n",
        "#Display the first few rows\n",
        "df.show(5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kYRMJt2n6EK",
        "outputId": "f43425f4-6e1d-400f-9c56-a99391fa4d15",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|            1|       101|      501|       2|150.0|2024-09-01|\n",
            "|            2|       102|      502|       1|250.0|2024-09-01|\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|\n",
            "|            5|       104|      504|       1|450.0|2024-09-03|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 3: Explore the Data**\n",
        "#1. **Print the Schema:**\n",
        "df.printSchema()\n",
        "\n",
        "#2. **Show the First Few Rows:**\n",
        "df.show(5)\n",
        "\n",
        "#3. **Get Summary Statistics:**\n",
        "df.describe(\"Quantity\", \"Price\").show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xCJ_DmMHpnfT",
        "outputId": "ff28f22f-7bfd-4d5b-e3e1-804d6ff7b5ae",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- TransactionID: integer (nullable = true)\n",
            " |-- CustomerID: integer (nullable = true)\n",
            " |-- ProductID: integer (nullable = true)\n",
            " |-- Quantity: integer (nullable = true)\n",
            " |-- Price: double (nullable = true)\n",
            " |-- Date: date (nullable = true)\n",
            "\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "|            1|       101|      501|       2|150.0|2024-09-01|\n",
            "|            2|       102|      502|       1|250.0|2024-09-01|\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|\n",
            "|            5|       104|      504|       1|450.0|2024-09-03|\n",
            "+-------------+----------+---------+--------+-----+----------+\n",
            "only showing top 5 rows\n",
            "\n",
            "+-------+-----------------+-----------------+\n",
            "|summary|         Quantity|            Price|\n",
            "+-------+-----------------+-----------------+\n",
            "|  count|               10|               10|\n",
            "|   mean|              2.2|            300.0|\n",
            "| stddev|1.398411797560202|141.4213562373095|\n",
            "|    min|                1|            150.0|\n",
            "|    max|                5|            550.0|\n",
            "+-------+-----------------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#### **Step 4: Perform Data Transformations and Analysis**\n",
        "#1. **Calculate the Total Sales Value for Each Transaction:**\n",
        "df = df.withColumn(\"TotalSales\", col(\"Quantity\") * col(\"Price\"))\n",
        "df.show()\n",
        "\n",
        "#2. **Group By ProductID and Calculate Total Sales Per Product:**\n",
        "df.groupBy(\"ProductID\").sum(\"TotalSales\").alias(\"TotalProductSales\").show()\n",
        "\n",
        "#3. **Identify the Top-Selling Product:**\n",
        "from pyspark.sql.functions import desc\n",
        "df.groupBy(\"ProductID\").sum(\"TotalSales\").alias(\"TotalProductSales\").orderBy(desc(\"sum(TotalSales)\")).show(1)\n",
        "\n",
        "#4. **Calculate the Total Sales by Date:**\n",
        "df.groupBy(\"Date\").sum(\"TotalSales\").alias(\"TotalSalesByDate\").orderBy(\"Date\").show()\n",
        "\n",
        "#5. **Filter High-Value Transactions:**\n",
        "df.filter(col(\"TotalSales\") > 500).show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ggm1_8eqKV5",
        "outputId": "989e88b6-2f65-4036-842d-ccaa2bf90f58",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|TotalSalesValue|TotalSales|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|            1|       101|      501|       2|150.0|2024-09-01|          300.0|     300.0|\n",
            "|            2|       102|      502|       1|250.0|2024-09-01|          250.0|     250.0|\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|          600.0|     600.0|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|          900.0|     900.0|\n",
            "|            5|       104|      504|       1|450.0|2024-09-03|          450.0|     450.0|\n",
            "|            6|       102|      502|       2|250.0|2024-09-03|          500.0|     500.0|\n",
            "|            7|       103|      503|       5|300.0|2024-09-04|         1500.0|    1500.0|\n",
            "|            8|       104|      504|       1|450.0|2024-09-04|          450.0|     450.0|\n",
            "|            9|       101|      501|       2|150.0|2024-09-05|          300.0|     300.0|\n",
            "|           10|       105|      505|       1|550.0|2024-09-05|          550.0|     550.0|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "\n",
            "+---------+---------------+\n",
            "|ProductID|sum(TotalSales)|\n",
            "+---------+---------------+\n",
            "|      501|         1200.0|\n",
            "|      504|          900.0|\n",
            "|      502|          750.0|\n",
            "|      505|          550.0|\n",
            "|      503|         2400.0|\n",
            "+---------+---------------+\n",
            "\n",
            "+---------+---------------+\n",
            "|ProductID|sum(TotalSales)|\n",
            "+---------+---------------+\n",
            "|      503|         2400.0|\n",
            "+---------+---------------+\n",
            "only showing top 1 row\n",
            "\n",
            "+----------+---------------+\n",
            "|      Date|sum(TotalSales)|\n",
            "+----------+---------------+\n",
            "|2024-09-01|          550.0|\n",
            "|2024-09-02|         1500.0|\n",
            "|2024-09-03|          950.0|\n",
            "|2024-09-04|         1950.0|\n",
            "|2024-09-05|          850.0|\n",
            "+----------+---------------+\n",
            "\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|TransactionID|CustomerID|ProductID|Quantity|Price|      Date|TotalSalesValue|TotalSales|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "|            3|       103|      501|       4|150.0|2024-09-02|          600.0|     600.0|\n",
            "|            4|       101|      503|       3|300.0|2024-09-02|          900.0|     900.0|\n",
            "|            7|       103|      503|       5|300.0|2024-09-04|         1500.0|    1500.0|\n",
            "|           10|       105|      505|       1|550.0|2024-09-05|          550.0|     550.0|\n",
            "+-------------+----------+---------+--------+-----+----------+---------------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Additional Challenge (Optional):**\n",
        "#1. **Identify Repeat Customers:**\n",
        "df.groupBy(\"CustomerID\").count().filter(col(\"count\") > 1).show()\n",
        "\n",
        "#2. **Calculate the Average Sale Price Per Product:**\n",
        "df.groupBy(\"ProductID\").avg(\"Price\").alias(\"AvgPricePerProduct\").show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RpqqJlw_uA9V",
        "outputId": "294f7e3e-1c69-445b-fe7f-8beea39b4932"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+-----+\n",
            "|CustomerID|count|\n",
            "+----------+-----+\n",
            "|       101|    3|\n",
            "|       103|    2|\n",
            "|       102|    2|\n",
            "|       104|    2|\n",
            "+----------+-----+\n",
            "\n",
            "+---------+----------+\n",
            "|ProductID|avg(Price)|\n",
            "+---------+----------+\n",
            "|      501|     150.0|\n",
            "|      504|     450.0|\n",
            "|      502|     250.0|\n",
            "|      505|     550.0|\n",
            "|      503|     300.0|\n",
            "+---------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "# Initialize SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"RDD Transformation Example\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "# Get the SparkContext from the SparkSession\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark Session Created\")\n",
        "\n",
        "data = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
        "rdd = sc.parallelize(data)\n",
        "\n",
        "# Print the original RDD\n",
        "print(\"Original RDD:\", rdd.collect())\n",
        "\n",
        "rdd2 =  rdd.map(lambda x: x* 2)\n",
        "\n",
        "#Print the transformed RDD\n",
        "print(\"RDD after map transformation (x2):\", rdd2.collect())\n",
        "\n"
      ],
      "metadata": {
        "id": "ii5Kd-L-y601",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "7917e0e3-3fd8-4686-fdce-0881670c771a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Created\n",
            "Original RDD: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "RDD after map transformation (x2): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rdd3 = rdd2.filter(lambda x: x % 2 == 0)\n",
        "\n",
        "#Print the filtered RDD\n",
        "print(\"RDD after filter transformation (even numbers):\", rdd3.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "DeBe5bJQNm3N",
        "outputId": "864c6a56-aa44-4a1b-95bf-2d9e7c649182"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD after filter transformation (even numbers): [2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = [\"Hello world\", \"PySpark is great\", \"RDD transformations\"]\n",
        "rdd4 = sc.parallelize (sentences)\n",
        "words_rdd = rdd4.flatMap(lambda sentence: sentence.split(\" \"))\n",
        "\n",
        "# Print the flatMapped RDD\n",
        "print(\"RDD after flatMap transformation (split into words):\", words_rdd.collect())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "VMnsYOFbNz43",
        "outputId": "9730efeb-ca25-4f17-b682-616236523b48"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RDD after flatMap transformation (split into words): ['Hello', 'world', 'PySpark', 'is', 'great', 'RDD', 'transformations']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = rdd3.collect()\n",
        "print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-4AXZhi6O4S-",
        "outputId": "f1b9b909-a359-4446-bc14-2bd679e5f361"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[2, 4, 6, 8, 10, 12, 14, 16, 18, 20]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "count =  rdd3.count()\n",
        "print(f\"Number of Elements: {count}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "p2Edy_ZyRv8F",
        "outputId": "d10d7bfc-da80-4610-dd75-2be07f739141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Elements: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "total_sum = rdd.reduce(lambda x, y: x + y)\n",
        "print(f\"Total Sum: {total_sum}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "9wERJSw3R-dM",
        "outputId": "13e5c609-cc47-4822-f20b-8be66e95cfa4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total Sum: 55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Exercise: Working with Key-Value Pair RDDs in PySpark**\n",
        "### **Step 1: Initialize Spark Context**\n",
        "\n",
        "#1. **Initialize SparkSession and SparkContext:**\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder \\\n",
        ".appName(\"Key-Value Pair RDD Transformations\") \\\n",
        ".getOrCreate()\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark Session Created\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QP8qISTGTZyT",
        "outputId": "98020faa-01dd-468b-9bef-1f8a0674896f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark Session Created\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 2: Create and Explore the RDD**\n",
        "#2. **Task 1: Create an RDD from the Sales Data**\n",
        "sales_data = [\n",
        "    (\"ProductA\", 100),\n",
        "    (\"ProductB\", 150),\n",
        "    (\"ProductA\", 200),\n",
        "    (\"ProductC\", 300),\n",
        "    (\"ProductB\", 250),\n",
        "    (\"ProductC\", 100)\n",
        "]\n",
        "\n",
        "sales_rdd = sc.parallelize(sales_data)\n",
        "\n",
        "# Print the first few elements of the RDD\n",
        "print(sales_rdd.collect())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "OwrwG9p8WvDd",
        "outputId": "dc7e4aef-a102-46e7-de54-c56e51c20d31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('ProductA', 100), ('ProductB', 150), ('ProductA', 200), ('ProductC', 300), ('ProductB', 250), ('ProductC', 100)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 3: Grouping and Aggregating Data**\n",
        "\n",
        "#3. **Task 2: Group Data by Product Name**\n",
        "grouped_sales_rdd = sales_rdd.groupByKey()\n",
        "\n",
        "# To see the grouped data\n",
        "grouped_sales = grouped_sales_rdd.mapValues(list).collect()\n",
        "print(\"grouped sales:\")\n",
        "print(grouped_sales)\n",
        "\n",
        "#4. **Task 3: Calculate Total Sales by Product**\n",
        "total_sales_rdd = sales_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the total sales for each product\n",
        "print(\"Total Sales by Product:\")\n",
        "print(total_sales_rdd.collect())\n",
        "\n",
        "#5. **Task 4: Sort Products by Total Sales**\n",
        "sorted_sales_rdd = total_sales_rdd.sortBy(lambda x: x[1], ascending=False)\n",
        "\n",
        "# Print the sorted list of products along with their sales amounts\n",
        "print(\"Sorted Products by Total Sales:\")\n",
        "print(sorted_sales_rdd.collect())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "sGachWqPXP-m",
        "outputId": "44483df9-b31f-4bbb-a11f-f4254cdaa59e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "grouped sales:\n",
            "[('ProductA', [100, 200]), ('ProductB', [150, 250]), ('ProductC', [300, 100])]\n",
            "Total Sales by Product:\n",
            "[('ProductA', 300), ('ProductB', 400), ('ProductC', 400)]\n",
            "Sorted Products by Total Sales:\n",
            "[('ProductB', 400), ('ProductC', 400), ('ProductA', 300)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 4: Additional Transformations**\n",
        "\n",
        "#6. **Task 5: Filter Products with High Sales**\n",
        "high_sales_rdd = total_sales_rdd.filter(lambda x: x[1] > 200)\n",
        "\n",
        "# Print the products with high sales\n",
        "print(\"Products with High Sales:\")\n",
        "print(high_sales_rdd.collect())\n",
        "\n",
        "#7. **Task 6: Combine Regional Sales Data**\n",
        "regional_sales_data = [\n",
        "    (\"ProductA\", 50),\n",
        "    (\"ProductC\", 150)\n",
        "]\n",
        "\n",
        "regional_sales_rdd = sc.parallelize(regional_sales_data)\n",
        "\n",
        "combined_rdd = sales_rdd.union(regional_sales_rdd)\n",
        "combined_total_sales_rdd = combined_rdd.reduceByKey(lambda a, b: a + b)\n",
        "\n",
        "# Print the combined sales data\n",
        "print(\"Combined Sales Data:\")\n",
        "print(combined_total_sales_rdd.collect())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "KKSfITawX3f9",
        "outputId": "6de19926-79f7-453f-c31e-5071c046294e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Products with High Sales:\n",
            "[('ProductA', 300), ('ProductB', 400), ('ProductC', 400)]\n",
            "Combined Sales Data:\n",
            "[('ProductA', 350), ('ProductC', 550), ('ProductB', 400)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Step 5: Perform Actions on the RDD**\n",
        "\n",
        "#8. **Task 7: Count the Number of Distinct Products**\n",
        "distinct_products_count = sales_rdd.map(lambda x: x[0]).distinct().count()\n",
        "\n",
        "# Print the count of distinct products\n",
        "print(\"Number of Distinct Products:\", distinct_products_count)\n",
        "\n",
        "#9. **Task 8: Identify the Product with Maximum Sales**\n",
        "total_sales_rdd = sales_rdd.reduceByKey(lambda x, y: x + y)\n",
        "max_sales_product = total_sales_rdd.reduce(lambda a, b: a if a[1] > b[1] else b)\n",
        "\n",
        "print(f\"Product with maximum sales: {max_sales_product[0]} with sales amount: {max_sales_product[1]}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "RsWXSnzMaZ39",
        "outputId": "af34d49e-64a1-485f-a240-3c7e6bf561c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Distinct Products: 3\n",
            "Product with maximum sales: ProductC with sales amount: 400\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Challenge Task: Calculate the Average Sales per Product**\n",
        "\n",
        "#10. *Challenge Task:**\n",
        "#Calculate the average sales amount per product using the key-value pair RDD.\n",
        "average_sales_per_product = total_sales_rdd.mapValues(lambda x: x / distinct_products_count).collect()\n",
        "\n",
        "# Print the average sales per product\n",
        "print(\"Average Sales per Product:\")\n",
        "print(average_sales_per_product)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Elbd_XoHb3L_",
        "outputId": "aa97366b-91d2-4da0-812e-e572d0a747d0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Sales per Product:\n",
            "[('ProductA', 100.0), ('ProductB', 133.33333333333334), ('ProductC', 133.33333333333334)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "#Create a DataFrames\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"DataFrameOperations\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [\n",
        "    (1, 'John Doe', 'New York', 28),\n",
        "    (2, 'Jane Smith', 'Los Angeles', 34),\n",
        "    (3, 'Sam Brown', 'Chicago', 22),\n",
        "    (4, 'Lisa Ray', 'Houston', 45)\n",
        "]\n",
        "\n",
        "# Creating a DataFrame\n",
        "columns = ['CustomerID', 'Name', 'City', 'Age']\n",
        "df = spark.createDataFrame(data, columns)\n",
        "df.show()\n",
        "\n",
        "#Selecting, Renaming, Filtering Data in a Pandas DataFrame\n",
        "# Selecting a single column\n",
        "df.select('Name').show()\n",
        "\n",
        "# Selecting multiple columns\n",
        "df.select('Name', 'City').show()\n",
        "\n",
        "# Renaming a column\n",
        "df.withColumnRenamed('City', 'Location').show()\n",
        "\n",
        "# Filtering data\n",
        "df.filter(col('Age') > 30).show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hei2BaO0xakA",
        "outputId": "72385b3a-e90b-47b1-fb26-0ac6abef019b",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+----------+-----------+---+\n",
            "|CustomerID|      Name|       City|Age|\n",
            "+----------+----------+-----------+---+\n",
            "|         1|  John Doe|   New York| 28|\n",
            "|         2|Jane Smith|Los Angeles| 34|\n",
            "|         3| Sam Brown|    Chicago| 22|\n",
            "|         4|  Lisa Ray|    Houston| 45|\n",
            "+----------+----------+-----------+---+\n",
            "\n",
            "+----------+\n",
            "|      Name|\n",
            "+----------+\n",
            "|  John Doe|\n",
            "|Jane Smith|\n",
            "| Sam Brown|\n",
            "|  Lisa Ray|\n",
            "+----------+\n",
            "\n",
            "+----------+-----------+\n",
            "|      Name|       City|\n",
            "+----------+-----------+\n",
            "|  John Doe|   New York|\n",
            "|Jane Smith|Los Angeles|\n",
            "| Sam Brown|    Chicago|\n",
            "|  Lisa Ray|    Houston|\n",
            "+----------+-----------+\n",
            "\n",
            "+----------+----------+-----------+---+\n",
            "|CustomerID|      Name|   Location|Age|\n",
            "+----------+----------+-----------+---+\n",
            "|         1|  John Doe|   New York| 28|\n",
            "|         2|Jane Smith|Los Angeles| 34|\n",
            "|         3| Sam Brown|    Chicago| 22|\n",
            "|         4|  Lisa Ray|    Houston| 45|\n",
            "+----------+----------+-----------+---+\n",
            "\n",
            "+----------+----------+-----------+---+\n",
            "|CustomerID|      Name|       City|Age|\n",
            "+----------+----------+-----------+---+\n",
            "|         2|Jane Smith|Los Angeles| 34|\n",
            "|         4|  Lisa Ray|    Houston| 45|\n",
            "+----------+----------+-----------+---+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Employee Data Analysis\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Sample employee data\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, 'Shalini', 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', 50000),\n",
        "    (5, 'Rahul', 'Finance', 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "# Create DataFrame\n",
        "employee_df = spark.createDataFrame(data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "MIRveRcKFpfQ",
        "outputId": "344ddb9b-76b2-4e2c-c7e3-e3e75aaf290f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#1. **Task 1: Filter Employees by Salary**\n",
        "filtered_df = employee_df.filter(col('Salary') > 60000)\n",
        "print(\"Employees with Salary > 60000:\")\n",
        "filtered_df.show()\n",
        "\n",
        "#2. **Task 2: Calculate the Average Salary by Department**\n",
        "#from pyspark.sql.functions import avg\n",
        "avg_salary_df = employee_df.groupBy('Department').avg('Salary').alias('AverageSalary')\n",
        "print(\"Average Salary by Department:\")\n",
        "avg_salary_df.show()\n",
        "\n",
        "#3. **Task 3: Sort Employees by Salary**\n",
        "sorted_salary_df = employee_df.orderBy(col(\"Salary\").desc())\n",
        "print(\"Employees sorted by salary:\")\n",
        "sorted_salary_df.show()\n",
        "\n",
        "#4. **Task 4: Add a Bonus Column**\n",
        "#from pyspark.sql.functions import expr\n",
        "bonus_df = employee_df.withColumn(\"Bonus\", col(\"Salary\") * 0.10)\n",
        "print(\"Employee DataFrame with Bonus Column:\")\n",
        "bonus_df.show()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "rz_eWMjpKxQH",
        "outputId": "35acc7e4-1bac-4df8-9c5a-6b2dac61de4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Employees with Salary > 60000:\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "Average Salary by Department:\n",
            "+----------+-----------------+\n",
            "|Department|      avg(Salary)|\n",
            "+----------+-----------------+\n",
            "|   Finance|          72500.0|\n",
            "|        IT|73333.33333333333|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "Employees sorted by salary:\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         3|     Shalini|        IT| 90000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         5|       Rahul|   Finance| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "Employee DataFrame with Bonus Column:\n",
            "+----------+------------+----------+------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary| Bonus|\n",
            "+----------+------------+----------+------+------+\n",
            "|         1|       Arjun|        IT| 75000|7500.0|\n",
            "|         2|       Vijay|   Finance| 85000|8500.0|\n",
            "|         3|     Shalini|        IT| 90000|9000.0|\n",
            "|         4|       Sneha|        HR| 50000|5000.0|\n",
            "|         5|       Rahul|   Finance| 60000|6000.0|\n",
            "|         6|        Amit|        IT| 55000|5500.0|\n",
            "+----------+------------+----------+------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col\n",
        "#Initialize Spark session\n",
        "spark = SparkSession.builder\\\n",
        ".appName(\"Employee Data Handling\") \\\n",
        ".getOrCreate()\n",
        "\n",
        "#Sample employee data with null values\n",
        "data = [\n",
        "    (1, 'Arjun', 'IT', 75000),\n",
        "    (2, 'Vijay', 'Finance', 85000),\n",
        "    (3, None, 'IT', 90000),\n",
        "    (4, 'Sneha', 'HR', None),\n",
        "    (5, 'Rahul', None, 60000),\n",
        "    (6, 'Amit', 'IT', 55000)\n",
        "]\n",
        "\n",
        "#Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary']\n",
        "\n",
        "#Create DataFrame\n",
        "employee_df = spark.createDataFrame (data, columns)\n",
        "\n",
        "# Show the DataFrame\n",
        "employee_df.show()\n",
        "\n",
        "#Fill null values in 'EmployeeName' and 'Department' with 'Unknown'\n",
        "filled_df = employee_df.fillna({'EmployeeName': 'Unknown', 'Department': 'Unknown'})\n",
        "filled_df.show()\n",
        "\n",
        "#Drop rows where 'Salary' is null\n",
        "dropped_null_salary_df= employee_df.dropna (subset=['Salary'])\n",
        "dropped_null_salary_df.show()\n",
        "\n",
        "#Fill null values in 'Salary' with 50000\n",
        "salary_filled_df = employee_df.fillna({'Salary': 50000})\n",
        "salary_filled_df.show()\n",
        "\n",
        "#Check for null values in the entire DataFrame\n",
        "null_counts = employee_df.select([col(c).isNull().alias(c) for c in employee_df.columns]).show()\n",
        "\n",
        "#Replace all null values in the DataFrame with N/A\n",
        "na_filled_df = employee_df.na.fill('N/A')\n",
        "na_filled_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "jG59SP9717bx",
        "outputId": "d3b529f9-c7df-4c4a-f2c1-9861adc3f858"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|     Unknown|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|   Unknown| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|        NULL|        IT| 90000|\n",
            "|         4|       Sneha|        HR| 50000|\n",
            "|         5|       Rahul|      NULL| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|     false|       false|     false| false|\n",
            "|     false|       false|     false| false|\n",
            "|     false|        true|     false| false|\n",
            "|     false|       false|     false|  true|\n",
            "|     false|       false|      true| false|\n",
            "|     false|       false|     false| false|\n",
            "+----------+------------+----------+------+\n",
            "\n",
            "+----------+------------+----------+------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|\n",
            "+----------+------------+----------+------+\n",
            "|         1|       Arjun|        IT| 75000|\n",
            "|         2|       Vijay|   Finance| 85000|\n",
            "|         3|         N/A|        IT| 90000|\n",
            "|         4|       Sneha|        HR|  NULL|\n",
            "|         5|       Rahul|       N/A| 60000|\n",
            "|         6|        Amit|        IT| 55000|\n",
            "+----------+------------+----------+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "#Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        ".appName (\"Advanced DataFrame Operations\") \\\n",
        ".getOrCreate()\n",
        "# Create two sample DataFrames\n",
        "datal = [\n",
        "    (1, 'Arjun', 'IT', 75000, '2022-01-15'),\n",
        "    (2, 'Vijay', 'Finance', 85000, '2022-03-12'),\n",
        "    (3, 'Shalini', 'IT', 90000, '2021-06-30')\n",
        "]\n",
        "data2 = [\n",
        "    (4, 'Sneha', 'HR', 50000, '2022-05-01'),\n",
        "    (5, 'Rahul', 'Finance', 60000, '2022-08-20'),\n",
        "    (6, 'Amit', 'IT', 55000, '2021-12-15')\n",
        "]\n",
        "\n",
        "#Define schema (columns)\n",
        "columns = ['EmployeeID', 'EmployeeName', 'Department', 'Salary', 'JoiningDate']\n",
        "\n",
        "#Create DataFrames\n",
        "employee_df1 = spark.createDataFrame (datal, columns)\n",
        "employee_df2 =  spark.createDataFrame (data2, columns)\n",
        "\n",
        "#Show the DataFrames\n",
        "employee_df1.show()\n",
        "employee_df2.show()\n",
        "\n",
        "# Union of two DataFrames (removes duplicates)\n",
        "union_df = employee_df1.union (employee_df2).dropDuplicates()\n",
        "print(\"Union of DataFrames (without duplicates):\")\n",
        "union_df.show()\n",
        "\n",
        "# Union of two DataFrames (includes duplicates)\n",
        "union_all_df = employee_df1.union(employee_df2)\n",
        "print(\"Union of DataFrames (with duplicates):\")\n",
        "union_all_df.show()\n",
        "\n",
        "from pyspark.sql.window import Window\n",
        "from pyspark.sql.functions import rank\n",
        "\n",
        "#Define a window specification to rank employees by salary within each department\n",
        "window_spec =  Window.partitionBy(\"Department\").orderBy(col(\"Salary\").desc())\n",
        "\n",
        "#Add a rank column to the DataFrame\n",
        "ranked_df  = union_all_df.withColumn(\"Rank\", rank().over(window_spec))\n",
        "print(\"DataFrame with ranks:\")\n",
        "ranked_df.show()\n",
        "\n",
        "from pyspark.sql.functions import sum\n",
        "\n",
        "#Define a window specification for cumulative sum of salaries within each department\n",
        "window_spec_sum =  Window.partitionBy(\"Department\").orderBy(\"JoiningDate\").rowsBetween(Window.unboundedPreceding, Window. currentRow)\n",
        "\n",
        "# Calculate the running total of salaries\n",
        "running_total_df = union_all_df.withColumn (\"RunningTotal\", sum(col (\"Salary\")).over(window_spec_sum))\n",
        "print(\"DataFrame with running total:\")\n",
        "running_total_df.show()\n",
        "\n",
        "# Convert JoiningDate from string to date type\n",
        "date_converted_df =  union_all_df.withColumn(\"JoiningDate\",F.to_date(col(\"JoiningDate\"), \"yyyy-MM-dd\").cast(\"date\"))\n",
        "print(\"DataFrame with converted JoiningDate:\")\n",
        "date_converted_df.show()\n",
        "\n",
        "# Replace invalid dates with null\n",
        "#date_converted_df = date_converted_df.fillna({'JoiningDate': None})\n",
        "#date_converted_df.show()\n",
        "\n",
        "# Calculate the number of years since joining\n",
        "experience_df = date_converted_df.withColumn(\"YearsOfExperience\", F.round (F.datediff (F.current_date(), col (\"JoiningDate\")) /\n",
        "365, 2))\n",
        "print(\"DataFrame with years of experience:\")\n",
        "experience_df.show()\n",
        "\n",
        "#Add a new column for next evaluation date (one year after joining)\n",
        "eval_date_df =  date_converted_df.withColumn(\"NextEvaluationDate\", F.date_add(col(\"JoiningDate\"), 365))\n",
        "print(\"DataFrame with next evaluation date:\")\n",
        "eval_date_df.show()\n",
        "\n",
        "#Calculate average salary per department\n",
        "avg_salary_df = union_all_df.groupBy(\"Department\").agg(F.avg(\"Salary\").alias(\"AverageSalary\"))\n",
        "print(\"Average salary per department:\")\n",
        "avg_salary_df.show()\n",
        "\n",
        "# Calculate the total number of employees\n",
        "total_employees_df = union_all_df.agg (F.count(\"EmployeeID\").alias (\"TotalEmployees\"))\n",
        "print(\"Total number of employees:\")\n",
        "total_employees_df.show()\n",
        "\n",
        "# Convert employee names to uppercase\n",
        "upper_name_df = union_all_df.withColumn(\"EmployeeNameUpper\", F.upper(col(\"EmployeeName\")))\n",
        "print(\"DataFrame with employee names in uppercase:\")\n",
        "upper_name_df.show()\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "CW-yTPEaNART",
        "outputId": "ca27acfe-4ed1-47e9-d02a-8ac76228f310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of DataFrames (without duplicates):\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "Union of DataFrames (with duplicates):\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "DataFrame with ranks:\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|Rank|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|   1|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|   2|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|   1|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|   1|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|   2|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|   3|\n",
            "+----------+------------+----------+------+-----------+----+\n",
            "\n",
            "DataFrame with running total:\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|RunningTotal|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|       85000|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|      145000|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|       50000|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|       90000|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|      145000|\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|      220000|\n",
            "+----------+------------+----------+------+-----------+------------+\n",
            "\n",
            "DataFrame with converted JoiningDate:\n",
            "+----------+------------+----------+------+-----------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|\n",
            "+----------+------------+----------+------+-----------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|\n",
            "+----------+------------+----------+------+-----------+\n",
            "\n",
            "DataFrame with years of experience:\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|YearsOfExperience|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|             2.64|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|             2.48|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|             3.18|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|             2.35|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|             2.04|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|             2.72|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "\n",
            "DataFrame with next evaluation date:\n",
            "+----------+------------+----------+------+-----------+------------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|NextEvaluationDate|\n",
            "+----------+------------+----------+------+-----------+------------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|        2023-01-15|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|        2023-03-12|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|        2022-06-30|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|        2023-05-01|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|        2023-08-20|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|        2022-12-15|\n",
            "+----------+------------+----------+------+-----------+------------------+\n",
            "\n",
            "Average salary per department:\n",
            "+----------+-----------------+\n",
            "|Department|    AverageSalary|\n",
            "+----------+-----------------+\n",
            "|        IT|73333.33333333333|\n",
            "|   Finance|          72500.0|\n",
            "|        HR|          50000.0|\n",
            "+----------+-----------------+\n",
            "\n",
            "Total number of employees:\n",
            "+--------------+\n",
            "|TotalEmployees|\n",
            "+--------------+\n",
            "|             6|\n",
            "+--------------+\n",
            "\n",
            "DataFrame with employee names in uppercase:\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|EmployeeID|EmployeeName|Department|Salary|JoiningDate|EmployeeNameUpper|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "|         1|       Arjun|        IT| 75000| 2022-01-15|            ARJUN|\n",
            "|         2|       Vijay|   Finance| 85000| 2022-03-12|            VIJAY|\n",
            "|         3|     Shalini|        IT| 90000| 2021-06-30|          SHALINI|\n",
            "|         4|       Sneha|        HR| 50000| 2022-05-01|            SNEHA|\n",
            "|         5|       Rahul|   Finance| 60000| 2022-08-20|            RAHUL|\n",
            "|         6|        Amit|        IT| 55000| 2021-12-15|             AMIT|\n",
            "+----------+------------+----------+------+-----------+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Data Setup:\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Advanced DataFrame Operations - Different Dataset\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Create two sample DataFrames for Product Sales\n",
        "data1 = [\n",
        "    (1, 'Product A', 'Electronics', 1200, '2022-05-10'),\n",
        "    (2, 'Product B', 'Clothing', 500, '2022-07-15'),\n",
        "    (3, 'Product C', 'Electronics', 1800, '2021-11-05')\n",
        "]\n",
        "\n",
        "data2 = [\n",
        "    (4, 'Product D', 'Furniture', 3000, '2022-03-25'),\n",
        "    (5, 'Product E', 'Clothing', 800, '2022-09-12'),\n",
        "    (6, 'Product F', 'Electronics', 1500, '2021-10-19')\n",
        "]\n",
        "\n",
        "# Define schema (columns)\n",
        "columns = ['ProductID', 'ProductName', 'Category', 'Price', 'SaleDate']\n",
        "\n",
        "# Create DataFrames\n",
        "sales_df1 = spark.createDataFrame(data1, columns)\n",
        "sales_df2 = spark.createDataFrame(data2, columns)\n",
        "\n",
        "#show the dataframes\n",
        "sales_df1.show()\n",
        "sales_df2.show()\n",
        "\n",
        "### Tasks:\n",
        "\n",
        "#1. **Union of DataFrames (removing duplicates)**:\n",
        "union_df = sales_df1.union(sales_df2).dropDuplicates()\n",
        "print(\"Union of DataFrames (without duplicates):\")\n",
        "union_df.show()\n",
        "\n",
        "#2. **Union of DataFrames (including duplicates)**:\n",
        "union_all_df = sales_df1.union(sales_df2)\n",
        "print(\"Union of DataFrames (with duplicates):\")\n",
        "union_all_df.show()\n",
        "\n",
        "#3. **Rank products by price within their category**:\n",
        "window_spec = Window.partitionBy(\"Category\").orderBy(F.desc(\"Price\"))\n",
        "ranked_df = union_df.withColumn(\"Rank\", F.row_number().over(window_spec))\n",
        "print(\"DataFrame with ranks:\")\n",
        "ranked_df.show()\n",
        "\n",
        "#4. **Calculate cumulative price per category**:\n",
        "cumulative_df = union_df.withColumn(\"CumulativePrice\", F.sum(\"Price\").over(window_spec))\n",
        "print(\"DataFrame with cumulative price:\")\n",
        "cumulative_df.show()\n",
        "\n",
        "#5. **Convert `SaleDate` from string to date type**:\n",
        "converted_df = union_df.withColumn(\"SaleDate\", F.to_date(\"SaleDate\", \"yyyy-MM-dd\"))\n",
        "print(\"DataFrame with converted SaleDate:\")\n",
        "converted_df.show()\n",
        "\n",
        "#6. **Calculate the number of days since each sale**:\n",
        "days_since_sale_df = converted_df.withColumn(\"DaysSinceSale\", F.datediff(F.current_date(), \"SaleDate\"))\n",
        "print(\"DataFrame with days since sale:\")\n",
        "days_since_sale_df.show()\n",
        "\n",
        "#7. **Add a column for the next sale deadline**:\n",
        "next_sale_deadline_df = converted_df.withColumn(\"NextSaleDeadline\", F.date_add(\"SaleDate\", 30))\n",
        "print(\"DataFrame with next sale deadline:\")\n",
        "next_sale_deadline_df.show()\n",
        "\n",
        "#8. **Calculate total revenue and average price per category**:\n",
        "revenue_df = union_df.groupBy(\"Category\").agg(\n",
        "    F.sum(\"Price\").alias(\"TotalRevenue\"),\n",
        "    F.avg(\"Price\").alias(\"AveragePrice\")\n",
        ")\n",
        "print(\"Total revenue and average price per category:\")\n",
        "revenue_df.show()\n",
        "\n",
        "#9. **Convert all product names to lowercase**:\n",
        "lower_case_df = union_df.withColumn(\"ProductNameLower\", F.lower(\"ProductName\"))\n",
        "print(\"DataFrame with product names in lowercase:\")\n",
        "lower_case_df.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9BI4FhLz6ZQ",
        "outputId": "c96fe35d-eb83-4b52-b803-99d566550e17",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "Union of DataFrames (without duplicates):\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "Union of DataFrames (with duplicates):\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "DataFrame with ranks:\n",
            "+---------+-----------+-----------+-----+----------+----+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|Rank|\n",
            "+---------+-----------+-----------+-----+----------+----+\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|   1|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|   2|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|   1|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|   2|\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|   3|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|   1|\n",
            "+---------+-----------+-----------+-----+----------+----+\n",
            "\n",
            "DataFrame with cumulative price:\n",
            "+---------+-----------+-----------+-----+----------+---------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|CumulativePrice|\n",
            "+---------+-----------+-----------+-----+----------+---------------+\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|            800|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|           1300|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|           1800|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|           3300|\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|           4500|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|           3000|\n",
            "+---------+-----------+-----------+-----+----------+---------------+\n",
            "\n",
            "DataFrame with converted SaleDate:\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|\n",
            "+---------+-----------+-----------+-----+----------+\n",
            "\n",
            "DataFrame with days since sale:\n",
            "+---------+-----------+-----------+-----+----------+-------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|DaysSinceSale|\n",
            "+---------+-----------+-----------+-----+----------+-------------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|          848|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|          782|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|         1034|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|          894|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|         1051|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|          723|\n",
            "+---------+-----------+-----------+-----+----------+-------------+\n",
            "\n",
            "DataFrame with next sale deadline:\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|NextSaleDeadline|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|      2022-06-09|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|      2022-08-14|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|      2021-12-05|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|      2022-04-24|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|      2021-11-18|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|      2022-10-12|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "\n",
            "Total revenue and average price per category:\n",
            "+-----------+------------+------------+\n",
            "|   Category|TotalRevenue|AveragePrice|\n",
            "+-----------+------------+------------+\n",
            "|Electronics|        4500|      1500.0|\n",
            "|   Clothing|        1300|       650.0|\n",
            "|  Furniture|        3000|      3000.0|\n",
            "+-----------+------------+------------+\n",
            "\n",
            "DataFrame with product names in lowercase:\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|ProductID|ProductName|   Category|Price|  SaleDate|ProductNameLower|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "|        1|  Product A|Electronics| 1200|2022-05-10|       product a|\n",
            "|        2|  Product B|   Clothing|  500|2022-07-15|       product b|\n",
            "|        3|  Product C|Electronics| 1800|2021-11-05|       product c|\n",
            "|        4|  Product D|  Furniture| 3000|2022-03-25|       product d|\n",
            "|        6|  Product F|Electronics| 1500|2021-10-19|       product f|\n",
            "|        5|  Product E|   Clothing|  800|2022-09-12|       product e|\n",
            "+---------+-----------+-----------+-----+----------+----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#create a spark seesion\n",
        "spark= SparkSession.builder \\\n",
        ".appName(\"DtaIntegestion\") \\\n",
        ".getOrCreate()"
      ],
      "metadata": {
        "id": "r0KOlr9tPNhe"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "csv_file_path = \"/content/sample_data/people.csv.txt\"\n",
        "#Now you can read it with PySpark\n",
        "df_csv = spark.read.format(\"csv\").option(\"header\", \"true\").load(csv_file_path)\n",
        "df_csv.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-UvaG2xTXZws",
        "outputId": "6e5b8918-a259-47c5-abfa-f6e13f0e7850"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----+---+------+\n",
            "|Name|Age|Gender|\n",
            "+----+---+------+\n",
            "|John| 28|  Male|\n",
            "|Jane| 32|Female|\n",
            "+----+---+------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
        "\n",
        "# Define the schema for the JSON file\n",
        "schema = StructType([\n",
        "    StructField(\"name\", StringType(), True),\n",
        "    StructField(\"age\", IntegerType(), True),\n",
        "    StructField(\"gender\", StringType(), True),\n",
        "    StructField(\"address\", StructType([\n",
        "        StructField(\"street\", StringType(), True),\n",
        "        StructField(\"city\", StringType(), True)\n",
        "    ]), True)\n",
        "])\n",
        "\n",
        "#Load the complex 350N file with the correct path\n",
        "json_file_path = \"/content/sample_data/sample.json.txt\"\n",
        "\n",
        "#Read the JSON file with schema\n",
        "df_json_complex = spark.read.schema(schema).json(json_file_path)\n",
        "\n",
        "#Read the file as text to inspect its contents\n",
        "with open(json_file_path, 'r') as f:\n",
        "  data = f.read()\n",
        "  print(data)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "WhOi5_QWZ94q",
        "outputId": "c91f132d-99a6-48a3-d627-c5ce63203603"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[\n",
            "  {\n",
            "    \"name\": \"John\",\n",
            "    \"age\": 28,\n",
            "    \"gender\": \"Male\",\n",
            "    \"address\": {\n",
            "      \"street\": \"123 Main St\",\n",
            "      \"city\": \"New York\"\n",
            "    }\n",
            "  },\n",
            "  {\n",
            "    \"name\": \"Jane\",\n",
            "    \"age\": 32,\n",
            "    \"gender\": \"Female\",\n",
            "    \"address\": {\n",
            "      \"street\": \"456 Elm St\",\n",
            "      \"city\": \"San Francisco\"\n",
            "    }\n",
            "  }\n",
            "]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "#Create a sample DataFrame\n",
        "data = {\n",
        "    \"name\": [\"John\", \"Jane\", \"Mike\", \"Emily\"],\n",
        "    \"age\": [28, 32, 45, 231],\n",
        "    \"gender\": [\"Male\", \"Female\", \"Male\", \"Female\"],\n",
        "    \"city\": [\"New York\", \"San Francisco\", \"Los Angeles\", \"Chicago\"]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "#Save the DataFrame to a CSV file in the Colal environment\n",
        "csv_file_path=\"/content/sample_people.csv\"\n",
        "df.to_csv (csv_file_path, index=False)\n",
        "\n",
        "#Confirm the file has been created\n",
        "print (f\"CSV file created at: {csv_file_path}\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "#Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"CreateViewExample\").getOrCreate ()\n",
        "\n",
        "#Load the CSV file into a PySpark DataFrame\n",
        "df_people = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(csv_file_path)\n",
        "\n",
        "#Show the DataFrame\n",
        "df_people.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "a2-k920skzop",
        "outputId": "80929ca5-3c05-4fc7-e6ec-0bc612c939ae"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at: /content/sample_people.csv\n",
            "+-----+---+------+-------------+\n",
            "| name|age|gender|         city|\n",
            "+-----+---+------+-------------+\n",
            "| John| 28|  Male|     New York|\n",
            "| Jane| 32|Female|San Francisco|\n",
            "| Mike| 45|  Male|  Los Angeles|\n",
            "|Emily|231|Female|      Chicago|\n",
            "+-----+---+------+-------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Create a temporary view\n",
        "df_people.createOrReplaceTempView(\"people_temp_view\")\n",
        "\n",
        "#Run an SQL query on the view\n",
        "result_temp_view = spark.sql (\"SELECT name, age, gender, city FROM people_temp_view WHERE age > 30\")\n",
        "\n",
        "# Show the result\n",
        "result_temp_view.show()\n",
        "\n",
        "#Create a global temporary view\n",
        "df_people.createOrReplaceGlobalTempView(\"people_global_view\")\n",
        "\n",
        "#Query the global temporary view\n",
        "result_global_view = spark.sql (\"SELECT name, age, city FROM global_temp.people_global_view WHERE age < 30\")\n",
        "\n",
        "# Show the result\n",
        "result_global_view.show()\n",
        "\n",
        "#List all temporary views and tables\n",
        "spark.catalog.listTables ()\n",
        "\n",
        "#Drop the local temporary view\n",
        "spark.catalog.dropTempView(\"people_temp_view\")\n",
        "\n",
        "#Drop the global temporary view\n",
        "spark.catalog.dropGlobalTempView(\"people_global_view\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "8wI-1JW6mOb2",
        "outputId": "e259957e-9a0c-4e6d-957b-2fb316b9c2b3"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+-------------+\n",
            "| name|age|gender|         city|\n",
            "+-----+---+------+-------------+\n",
            "| Jane| 32|Female|San Francisco|\n",
            "| Mike| 45|  Male|  Los Angeles|\n",
            "|Emily|231|Female|      Chicago|\n",
            "+-----+---+------+-------------+\n",
            "\n",
            "+----+---+--------+\n",
            "|name|age|    city|\n",
            "+----+---+--------+\n",
            "|John| 28|New York|\n",
            "+----+---+--------+\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a new database in Spark SQL\n",
        "spark.sql(\"CREATE DATABASE IF NOT EXISTS my_database\")\n",
        "\n",
        "# Use the created database\n",
        "spark.sql(\"USE my_database\")\n",
        "\n",
        "# Verify that the database is being used\n",
        "spark.sql(\"SHOW DATABASES\").show()"
      ],
      "metadata": {
        "id": "2ubL6qqIno4L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Create a sample CSV data\n",
        "data = {\n",
        "    \"name\": [\"John\", \"Jane\", \"Mike\", \"Emily\", \"Alex\"],\n",
        "    \"age\": [28, 32, 45, 23, 36],\n",
        "    \"gender\": [\"Male\", \"Female\", \"Male\", \"Female\", \"Male\"],\n",
        "    \"salary\": [60000, 72000, 84000, 52000, 67000]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Save the DataFrame as a CSV file\n",
        "csv_file_path = \"/content/sample_people.csv\"\n",
        "df.to_csv(csv_file_path, index=False)\n",
        "\n",
        "# Confirm the CSV file is created\n",
        "print(f\"CSV file created at: {csv_file_path}\")\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.appName(\"Employee Salary ETL\").getOrCreate()\n",
        "\n",
        "# Load CSV data\n",
        "employee_df = spark.read.csv(\"sample_people.csv\", header=True, inferSchema=True)\n",
        "employee_df.show()\n",
        "\n",
        "# Filter employees with age >= 30\n",
        "filtered_df = employee_df.filter(employee_df.age >= 30)\n",
        "print(\"Filtered DataFrame:\")\n",
        "filtered_df.show()\n",
        "\n",
        "# Add a new column 'salary_with_bonus' (10% bonus on current salary)\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "transformed_df = filtered_df.withColumn(\"salary_with_bonus\", col(\"salary\") * 1.10)\n",
        "print(\"Transformed DataFrame:\")\n",
        "transformed_df.show()\n",
        "\n",
        "# Calculate average salary by gender\n",
        "avg_salary_by_gender = transformed_df.groupBy(\"gender\").avg(\"salary\")\n",
        "print(\"Average Salary by Gender:\")\n",
        "avg_salary_by_gender.show()\n",
        "\n",
        "# Save the transformed data in Parquet format\n",
        "transformed_df.write.parquet(\"transformed_sample_people.parquet\")\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "btADSKed7eaW",
        "outputId": "dbafda28-51b4-464d-8e18-ea0ee997ca6a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file created at: /content/sample_people.csv\n",
            "+-----+---+------+------+\n",
            "| name|age|gender|salary|\n",
            "+-----+---+------+------+\n",
            "| John| 28|  Male| 60000|\n",
            "| Jane| 32|Female| 72000|\n",
            "| Mike| 45|  Male| 84000|\n",
            "|Emily| 23|Female| 52000|\n",
            "| Alex| 36|  Male| 67000|\n",
            "+-----+---+------+------+\n",
            "\n",
            "Filtered DataFrame:\n",
            "+----+---+------+------+\n",
            "|name|age|gender|salary|\n",
            "+----+---+------+------+\n",
            "|Jane| 32|Female| 72000|\n",
            "|Mike| 45|  Male| 84000|\n",
            "|Alex| 36|  Male| 67000|\n",
            "+----+---+------+------+\n",
            "\n",
            "Transformed DataFrame:\n",
            "+----+---+------+------+-----------------+\n",
            "|name|age|gender|salary|salary_with_bonus|\n",
            "+----+---+------+------+-----------------+\n",
            "|Jane| 32|Female| 72000|          79200.0|\n",
            "|Mike| 45|  Male| 84000|92400.00000000001|\n",
            "|Alex| 36|  Male| 67000|          73700.0|\n",
            "+----+---+------+------+-----------------+\n",
            "\n",
            "Average Salary by Gender:\n",
            "+------+-----------+\n",
            "|gender|avg(salary)|\n",
            "+------+-----------+\n",
            "|Female|    72000.0|\n",
            "|  Male|    75500.0|\n",
            "+------+-----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Full refresh: Load the entire dataset\n",
        "df_sales = spark.read.format(\"CSV\") \\\n",
        ".option(\"header\", \"true\")\\\n",
        ".option(\"inferSchema\", \"true\") \\\n",
        ".load(\"/content/sample_data/sales_data.csv.txt\")\n",
        "\n",
        "\n",
        "#Apply transformations (if necessary)\n",
        "df_transformed  = df_sales.withColumn(\"total_sales\", df_sales[\"quantity\"] * df_sales[\"price\"])\n",
        "\n",
        "#Full refresh: Partition the data by 'date' and overwrite the existing data\n",
        "output_path = \"/content/sample_data/partitioned_data\"\n",
        "df_transformed.write.partitionBy(\"date\").mode(\"overwrite\").parquet(output_path)\n",
        "\n",
        "#Verify partitioned data\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "partitioned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-MtGL8XfXIyi",
        "outputId": "dad009a7-294e-47ec-d039-d77a9f20da3e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|transaction_id|customer_id| product|quantity|price|         updated_at|total_sales|      date|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Full refresh: Load the entire dataset\n",
        "df_sales = spark.read.format(\"CSV\") \\\n",
        ".option(\"header\", \"true\")\\\n",
        ".option(\"inferSchema\", \"true\") \\\n",
        ".load(\"/content/sample_data/sales_data.csv.txt\")\n",
        "\n",
        "\n",
        "#Apply transformations (if necessary)\n",
        "df_transformed  = df_sales.withColumn(\"total_sales\", df_sales[\"quantity\"] * df_sales[\"price\"])\n",
        "\n",
        "#Full refresh: Partition the data by 'date' and overwrite the existing data\n",
        "output_path = \"/content/sample_data/partitioned_data\"\n",
        "df_transformed.write.partitionBy(\"date\").mode(\"append\").parquet(output_path)\n",
        "\n",
        "#Verify partitioned data\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "partitioned_df.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "_VvJ_V0hlFqZ",
        "outputId": "56bacfe0-e719-428e-bf91-f604865f8e3e"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|transaction_id|customer_id| product|quantity|price|         updated_at|total_sales|      date|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "\n",
        "#Incremental load: Define the last ETL run timestamp (this should be tracked externally)\n",
        "last_etl_run = '2024-09-01 00:00:00'\n",
        "\n",
        "#Load only new or updated records since the last ETL run\n",
        "df_incremental = spark.read.format(\"csv\") \\\n",
        ".option(\"header\", \"true\") \\\n",
        ".option(\"InferSchema\", \"true\") \\\n",
        ".load(\"/content/sample_data/sales_data.csv.txt\") \\\n",
        ".filter(F.col(\"updated_at\") > last_etl_run)\n",
        "\n",
        "#Apply transformations (if necessary)\n",
        "df_transformed_incremental = df_incremental.withColumn(\"total_sales\", df_incremental [\"quantity\"] * df_incremental [\"price\"])\n",
        "\n",
        "#Incremental load: Append the new data to the existing partitioned dataset\n",
        "output_path=\"/content/sample_data/partitioned_data\"\n",
        "df_transformed_incremental.write.partitionBy(\"date\").mode(\"append\").parquet (output_path)\n",
        "\n",
        "#Verify partitioned data after incremental load\n",
        "partitioned_df = spark.read.parquet(output_path)\n",
        "partitioned_df.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "nXaxt2ZInSjg",
        "outputId": "3c792117-6450-4c6e-b5fe-cb18749c03a1"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|transaction_id|customer_id| product|quantity|price|         updated_at|total_sales|      date|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             1|        101|  Laptop|       1| 1000|2024-09-01 08:00:00|       1000|2024-09-01|\n",
            "|             2|        102|   Phone|       2|  500|2024-09-01 09:00:00|       1000|2024-09-01|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             5|        105|Keyboard|       1|   50|2024-09-03 12:00:00|         50|2024-09-03|\n",
            "|             6|        106|   Mouse|       3|   30|2024-09-03 13:00:00|         90|2024-09-03|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "|             3|        103|  Tablet|       1|  300|2024-09-02 10:00:00|        300|2024-09-02|\n",
            "|             4|        104| Monitor|       2|  200|2024-09-02 11:00:00|        400|2024-09-02|\n",
            "+--------------+-----------+--------+--------+-----+-------------------+-----------+----------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#widgets\n",
        "# Install ipywidgets in Colab or Jupyter if needed\n",
        "!pip install ipywidgets"
      ],
      "metadata": {
        "id": "ZyG0qrVzt1xk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display\n",
        "\n",
        "# Step 1: Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"PySpark with Widgets Example\").getOrCreate()\n",
        "\n",
        "# Step 2: Create a simple DataFrame\n",
        "data = [\n",
        "    (\"John\", 28, \"Male\", 60000),\n",
        "    (\"Jane\", 32, \"Female\", 72000),\n",
        "    (\"Mike\", 45, \"Male\", 84000),\n",
        "    (\"Emily\", 23, \"Female\", 52000),\n",
        "    (\"Alex\", 36, \"Male\", 67000)\n",
        "]\n",
        "\n",
        "df = spark.createDataFrame(data, [\"name\", \"age\", \"gender\", \"salary\"])\n",
        "\n",
        "# Show the DataFrame\n",
        "df.show()\n",
        "\n",
        "#step 3: create widgets\n",
        "#Dropdown widget to select column for filtering\n",
        "column_dropdown = widgets.Dropdown (\n",
        "    options=[\"age\", \"salary\"],\n",
        "    value=\"age\",\n",
        "    description=\"Filter By:\",\n",
        ")\n",
        "\n",
        "#Slider widget to choose a value for filtering\n",
        "slider = widgets.IntSlider (\n",
        "    value = 30,\n",
        "    min=20,\n",
        "    max=100,\n",
        "    step=5,\n",
        "    description=\"Threshold:\",\n",
        "    continuous_update=False\n",
        ")\n",
        "\n",
        "#Button to trigger filtering\n",
        "button = widgets.Button(description=\"Apply Filter\")\n",
        "\n",
        "#Output area to show the results\n",
        "output = widgets.Output()\n",
        "\n",
        "#Display the widgets\n",
        "display(column_dropdown, slider, button, output)\n",
        "\n",
        "#Step 4: Define a function to apply the filtering based on widget inputs\n",
        "def apply_filter(b):\n",
        "  column = column_dropdown.value\n",
        "  threshold = slider.value\n",
        "\n",
        "# Clear previous output\n",
        "output.clear_output()\n",
        "\n",
        "#Filter the DataFrame based on widget values\n",
        "df_filtered = df.filter(df[column] > threshold)\n",
        "\n",
        "#Show the filtered DataFrame\n",
        "with output:\n",
        "  print (f\"Filtering by {column} > {threshold}\")\n",
        "  df_filtered.show()\n",
        "\n",
        "#Step 5: Attach the function to the button click event\n",
        "button.on_click(apply_filter)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478,
          "referenced_widgets": [
            "5c91802837fd42cbbfede951e4cb1687",
            "e78c39f851ed41b7990a4f3bdac75748",
            "278d67a682584ea598d5c385f1fbccb7",
            "b1cca2cfbf3c446e8b21903dfa88e909",
            "ef96b6950283408f9af118f9f2a173e4",
            "675d1b8bfbd146bd95989dad0ab52614",
            "d0e1ac04e00c4c449c9ec8c76080ae85",
            "6549c656e47348c8b9199338fe18c0d1",
            "deb3f94b74974c02a0193ee54248dcb8",
            "011b1dabb86a40ce8024f84c536e1587",
            "43241bffd860421abf965f2d12684987"
          ]
        },
        "id": "56Ww_kztsX9v",
        "outputId": "ef056cf7-0b64-4f11-8825-aa7183e9f6a4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----+---+------+------+\n",
            "| name|age|gender|salary|\n",
            "+-----+---+------+------+\n",
            "| John| 28|  Male| 60000|\n",
            "| Jane| 32|Female| 72000|\n",
            "| Mike| 45|  Male| 84000|\n",
            "|Emily| 23|Female| 52000|\n",
            "| Alex| 36|  Male| 67000|\n",
            "+-----+---+------+------+\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Dropdown(description='Filter By:', options=('age', 'salary'), value='age')"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5c91802837fd42cbbfede951e4cb1687"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "IntSlider(value=30, continuous_update=False, description='Threshold:', min=20, step=5)"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b1cca2cfbf3c446e8b21903dfa88e909"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Button(description='Apply Filter', style=ButtonStyle())"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d0e1ac04e00c4c449c9ec8c76080ae85"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Output()"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "011b1dabb86a40ce8024f84c536e1587"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'column' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-53c45cc33233>\u001b[0m in \u001b[0;36m<cell line: 58>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;31m#Filter the DataFrame based on widget values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mdf_filtered\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolumn\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#Show the filtered DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'column' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### **Exercise: PySpark Data Transformations on Movie Data**\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, year, to_date, avg\n",
        "\n",
        "# Initialize Spark Session\n",
        "spark = SparkSession.builder.appName(\"MovieDataTransformations\").getOrCreate()\n",
        "\n",
        "### **Tasks**:\n",
        "\n",
        "#1. **Load the Dataset**:\n",
        "file_path = \"/content/sample_data/movies.csv.txt\"\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the DataFrame\n",
        "print(\"Initial DataFrame:\")\n",
        "df.show()\n",
        "\n",
        "# *2. Filter Movies by Genre*\n",
        "sci_fi_movies = df.filter(df.genre == \"Sci-Fi\")\n",
        "print(\"Movies in the 'Sci-Fi' genre:\")\n",
        "sci_fi_movies.show()\n",
        "\n",
        "# *3. Top-Rated Movies*\n",
        "top_rated_movies = df.orderBy(col(\"rating\").desc()).limit(3)\n",
        "print(\"Top 3 highest-rated movies:\")\n",
        "top_rated_movies.show()\n",
        "\n",
        "# *4. Movies Released After 2010*\n",
        "df = df.withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\"))\n",
        "movies_after_2010 = df.filter(year(col(\"date\")) > 2010)\n",
        "print(\"Movies released after 2010:\")\n",
        "movies_after_2010.show()\n",
        "\n",
        "# *5. Calculate Average Box Office Collection by Genre*\n",
        "avg_box_office_by_genre = df.groupBy(\"genre\").agg(avg(\"box_office\").alias(\"average_box_office\"))\n",
        "print(\"Average box office collection by genre:\")\n",
        "avg_box_office_by_genre.show()\n",
        "\n",
        "# *6. Add a New Column for Box Office in Billions*\n",
        "df = df.withColumn(\"box_office_in_billions\", col(\"box_office\") / 1e9)\n",
        "print(\"DataFrame with box office in billions:\")\n",
        "df.show()\n",
        "\n",
        "# *7. Sort Movies by Box Office Collection*\n",
        "sorted_movies_by_box_office = df.orderBy(col(\"box_office\").desc())\n",
        "print(\"Movies sorted by box office collection (descending):\")\n",
        "sorted_movies_by_box_office.show()\n",
        "\n",
        "# *8. Count the Number of Movies per Genre*\n",
        "count_movies_per_genre = df.groupBy(\"genre\").count()\n",
        "print(\"Number of movies per genre:\")\n",
        "count_movies_per_genre.show()\n",
        "\n",
        "# Stop Spark Session\n",
        "spark.stop()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xr23gHFh1Nwz",
        "outputId": "f0343053-fb8f-4ede-af27-b85951cdbd3b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial DataFrame:\n",
            "+--------+-----------------+---------+------+----------+----------+\n",
            "|movie_id|            title|    genre|rating|box_office|      date|\n",
            "+--------+-----------------+---------+------+----------+----------+\n",
            "|       1|        Inception|   Sci-Fi|   8.8| 830000000|2010-07-16|\n",
            "|       2|  The Dark Knight|   Action|   9.0|1004000000|2008-07-18|\n",
            "|       3|     Interstellar|   Sci-Fi|   8.6| 677000000|2014-11-07|\n",
            "|       4|Avengers: Endgame|   Action|   8.4|2797000000|2019-04-26|\n",
            "|       5|    The Lion King|Animation|   8.5|1657000000|1994-06-15|\n",
            "|       6|      Toy Story 4|Animation|   7.8|1073000000|2019-06-21|\n",
            "|       7|        Frozen II|Animation|   7.0|1450000000|2019-11-22|\n",
            "|       8|            Joker|    Drama|   8.5|1074000000|2019-10-04|\n",
            "|       9|         Parasite|    Drama|   8.6| 258000000|2019-05-30|\n",
            "+--------+-----------------+---------+------+----------+----------+\n",
            "\n",
            "Movies in the 'Sci-Fi' genre:\n",
            "+--------+------------+------+------+----------+----------+\n",
            "|movie_id|       title| genre|rating|box_office|      date|\n",
            "+--------+------------+------+------+----------+----------+\n",
            "|       1|   Inception|Sci-Fi|   8.8| 830000000|2010-07-16|\n",
            "|       3|Interstellar|Sci-Fi|   8.6| 677000000|2014-11-07|\n",
            "+--------+------------+------+------+----------+----------+\n",
            "\n",
            "Top 3 highest-rated movies:\n",
            "+--------+---------------+------+------+----------+----------+\n",
            "|movie_id|          title| genre|rating|box_office|      date|\n",
            "+--------+---------------+------+------+----------+----------+\n",
            "|       2|The Dark Knight|Action|   9.0|1004000000|2008-07-18|\n",
            "|       1|      Inception|Sci-Fi|   8.8| 830000000|2010-07-16|\n",
            "|       3|   Interstellar|Sci-Fi|   8.6| 677000000|2014-11-07|\n",
            "+--------+---------------+------+------+----------+----------+\n",
            "\n",
            "Movies released after 2010:\n",
            "+--------+-----------------+---------+------+----------+----------+\n",
            "|movie_id|            title|    genre|rating|box_office|      date|\n",
            "+--------+-----------------+---------+------+----------+----------+\n",
            "|       3|     Interstellar|   Sci-Fi|   8.6| 677000000|2014-11-07|\n",
            "|       4|Avengers: Endgame|   Action|   8.4|2797000000|2019-04-26|\n",
            "|       6|      Toy Story 4|Animation|   7.8|1073000000|2019-06-21|\n",
            "|       7|        Frozen II|Animation|   7.0|1450000000|2019-11-22|\n",
            "|       8|            Joker|    Drama|   8.5|1074000000|2019-10-04|\n",
            "|       9|         Parasite|    Drama|   8.6| 258000000|2019-05-30|\n",
            "+--------+-----------------+---------+------+----------+----------+\n",
            "\n",
            "Average box office collection by genre:\n",
            "+---------+--------------------+\n",
            "|    genre|  average_box_office|\n",
            "+---------+--------------------+\n",
            "|    Drama|              6.66E8|\n",
            "|Animation|1.3933333333333333E9|\n",
            "|   Action|            1.9005E9|\n",
            "|   Sci-Fi|             7.535E8|\n",
            "+---------+--------------------+\n",
            "\n",
            "DataFrame with box office in billions:\n",
            "+--------+-----------------+---------+------+----------+----------+----------------------+\n",
            "|movie_id|            title|    genre|rating|box_office|      date|box_office_in_billions|\n",
            "+--------+-----------------+---------+------+----------+----------+----------------------+\n",
            "|       1|        Inception|   Sci-Fi|   8.8| 830000000|2010-07-16|                  0.83|\n",
            "|       2|  The Dark Knight|   Action|   9.0|1004000000|2008-07-18|                 1.004|\n",
            "|       3|     Interstellar|   Sci-Fi|   8.6| 677000000|2014-11-07|                 0.677|\n",
            "|       4|Avengers: Endgame|   Action|   8.4|2797000000|2019-04-26|                 2.797|\n",
            "|       5|    The Lion King|Animation|   8.5|1657000000|1994-06-15|                 1.657|\n",
            "|       6|      Toy Story 4|Animation|   7.8|1073000000|2019-06-21|                 1.073|\n",
            "|       7|        Frozen II|Animation|   7.0|1450000000|2019-11-22|                  1.45|\n",
            "|       8|            Joker|    Drama|   8.5|1074000000|2019-10-04|                 1.074|\n",
            "|       9|         Parasite|    Drama|   8.6| 258000000|2019-05-30|                 0.258|\n",
            "+--------+-----------------+---------+------+----------+----------+----------------------+\n",
            "\n",
            "Movies sorted by box office collection (descending):\n",
            "+--------+-----------------+---------+------+----------+----------+----------------------+\n",
            "|movie_id|            title|    genre|rating|box_office|      date|box_office_in_billions|\n",
            "+--------+-----------------+---------+------+----------+----------+----------------------+\n",
            "|       4|Avengers: Endgame|   Action|   8.4|2797000000|2019-04-26|                 2.797|\n",
            "|       5|    The Lion King|Animation|   8.5|1657000000|1994-06-15|                 1.657|\n",
            "|       7|        Frozen II|Animation|   7.0|1450000000|2019-11-22|                  1.45|\n",
            "|       8|            Joker|    Drama|   8.5|1074000000|2019-10-04|                 1.074|\n",
            "|       6|      Toy Story 4|Animation|   7.8|1073000000|2019-06-21|                 1.073|\n",
            "|       2|  The Dark Knight|   Action|   9.0|1004000000|2008-07-18|                 1.004|\n",
            "|       1|        Inception|   Sci-Fi|   8.8| 830000000|2010-07-16|                  0.83|\n",
            "|       3|     Interstellar|   Sci-Fi|   8.6| 677000000|2014-11-07|                 0.677|\n",
            "|       9|         Parasite|    Drama|   8.6| 258000000|2019-05-30|                 0.258|\n",
            "+--------+-----------------+---------+------+----------+----------+----------------------+\n",
            "\n",
            "Number of movies per genre:\n",
            "+---------+-----+\n",
            "|    genre|count|\n",
            "+---------+-----+\n",
            "|    Drama|    2|\n",
            "|Animation|    3|\n",
            "|   Action|    2|\n",
            "|   Sci-Fi|    2|\n",
            "+---------+-----+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}